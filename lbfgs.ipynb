{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1316827294/-/blob/master/lbfgs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAaypoJB1P66"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/lbfgs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Juf_fYzcH20_"
      },
      "outputs": [],
      "source": [
        "# !pip install pycutest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyM9dqjy1Pt9"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/hjmshi/PyTorch-LBFGS.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !kill -9 91"
      ],
      "metadata": {
        "id": "RPW3z9sxgVeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GswCuPHgejHx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/lbfgs\")\n",
        "from torch import matmul\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import reduce\n",
        "from copy import deepcopy\n",
        "from torch.optim import Optimizer\n",
        "\n",
        "\n",
        "def is_legal(v):\n",
        "    \"\"\"\n",
        "    Checks that tensor is not NaN or Inf.\n",
        "\n",
        "    Inputs:\n",
        "        v (tensor): tensor to be checked\n",
        "\n",
        "    \"\"\"\n",
        "    legal = not torch.isnan(v).any() and not torch.isinf(v)\n",
        "\n",
        "    return legal\n",
        "\n",
        "\n",
        "def polyinterp(points, x_min_bound=None, x_max_bound=None, plot=False):\n",
        "    \"\"\"\n",
        "    Gives the minimizer and minimum of the interpolating polynomial over given points\n",
        "    based on function and derivative information. Defaults to bisection if no critical\n",
        "    points are valid.\n",
        "\n",
        "    Based on polyinterp.m Matlab function in minFunc by Mark Schmidt with some slight\n",
        "    modifications.\n",
        "\n",
        "    Implemented by: Hao-Jun Michael Shi and Dheevatsa Mudigere\n",
        "    Last edited 12/6/18.\n",
        "\n",
        "    Inputs:\n",
        "        points (nparray): two-dimensional array with each point of form [x f g]\n",
        "        x_min_bound (float): minimum value that brackets minimum (default: minimum of points)\n",
        "        x_max_bound (float): maximum value that brackets minimum (default: maximum of points)\n",
        "        plot (bool): plot interpolating polynomial\n",
        "\n",
        "    Outputs:\n",
        "        x_sol (float): minimizer of interpolating polynomial\n",
        "        F_min (float): minimum of interpolating polynomial\n",
        "\n",
        "    Note:\n",
        "      . Set f or g to np.nan if they are unknown\n",
        "\n",
        "    \"\"\"\n",
        "    no_points = points.shape[0]\n",
        "    order = np.sum(1 - np.isnan(points[:, 1:3]).astype('int')) - 1\n",
        "\n",
        "    x_min = np.min(points[:, 0])\n",
        "    x_max = np.max(points[:, 0])\n",
        "\n",
        "    # compute bounds of interpolation area\n",
        "    if x_min_bound is None:\n",
        "        x_min_bound = x_min\n",
        "    if x_max_bound is None:\n",
        "        x_max_bound = x_max\n",
        "\n",
        "    # explicit formula for quadratic interpolation\n",
        "    if no_points == 2 and order == 2 and plot is False:\n",
        "        # Solution to quadratic interpolation is given by:\n",
        "        # a = -(f1 - f2 - g1(x1 - x2))/(x1 - x2)^2\n",
        "        # x_min = x1 - g1/(2a)\n",
        "        # if x1 = 0, then is given by:\n",
        "        # x_min = - (g1*x2^2)/(2(f2 - f1 - g1*x2))\n",
        "\n",
        "        if points[0, 0] == 0:\n",
        "            x_sol = -points[0, 2] * points[1, 0] ** 2 / (\n",
        "                        2 * (points[1, 1] - points[0, 1] - points[0, 2] * points[1, 0]))\n",
        "        else:\n",
        "            a = -(points[0, 1] - points[1, 1] - points[0, 2] * (points[0, 0] - points[1, 0])) / (\n",
        "                        points[0, 0] - points[1, 0]) ** 2\n",
        "            x_sol = points[0, 0] - points[0, 2] / (2 * a)\n",
        "\n",
        "        x_sol = np.minimum(np.maximum(x_min_bound, x_sol), x_max_bound)\n",
        "\n",
        "    # explicit formula for cubic interpolation\n",
        "    elif no_points == 2 and order == 3 and plot is False:\n",
        "        # Solution to cubic interpolation is given by:\n",
        "        # d1 = g1 + g2 - 3((f1 - f2)/(x1 - x2))\n",
        "        # d2 = sqrt(d1^2 - g1*g2)\n",
        "        # x_min = x2 - (x2 - x1)*((g2 + d2 - d1)/(g2 - g1 + 2*d2))\n",
        "        d1 = points[0, 2] + points[1, 2] - 3 * ((points[0, 1] - points[1, 1]) / (points[0, 0] - points[1, 0]))\n",
        "        d2 = np.sqrt(d1 ** 2 - points[0, 2] * points[1, 2])\n",
        "        if np.isreal(d2):\n",
        "            x_sol = points[1, 0] - (points[1, 0] - points[0, 0]) * (\n",
        "                        (points[1, 2] + d2 - d1) / (points[1, 2] - points[0, 2] + 2 * d2))\n",
        "            x_sol = np.minimum(np.maximum(x_min_bound, x_sol), x_max_bound)\n",
        "        else:\n",
        "            x_sol = (x_max_bound + x_min_bound) / 2\n",
        "\n",
        "    # solve linear system\n",
        "    else:\n",
        "        # define linear constraints\n",
        "        A = np.zeros((0, order + 1))\n",
        "        b = np.zeros((0, 1))\n",
        "\n",
        "        # add linear constraints on function values\n",
        "        for i in range(no_points):\n",
        "            if not np.isnan(points[i, 1]):\n",
        "                constraint = np.zeros((1, order + 1))\n",
        "                for j in range(order, -1, -1):\n",
        "                    constraint[0, order - j] = points[i, 0] ** j\n",
        "                A = np.append(A, constraint, 0)\n",
        "                b = np.append(b, points[i, 1])\n",
        "\n",
        "        # add linear constraints on gradient values\n",
        "        for i in range(no_points):\n",
        "            if not np.isnan(points[i, 2]):\n",
        "                constraint = np.zeros((1, order + 1))\n",
        "                for j in range(order):\n",
        "                    constraint[0, j] = (order - j) * points[i, 0] ** (order - j - 1)\n",
        "                A = np.append(A, constraint, 0)\n",
        "                b = np.append(b, points[i, 2])\n",
        "\n",
        "        # check if system is solvable\n",
        "        if A.shape[0] != A.shape[1] or np.linalg.matrix_rank(A) != A.shape[0]:\n",
        "            x_sol = (x_min_bound + x_max_bound) / 2\n",
        "            f_min = np.Inf\n",
        "        else:\n",
        "            # solve linear system for interpolating polynomial\n",
        "            coeff = np.linalg.solve(A, b)\n",
        "\n",
        "            # compute critical points\n",
        "            dcoeff = np.zeros(order)\n",
        "            for i in range(len(coeff) - 1):\n",
        "                dcoeff[i] = coeff[i] * (order - i)\n",
        "\n",
        "            crit_pts = np.array([x_min_bound, x_max_bound])\n",
        "            crit_pts = np.append(crit_pts, points[:, 0])\n",
        "\n",
        "            if not np.isinf(dcoeff).any():\n",
        "                roots = np.roots(dcoeff)\n",
        "                crit_pts = np.append(crit_pts, roots)\n",
        "\n",
        "            # test critical points\n",
        "            f_min = np.Inf\n",
        "            x_sol = (x_min_bound + x_max_bound) / 2  # defaults to bisection\n",
        "            for crit_pt in crit_pts:\n",
        "                if np.isreal(crit_pt) and crit_pt >= x_min_bound and crit_pt <= x_max_bound:\n",
        "                    F_cp = np.polyval(coeff, crit_pt)\n",
        "                    if np.isreal(F_cp) and F_cp < f_min:\n",
        "                        x_sol = np.real(crit_pt)\n",
        "                        f_min = np.real(F_cp)\n",
        "\n",
        "            if (plot):\n",
        "                plt.figure()\n",
        "                x = np.arange(x_min_bound, x_max_bound, (x_max_bound - x_min_bound) / 10000)\n",
        "                f = np.polyval(coeff, x)\n",
        "                plt.plot(x, f)\n",
        "                plt.plot(x_sol, f_min, 'x')\n",
        "\n",
        "    return x_sol\n",
        "\n",
        "\n",
        "class LBFGS2(Optimizer):\n",
        "    \"\"\"\n",
        "    Implements the L-BFGS algorithm. Compatible with multi-batch and full-overlap\n",
        "    L-BFGS implementations and (stochastic) Powell damping. Partly based on the\n",
        "    original L-BFGS implementation in PyTorch, Mark Schmidt's minFunc MATLAB code,\n",
        "    and Michael Overton's weak Wolfe line search MATLAB code.\n",
        "\n",
        "    Implemented by: Hao-Jun Michael Shi and Dheevatsa Mudigere\n",
        "    Last edited 10/20/20.\n",
        "\n",
        "    Warnings:\n",
        "      . Does not support per-parameter options and parameter groups.\n",
        "      . All parameters have to be on a single device.\n",
        "\n",
        "    Inputs:\n",
        "        lr (float): steplength or learning rate (default: 1)\n",
        "        history_size (int): update history size (default: 10)\n",
        "        line_search (str): designates line search to use (default: 'Wolfe')\n",
        "            Options:\n",
        "                'None': uses steplength designated in algorithm\n",
        "                'Armijo': uses Armijo backtracking line search\n",
        "                'Wolfe': uses Armijo-Wolfe bracketing line search\n",
        "        dtype: data type (default: torch.float)\n",
        "        debug (bool): debugging mode\n",
        "\n",
        "    References:\n",
        "    [1] Berahas, Albert S., Jorge Nocedal, and Martin Takác. \"A Multi-Batch L-BFGS\n",
        "        Method for Machine Learning.\" Advances in Neural Information Processing\n",
        "        Systems. 2016.\n",
        "    [2] Bollapragada, Raghu, et al. \"A Progressive Batching L-BFGS Method for Machine\n",
        "        Learning.\" International Conference on Machine Learning. 2018.\n",
        "    [3] Lewis, Adrian S., and Michael L. Overton. \"Nonsmooth Optimization via Quasi-Newton\n",
        "        Methods.\" Mathematical Programming 141.1-2 (2013): 135-163.\n",
        "    [4] Liu, Dong C., and Jorge Nocedal. \"On the Limited Memory BFGS Method for\n",
        "        Large Scale Optimization.\" Mathematical Programming 45.1-3 (1989): 503-528.\n",
        "    [5] Nocedal, Jorge. \"Updating Quasi-Newton Matrices With Limited Storage.\"\n",
        "        Mathematics of Computation 35.151 (1980): 773-782.\n",
        "    [6] Nocedal, Jorge, and Stephen J. Wright. \"Numerical Optimization.\" Springer New York,\n",
        "        2006.\n",
        "    [7] Schmidt, Mark. \"minFunc: Unconstrained Differentiable Multivariate Optimization\n",
        "        in Matlab.\" Software available at http://www.cs.ubc.ca/~schmidtm/Software/minFunc.html\n",
        "        (2005).\n",
        "    [8] Schraudolph, Nicol N., Jin Yu, and Simon Günter. \"A Stochastic Quasi-Newton\n",
        "        Method for Online Convex Optimization.\" Artificial Intelligence and Statistics.\n",
        "        2007.\n",
        "    [9] Wang, Xiao, et al. \"Stochastic Quasi-Newton Methods for Nonconvex Stochastic\n",
        "        Optimization.\" SIAM Journal on Optimization 27.2 (2017): 927-956.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=1., history_size=10, line_search='Wolfe',\n",
        "                 dtype=torch.float, debug=False):\n",
        "\n",
        "        # ensure inputs are valid\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0 <= history_size:\n",
        "            raise ValueError(\"Invalid history size: {}\".format(history_size))\n",
        "        if line_search not in ['Armijo', 'Wolfe', 'None']:\n",
        "            raise ValueError(\"Invalid line search: {}\".format(line_search))\n",
        "\n",
        "        defaults = dict(lr=lr, history_size=history_size, line_search=line_search, dtype=dtype, debug=debug)\n",
        "        super(LBFGS2, self).__init__(params, defaults)\n",
        "\n",
        "        if len(self.param_groups) != 1:\n",
        "            raise ValueError(\"L-BFGS doesn't support per-parameter options \"\n",
        "                             \"(parameter groups)\")\n",
        "\n",
        "        self._params = self.param_groups[0]['params']\n",
        "        self._numel_cache = None\n",
        "\n",
        "        state = self.state['global_state']\n",
        "        state.setdefault('n_iter', 0)\n",
        "        state.setdefault('curv_skips', 0)\n",
        "        state.setdefault('fail_skips', 0)\n",
        "        state.setdefault('H_diag', 1)\n",
        "        state.setdefault('fail', True)\n",
        "\n",
        "        state['old_dirs'] = []\n",
        "        state['old_stps'] = []\n",
        "\n",
        "    def _numel(self):\n",
        "        if self._numel_cache is None:\n",
        "            self._numel_cache = reduce(lambda total, p: total + p.numel(), self._params, 0)\n",
        "        return self._numel_cache\n",
        "\n",
        "    def _gather_flat_grad(self):\n",
        "        views = []\n",
        "        for p in self._params:\n",
        "            if p.grad is None:\n",
        "                view = p.data.new(p.data.numel()).zero_()\n",
        "            elif p.grad.data.is_sparse:\n",
        "                view = p.grad.data.to_dense().view(-1)\n",
        "            else:\n",
        "                view = p.grad.data.view(-1)\n",
        "            views.append(view)\n",
        "        return torch.cat(views, 0)\n",
        "\n",
        "    def _add_update(self, step_size, update):\n",
        "        offset = 0\n",
        "        for p in self._params:\n",
        "            numel = p.numel()\n",
        "            # view as to avoid deprecated pointwise semantics\n",
        "            p.data.add_(step_size, update[offset:offset + numel].view_as(p.data))\n",
        "            offset += numel\n",
        "        assert offset == self._numel()\n",
        "\n",
        "    def _copy_params(self):\n",
        "        current_params = []\n",
        "        for param in self._params:\n",
        "            current_params.append(deepcopy(param.data))\n",
        "        return current_params\n",
        "\n",
        "    def _load_params(self, current_params):\n",
        "        i = 0\n",
        "        for param in self._params:\n",
        "            param.data[:] = current_params[i]\n",
        "            i += 1\n",
        "\n",
        "    def line_search(self, line_search):\n",
        "        \"\"\"\n",
        "        Switches line search option.\n",
        "\n",
        "        Inputs:\n",
        "            line_search (str): designates line search to use\n",
        "                Options:\n",
        "                    'None': uses steplength designated in algorithm\n",
        "                    'Armijo': uses Armijo backtracking line search\n",
        "                    'Wolfe': uses Armijo-Wolfe bracketing line search\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        group = self.param_groups[0]\n",
        "        group['line_search'] = line_search\n",
        "\n",
        "        return\n",
        "\n",
        "    def two_loop_recursion(self, vec):\n",
        "        \"\"\"\n",
        "        Performs two-loop recursion on given vector to obtain Hv.\n",
        "\n",
        "        Inputs:\n",
        "            vec (tensor): 1-D tensor to apply two-loop recursion to\n",
        "\n",
        "        Output:\n",
        "            r (tensor): matrix-vector product Hv\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        group = self.param_groups[0]\n",
        "        history_size = group['history_size']\n",
        "\n",
        "        state = self.state['global_state']\n",
        "        old_dirs = state.get('old_dirs')  # change in gradients\n",
        "        old_stps = state.get('old_stps')  # change in iterates\n",
        "        H_diag = state.get('H_diag')\n",
        "\n",
        "        # compute the product of the inverse Hessian approximation and the gradient\n",
        "        num_old = len(old_dirs)\n",
        "\n",
        "        if 'rho' not in state:\n",
        "            state['rho'] = [None] * history_size\n",
        "            state['alpha'] = [None] * history_size\n",
        "        rho = state['rho']\n",
        "        alpha = state['alpha']\n",
        "\n",
        "        for i in range(num_old):\n",
        "            rho[i] = 1. / old_stps[i].dot(old_dirs[i])\n",
        "\n",
        "        q = vec\n",
        "        for i in range(num_old - 1, -1, -1):\n",
        "            alpha[i] = old_dirs[i].dot(q) * rho[i]\n",
        "            q.add_(-alpha[i], old_stps[i])\n",
        "\n",
        "        # multiply by initial Hessian\n",
        "        # r/d is the final direction\n",
        "        r = torch.mul(q, H_diag)\n",
        "        for i in range(num_old):\n",
        "            beta = old_stps[i].dot(r) * rho[i]\n",
        "            r.add_(alpha[i] - beta, old_dirs[i])\n",
        "\n",
        "        return r\n",
        "\n",
        "    def curvature_update(self, flat_grad, eps=1e-2, damping=False):\n",
        "        \"\"\"\n",
        "        Performs curvature update.\n",
        "\n",
        "        Inputs:\n",
        "            flat_grad (tensor): 1-D tensor of flattened gradient for computing\n",
        "                gradient difference with previously stored gradient\n",
        "            eps (float): constant for curvature pair rejection or damping (default: 1e-2)\n",
        "            damping (bool): flag for using Powell damping (default: False)\n",
        "        \"\"\"\n",
        "\n",
        "        assert len(self.param_groups) == 1\n",
        "\n",
        "        # load parameters\n",
        "        if (eps <= 0):\n",
        "            raise (ValueError('Invalid eps; must be positive.'))\n",
        "\n",
        "        group = self.param_groups[0]\n",
        "        history_size = group['history_size']\n",
        "        debug = group['debug']\n",
        "\n",
        "        # variables cached in state (for tracing)\n",
        "        state = self.state['global_state']\n",
        "        fail = state.get('fail')\n",
        "\n",
        "        # check if line search failed\n",
        "        if not fail:\n",
        "\n",
        "            d = state.get('d')\n",
        "            t = state.get('t')\n",
        "            old_dirs = state.get('old_dirs')\n",
        "            old_stps = state.get('old_stps')\n",
        "            H_diag = state.get('H_diag')\n",
        "            prev_flat_grad = state.get('prev_flat_grad')\n",
        "            Bs = state.get('Bs')\n",
        "\n",
        "            # compute y's\n",
        "            y = flat_grad.sub(prev_flat_grad)\n",
        "            s = d.mul(t)\n",
        "            sBs = s.dot(Bs)\n",
        "            ys = y.dot(s)  # y*s\n",
        "\n",
        "            # update L-BFGS matrix\n",
        "            if ys > eps * sBs or damping == True:\n",
        "\n",
        "                # perform Powell damping\n",
        "                if damping == True and ys < eps * sBs:\n",
        "                    if debug:\n",
        "                        print('Applying Powell damping...')\n",
        "                    theta = ((1 - eps) * sBs) / (sBs - ys)\n",
        "                    y = theta * y + (1 - theta) * Bs\n",
        "\n",
        "                # updating memory\n",
        "                if len(old_dirs) == history_size:\n",
        "                    # shift history by one (limited-memory)\n",
        "                    old_dirs.pop(0)\n",
        "                    old_stps.pop(0)\n",
        "\n",
        "                # store new direction/step\n",
        "                old_dirs.append(s)\n",
        "                old_stps.append(y)\n",
        "\n",
        "                # update scale of initial Hessian approximation\n",
        "                H_diag = ys / y.dot(y)  # (y*y)\n",
        "\n",
        "                state['old_dirs'] = old_dirs\n",
        "                state['old_stps'] = old_stps\n",
        "                state['H_diag'] = H_diag\n",
        "\n",
        "            else:\n",
        "                # save skip\n",
        "                state['curv_skips'] += 1\n",
        "                if debug:\n",
        "                    print('Curvature pair skipped due to failed criterion')\n",
        "\n",
        "        else:\n",
        "            # save skip\n",
        "            state['fail_skips'] += 1\n",
        "            if debug:\n",
        "                print('Line search failed; curvature pair update skipped')\n",
        "\n",
        "        return\n",
        "\n",
        "    def _step(self, p_k, g_Ok, g_Sk=None, options=None):\n",
        "\n",
        "        if options is None:\n",
        "            options = {}\n",
        "        assert len(self.param_groups) == 1\n",
        "\n",
        "        # load parameter options\n",
        "        group = self.param_groups[0]\n",
        "        lr = group['lr']\n",
        "        line_search = group['line_search']\n",
        "        dtype = group['dtype']\n",
        "        debug = group['debug']\n",
        "\n",
        "        # variables cached in state (for tracing)\n",
        "        state = self.state['global_state']\n",
        "        d = state.get('d')\n",
        "        t = state.get('t')\n",
        "        prev_flat_grad = state.get('prev_flat_grad')\n",
        "        Bs = state.get('Bs')\n",
        "\n",
        "        # keep track of nb of iterations\n",
        "        state['n_iter'] += 1\n",
        "\n",
        "        # set search direction\n",
        "        d = p_k\n",
        "\n",
        "        # modify previous gradient\n",
        "        if prev_flat_grad is None:\n",
        "            prev_flat_grad = g_Ok.clone()\n",
        "        else:\n",
        "            prev_flat_grad.copy_(g_Ok)\n",
        "\n",
        "        # set initial step size\n",
        "        t = lr\n",
        "\n",
        "        # closure evaluation counter\n",
        "        closure_eval = 0\n",
        "\n",
        "        if g_Sk is None:\n",
        "            g_Sk = g_Ok.clone()\n",
        "\n",
        "        # perform Armijo backtracking line search\n",
        "        if line_search == 'Armijo':\n",
        "\n",
        "            # load options\n",
        "            if options:\n",
        "                if 'closure' not in options.keys():\n",
        "                    raise (ValueError('closure option not specified.'))\n",
        "                else:\n",
        "                    closure = options['closure']\n",
        "\n",
        "                if 'gtd' not in options.keys():\n",
        "                    # if closure_eval==0:\n",
        "                    #     d =-prev_flat_grad\n",
        "                    gtd = g_Sk.dot(d)\n",
        "                else:\n",
        "                    gtd = options['gtd']\n",
        "\n",
        "                if 'current_loss' not in options.keys():\n",
        "                    F_k = closure()\n",
        "                    closure_eval += 1\n",
        "                else:\n",
        "                    F_k = options['current_loss']\n",
        "\n",
        "                if 'eta' not in options.keys():\n",
        "                    eta = 2\n",
        "                elif options['eta'] <= 0:\n",
        "                    raise (ValueError('Invalid eta; must be positive.'))\n",
        "                else:\n",
        "                    eta = options['eta']\n",
        "\n",
        "                if 'c1' not in options.keys():\n",
        "                    c1 = 1e-4\n",
        "                elif options['c1'] >= 1 or options['c1'] <= 0:\n",
        "                    raise (ValueError('Invalid c1; must be strictly between 0 and 1.'))\n",
        "                else:\n",
        "                    c1 = options['c1']\n",
        "\n",
        "                if 'max_ls' not in options.keys():\n",
        "                    max_ls = 10\n",
        "                elif options['max_ls'] <= 0:\n",
        "                    raise (ValueError('Invalid max_ls; must be positive.'))\n",
        "                else:\n",
        "                    max_ls = options['max_ls']\n",
        "\n",
        "                if 'interpolate' not in options.keys():\n",
        "                    interpolate = True\n",
        "                else:\n",
        "                    interpolate = options['interpolate']\n",
        "\n",
        "                if 'inplace' not in options.keys():\n",
        "                    inplace = True\n",
        "                else:\n",
        "                    inplace = options['inplace']\n",
        "\n",
        "                if 'ls_debug' not in options.keys():\n",
        "                    ls_debug = False\n",
        "                else:\n",
        "                    ls_debug = options['ls_debug']\n",
        "\n",
        "            else:\n",
        "                raise (ValueError('Options are not specified; need closure evaluating function.'))\n",
        "\n",
        "            # initialize values\n",
        "            if interpolate:\n",
        "                if torch.cuda.is_available():\n",
        "                    F_prev = torch.tensor(np.nan, dtype=dtype).cuda()\n",
        "                else:\n",
        "                    F_prev = torch.tensor(np.nan, dtype=dtype)\n",
        "\n",
        "            ls_step = 0\n",
        "            t_prev = 0  # old steplength\n",
        "            fail = False  # failure flag\n",
        "\n",
        "            # begin print for debug mode\n",
        "            if ls_debug:\n",
        "                print(\n",
        "                    '==================================== Begin Armijo line search ===================================')\n",
        "                print('F(x): %.8e  g*d: %.8e' % (F_k, gtd))\n",
        "\n",
        "            # check if search direction is descent direction\n",
        "            if gtd >= 0:\n",
        "                desc_dir = False\n",
        "                if debug:\n",
        "                    print('Not a descent direction!')\n",
        "            else:\n",
        "                desc_dir = True\n",
        "\n",
        "            # store values if not in-place\n",
        "            if not inplace:\n",
        "                current_params = self._copy_params()\n",
        "            # if closure_eval == 0:\n",
        "            #     d = -prev_flat_grad\n",
        "            # update and evaluate at new point\n",
        "            self._add_update(t, d)\n",
        "            F_new = closure()\n",
        "            closure_eval += 1\n",
        "\n",
        "            # print info if debugging\n",
        "            if ls_debug:\n",
        "                print('LS Step: %d  t: %.8e  F(x+td): %.8e  F-c1*t*g*d: %.8e  F(x): %.8e'\n",
        "                      % (ls_step, t, F_new, F_k + c1 * t * gtd, F_k))\n",
        "\n",
        "            # check Armijo condition\n",
        "            while F_new > F_k + c1 * t * gtd or not is_legal(F_new):\n",
        "\n",
        "                # check if maximum number of iterations reached\n",
        "                if ls_step >= max_ls:\n",
        "                    if inplace:\n",
        "                        self._add_update(-t, d)\n",
        "                    else:\n",
        "                        self._load_params(current_params)\n",
        "\n",
        "                    t = 0\n",
        "                    F_new = closure()\n",
        "                    closure_eval += 1\n",
        "                    fail = True\n",
        "                    break\n",
        "\n",
        "                else:\n",
        "                    # store current steplength\n",
        "                    t_new = t\n",
        "\n",
        "                    # compute new steplength\n",
        "\n",
        "                    # if first step or not interpolating, then multiply by factor\n",
        "                    if ls_step == 0 or not interpolate or not is_legal(F_new):\n",
        "                        t = t / eta\n",
        "\n",
        "                    # if second step, use function value at new point along with\n",
        "                    # gradient and function at current iterate\n",
        "                    elif ls_step == 1 or not is_legal(F_prev):\n",
        "                        t = polyinterp(np.array([[0, F_k.item(), gtd.item()], [t_new, F_new.item(), np.nan]]))\n",
        "\n",
        "                    # otherwise, use function values at new point, previous point,\n",
        "                    # and gradient and function at current iterate\n",
        "                    else:\n",
        "                        t = polyinterp(np.array([[0, F_k.item(), gtd.item()], [t_new, F_new.item(), np.nan],\n",
        "                                                 [t_prev, F_prev.item(), np.nan]]))\n",
        "\n",
        "                    # if values are too extreme, adjust t\n",
        "                    if interpolate:\n",
        "                        if t < 1e-3 * t_new:\n",
        "                            t = 1e-3 * t_new\n",
        "                        elif t > 0.6 * t_new:\n",
        "                            t = 0.6 * t_new\n",
        "\n",
        "                        # store old point\n",
        "                        F_prev = F_new\n",
        "                        t_prev = t_new\n",
        "\n",
        "                    # update iterate and reevaluate\n",
        "\n",
        "                    if inplace:\n",
        "                        self._add_update(t - t_new, d)\n",
        "                    else:\n",
        "                        self._load_params(current_params)\n",
        "                        self._add_update(t, d)\n",
        "\n",
        "                    F_new = closure()\n",
        "                    closure_eval += 1\n",
        "                    ls_step += 1  # iterate\n",
        "\n",
        "                    # print info if debugging\n",
        "                    if ls_debug:\n",
        "                        print('LS Step: %d  t: %.8e  F(x+td):   %.8e  F-c1*t*g*d: %.8e  F(x): %.8e'\n",
        "                              % (ls_step, t, F_new, F_k + c1 * t * gtd, F_k))\n",
        "\n",
        "            # store Bs\n",
        "            if Bs is None:\n",
        "                Bs = (g_Sk.mul(-t)).clone()\n",
        "            else:\n",
        "                Bs.copy_(g_Sk.mul(-t))\n",
        "\n",
        "            # print final steplength\n",
        "            if ls_debug:\n",
        "                print('Final Steplength:', t)\n",
        "                print(\n",
        "                    '===================================== End Armijo line search ====================================')\n",
        "\n",
        "            state['d'] = d\n",
        "            state['prev_flat_grad'] = prev_flat_grad\n",
        "            state['t'] = t\n",
        "            state['Bs'] = Bs\n",
        "            state['fail'] = fail\n",
        "\n",
        "            return F_new, t, ls_step, closure_eval, desc_dir, fail\n",
        "\n",
        "        # perform weak Wolfe line search\n",
        "        elif line_search == 'Wolfe':\n",
        "\n",
        "            # load options\n",
        "            if options:\n",
        "                if 'closure' not in options.keys():\n",
        "                    raise (ValueError('closure option not specified.'))\n",
        "                else:\n",
        "                    closure = options['closure']\n",
        "\n",
        "                if 'current_loss' not in options.keys():\n",
        "                    F_k = closure()\n",
        "                    closure_eval += 1\n",
        "                else:\n",
        "                    F_k = options['current_loss']\n",
        "\n",
        "                if 'gtd' not in options.keys():\n",
        "                    gtd = g_Sk.dot(d)\n",
        "                else:\n",
        "                    gtd = options['gtd']\n",
        "\n",
        "                if 'eta' not in options.keys():\n",
        "                    eta = 2\n",
        "                elif options['eta'] <= 1:\n",
        "                    raise (ValueError('Invalid eta; must be greater than 1.'))\n",
        "                else:\n",
        "                    eta = options['eta']\n",
        "\n",
        "                if 'c1' not in options.keys():\n",
        "                    c1 = 1e-4\n",
        "                elif options['c1'] >= 1 or options['c1'] <= 0:\n",
        "                    raise (ValueError('Invalid c1; must be strictly between 0 and 1.'))\n",
        "                else:\n",
        "                    c1 = options['c1']\n",
        "\n",
        "                if 'c2' not in options.keys():\n",
        "                    c2 = 0.9\n",
        "                elif options['c2'] >= 1 or options['c2'] <= 0:\n",
        "                    raise (ValueError('Invalid c2; must be strictly between 0 and 1.'))\n",
        "                elif options['c2'] <= c1:\n",
        "                    raise (ValueError('Invalid c2; must be strictly larger than c1.'))\n",
        "                else:\n",
        "                    c2 = options['c2']\n",
        "\n",
        "                if 'max_ls' not in options.keys():\n",
        "                    max_ls = 10\n",
        "                elif options['max_ls'] <= 0:\n",
        "                    raise (ValueError('Invalid max_ls; must be positive.'))\n",
        "                else:\n",
        "                    max_ls = options['max_ls']\n",
        "\n",
        "                if 'interpolate' not in options.keys():\n",
        "                    interpolate = True\n",
        "                else:\n",
        "                    interpolate = options['interpolate']\n",
        "\n",
        "                if 'inplace' not in options.keys():\n",
        "                    inplace = True\n",
        "                else:\n",
        "                    inplace = options['inplace']\n",
        "\n",
        "                if 'ls_debug' not in options.keys():\n",
        "                    ls_debug = False\n",
        "                else:\n",
        "                    ls_debug = options['ls_debug']\n",
        "\n",
        "            else:\n",
        "                raise (ValueError('Options are not specified; need closure evaluating function.'))\n",
        "\n",
        "            # initialize counters\n",
        "            ls_step = 0\n",
        "            grad_eval = 0  # tracks gradient evaluations\n",
        "            t_prev = 0  # old steplength\n",
        "\n",
        "            # initialize bracketing variables and flag\n",
        "            alpha = 0\n",
        "            beta = float('Inf')\n",
        "            fail = False\n",
        "\n",
        "            # initialize values for line search\n",
        "            if (interpolate):\n",
        "                F_a = F_k\n",
        "                g_a = gtd\n",
        "\n",
        "                if (torch.cuda.is_available()):\n",
        "                    F_b = torch.tensor(np.nan, dtype=dtype).cuda()\n",
        "                    g_b = torch.tensor(np.nan, dtype=dtype).cuda()\n",
        "                else:\n",
        "                    F_b = torch.tensor(np.nan, dtype=dtype)\n",
        "                    g_b = torch.tensor(np.nan, dtype=dtype)\n",
        "\n",
        "            # begin print for debug mode\n",
        "            if ls_debug:\n",
        "                print(\n",
        "                    '==================================== Begin Wolfe line search ====================================')\n",
        "                print('F(x): %.8e  g*d: %.8e' % (F_k, gtd))\n",
        "\n",
        "            # check if search direction is descent direction\n",
        "            if gtd >= 0:\n",
        "                desc_dir = False\n",
        "                if debug:\n",
        "                    print('Not a descent direction!')\n",
        "            else:\n",
        "                desc_dir = True\n",
        "\n",
        "            # store values if not in-place\n",
        "            if not inplace:\n",
        "                current_params = self._copy_params()\n",
        "\n",
        "            # update and evaluate at new point\n",
        "            self._add_update(t, d)\n",
        "            F_new = closure()\n",
        "            closure_eval += 1\n",
        "\n",
        "            # main loop\n",
        "            while True:\n",
        "\n",
        "                # check if maximum number of line search steps have been reached\n",
        "                if ls_step >= max_ls:\n",
        "                    if inplace:\n",
        "                        self._add_update(-t, d)\n",
        "                    else:\n",
        "                        self._load_params(current_params)\n",
        "\n",
        "                    t = 0\n",
        "                    F_new = closure()\n",
        "                    F_new.backward()\n",
        "                    g_new = self._gather_flat_grad()\n",
        "                    closure_eval += 1\n",
        "                    grad_eval += 1\n",
        "                    fail = True\n",
        "                    break\n",
        "\n",
        "                # print info if debugging\n",
        "                if ls_debug:\n",
        "                    print('LS Step: %d  t: %.8e  alpha: %.8e  beta: %.8e'\n",
        "                          % (ls_step, t, alpha, beta))\n",
        "                    print('Armijo:  F(x+td): %.8e  F-c1*t*g*d: %.8e  F(x): %.8e'\n",
        "                          % (F_new, F_k + c1 * t * gtd, F_k))\n",
        "\n",
        "                # check Armijo condition\n",
        "                if F_new > F_k + c1 * t * gtd:\n",
        "\n",
        "                    # set upper bound\n",
        "                    beta = t\n",
        "                    t_prev = t\n",
        "\n",
        "                    # update interpolation quantities\n",
        "                    if interpolate:\n",
        "                        F_b = F_new\n",
        "                        if torch.cuda.is_available():\n",
        "                            g_b = torch.tensor(np.nan, dtype=dtype).cuda()\n",
        "                        else:\n",
        "                            g_b = torch.tensor(np.nan, dtype=dtype)\n",
        "\n",
        "                else:\n",
        "\n",
        "                    # compute gradient\n",
        "                    F_new.backward()\n",
        "                    g_new = self._gather_flat_grad()\n",
        "\n",
        "                    grad_eval += 1\n",
        "                    gtd_new = g_new.dot(d)\n",
        "\n",
        "                    # print info if debugging\n",
        "                    if ls_debug:\n",
        "                        print('Wolfe: g(x+td)*d: %.8e  c2*g*d: %.8e  gtd: %.8e'\n",
        "                              % (gtd_new, c2 * gtd, gtd))\n",
        "\n",
        "                    # check curvature condition\n",
        "                    if gtd_new < c2 * gtd:\n",
        "\n",
        "                        # set lower bound\n",
        "                        alpha = t\n",
        "                        t_prev = t\n",
        "\n",
        "                        # update interpolation quantities\n",
        "                        if interpolate:\n",
        "                            F_a = F_new\n",
        "                            g_a = gtd_new\n",
        "\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "                # compute new steplength\n",
        "\n",
        "                # if first step or not interpolating, then bisect or multiply by factor\n",
        "                if not interpolate or not is_legal(F_b):\n",
        "                    if beta == float('Inf'):\n",
        "                        t = eta * t\n",
        "                    else:\n",
        "                        t = (alpha + beta) / 2.0\n",
        "\n",
        "                # otherwise interpolate between a and b\n",
        "                else:\n",
        "                    t = polyinterp(np.array([[alpha, F_a.item(), g_a.item()], [beta, F_b.item(), g_b.item()]]))\n",
        "\n",
        "                    # if values are too extreme, adjust t\n",
        "                    if beta == float('Inf'):\n",
        "                        if t > 2 * eta * t_prev:\n",
        "                            t = 2 * eta * t_prev\n",
        "                        elif t < eta * t_prev:\n",
        "                            t = eta * t_prev\n",
        "                    else:\n",
        "                        if t < alpha + 0.2 * (beta - alpha):\n",
        "                            t = alpha + 0.2 * (beta - alpha)\n",
        "                        elif t > (beta - alpha) / 2.0:\n",
        "                            t = (beta - alpha) / 2.0\n",
        "\n",
        "                    # if we obtain nonsensical value from interpolation\n",
        "                    if t <= 0:\n",
        "                        t = (beta - alpha) / 2.0\n",
        "\n",
        "                # update parameters\n",
        "                if inplace:\n",
        "                    self._add_update(t - t_prev, d)\n",
        "                else:\n",
        "                    self._load_params(current_params)\n",
        "                    self._add_update(t, d)\n",
        "\n",
        "                # evaluate closure\n",
        "                F_new = closure()\n",
        "                closure_eval += 1\n",
        "                ls_step += 1\n",
        "\n",
        "            # store Bs\n",
        "            if Bs is None:\n",
        "                Bs = (g_Sk.mul(-t)).clone()\n",
        "            else:\n",
        "                Bs.copy_(g_Sk.mul(-t))\n",
        "\n",
        "            # print final steplength\n",
        "            if ls_debug:\n",
        "                print('Final Steplength:', t)\n",
        "                print(\n",
        "                    '===================================== End Wolfe line search =====================================')\n",
        "\n",
        "            state['d'] = d\n",
        "            state['prev_flat_grad'] = prev_flat_grad\n",
        "            state['t'] = t\n",
        "            state['Bs'] = Bs\n",
        "            state['fail'] = fail\n",
        "            return F_new, g_new, t, ls_step, closure_eval, grad_eval, desc_dir, fail\n",
        "\n",
        "        else:\n",
        "\n",
        "            # perform update\n",
        "            self._add_update(t, d)\n",
        "\n",
        "            # store Bs\n",
        "            if Bs is None:\n",
        "                Bs = (g_Sk.mul(-t)).clone()\n",
        "            else:\n",
        "                Bs.copy_(g_Sk.mul(-t))\n",
        "\n",
        "            state['d'] = d\n",
        "            state['prev_flat_grad'] = prev_flat_grad\n",
        "            state['t'] = t\n",
        "            state['Bs'] = Bs\n",
        "            state['fail'] = False\n",
        "\n",
        "            return t\n",
        "\n",
        "    def step(self, p_k, g_Ok, g_Sk=None, options={}):\n",
        "        return self._step(p_k, g_Ok, g_Sk, options)\n",
        "\n",
        "\n",
        "class FullBatchLBFGS(LBFGS2):\n",
        "\n",
        "    def __init__(self, params, lr=1, history_size=10, line_search='Wolfe',\n",
        "                 dtype=torch.float, debug=False):\n",
        "        super(FullBatchLBFGS, self).__init__(params, lr, history_size, line_search,\n",
        "                                             dtype, debug)\n",
        "\n",
        "    def step(self, options=None):\n",
        "\n",
        "        # load options for damping and eps\n",
        "        if 'damping' not in options.keys():\n",
        "            damping = False\n",
        "        else:\n",
        "            damping = options['damping']\n",
        "\n",
        "        if 'eps' not in options.keys():\n",
        "            eps = 1e-2\n",
        "        else:\n",
        "            eps = options['eps']\n",
        "\n",
        "        # gather gradient\n",
        "        grad = self._gather_flat_grad()\n",
        "\n",
        "        # update curvature if after 1st iteration\n",
        "        state = self.state['global_state']\n",
        "        if state['n_iter'] > 0:\n",
        "            self.curvature_update(grad, eps, damping)\n",
        "\n",
        "        # compute search direction\n",
        "        p = self.two_loop_recursion(-grad)\n",
        "\n",
        "        # take step\n",
        "        return self._step(p, grad, options=options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWpAbadq1PzL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "713f8b70-2a78-41da-87ac-b1e0279c97d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 6s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-682c6b447c0f>:264: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1420.)\n",
            "  p.data.add_(step_size, update[offset:offset + numel].view_as(p.data))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-52205c9bc146>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;31m# perform line search step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'closure'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'current_loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;31m# curvature update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-682c6b447c0f>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, p_k, g_Ok, g_Sk, options)\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_Ok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_Sk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_Ok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_Sk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-682c6b447c0f>\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, p_k, g_Ok, g_Sk, options)\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0;31m# update and evaluate at new point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m             \u001b[0mF_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m             \u001b[0mclosure_eval\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-52205c9bc146>\u001b[0m in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msubsmpl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mghost_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubsmpl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-52205c9bc146>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mopfun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mopfun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-52205c9bc146>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# x.size()[0]: batch size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         return F.max_pool2d(input, self.kernel_size, self.stride,\n\u001b[0m\u001b[1;32m    167\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                             return_indices=self.return_indices)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    780\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 14.76 GiB total capacity; 13.41 GiB already allocated; 3.88 MiB free; 13.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/lbfgs/PyTorch-LBFGS/functions')\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from keras.datasets import cifar10 # to load dataset\n",
        "from time import process_time\n",
        "\n",
        "from utils import compute_stats, get_grad\n",
        "# from LBFGS import LBFGS\n",
        "\n",
        "# Parameters for L-BFGS training\n",
        "max_iter = 100\n",
        "ghost_batch = 128\n",
        "batch_size = 8192\n",
        "\n",
        "# Load data\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "\n",
        "X_train = np.transpose(X_train, (0, 3, 1, 2))\n",
        "X_test = np.transpose(X_test, (0, 3, 1, 2))\n",
        "\n",
        "# Define network\n",
        "# Define network\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 1000)\n",
        "        self.fc2 = nn.Linear(1000, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "'''\n",
        "创建VGG块\n",
        "参数分别为输入通道数，输出通道数，卷积层个数，是否做最大池化\n",
        "'''\n",
        "def make_vgg_block(in_channel, out_channel, convs, pool=True):\n",
        "    net = []\n",
        "\n",
        "    # 不改变图片尺寸卷积\n",
        "    net.append(nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1))\n",
        "    net.append(nn.BatchNorm2d(out_channel))\n",
        "    net.append(nn.ReLU(inplace=True))\n",
        "\n",
        "    for i in range(convs - 1):\n",
        "        # 不改变图片尺寸卷积\n",
        "        net.append(nn.Conv2d(out_channel, out_channel, kernel_size=3, padding=1))\n",
        "        net.append(nn.BatchNorm2d(out_channel))\n",
        "        net.append(nn.ReLU(inplace=True))\n",
        "\n",
        "    if pool:\n",
        "        # 2*2最大池化，图片变为w/2 * h/2\n",
        "        net.append(nn.MaxPool2d(2))\n",
        "\n",
        "    return nn.Sequential(*net)\n",
        "\n",
        "\n",
        "# 定义网络模型\n",
        "class VGG19Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG19Net, self).__init__()\n",
        "\n",
        "        net = []\n",
        "\n",
        "        # 输入32*32，输出16*16\n",
        "        net.append(make_vgg_block(3, 64, 2))\n",
        "\n",
        "        # 输出8*8\n",
        "        net.append(make_vgg_block(64, 128, 2))\n",
        "\n",
        "        # 输出4*4\n",
        "        net.append(make_vgg_block(128, 256, 4))\n",
        "\n",
        "        # 输出2*2\n",
        "        net.append(make_vgg_block(256, 512, 4))\n",
        "\n",
        "        # 无池化层，输出保持2*2\n",
        "        net.append(make_vgg_block(512, 512, 4, False))\n",
        "\n",
        "        self.cnn = nn.Sequential(*net)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            # 512个feature，每个feature 2*2\n",
        "            nn.Linear(512*2*2, 256),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)\n",
        "\n",
        "        # x.size()[0]: batch size\n",
        "        x = x.view(x.size()[0], -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Check cuda availability\n",
        "cuda = torch.cuda.is_available()\n",
        "    \n",
        "# Create neural network model\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(2018)\n",
        "    model = VGG19Net().cuda() \n",
        "else:\n",
        "    torch.manual_seed(2018)\n",
        "    model = VGG19Net()\n",
        "# Define helper functions\n",
        "\n",
        "# Forward pass\n",
        "if cuda:\n",
        "    opfun = lambda X: model.forward(torch.from_numpy(X).cuda())\n",
        "else:\n",
        "    opfun = lambda X: model.forward(torch.from_numpy(X))\n",
        "\n",
        "# Forward pass through the network given the input\n",
        "if cuda:\n",
        "    predsfun = lambda op: np.argmax(op.cpu().data.numpy(), 1)\n",
        "else:\n",
        "    predsfun = lambda op: np.argmax(op.data.numpy(), 1)\n",
        "\n",
        "# Do the forward pass, then compute the accuracy\n",
        "accfun = lambda op, y: np.mean(np.equal(predsfun(op), y.squeeze())) * 100\n",
        "\n",
        "# Define optimizer\n",
        "# optimizer1 = LBFGS1(model.parameters(), lr=1., history_size=10, line_search='Wolfe', debug=True)\n",
        "optimizer2 = LBFGS2(model.parameters(), lr=1., history_size=10, line_search='Wolfe', debug=True)\n",
        "train_loss_list ,test_loss_list, test_acc_list,x_list = [],[],[],[]\n",
        "process_time()\n",
        "# Main training loop\n",
        "for n_iter in range(max_iter):\n",
        "    \n",
        "    # training mode\n",
        "    model.train()\n",
        "    \n",
        "    # sample batch\n",
        "    random_index = np.random.permutation(range(X_train.shape[0]))\n",
        "    Sk = random_index[0:batch_size]\n",
        "    \n",
        "    # compute initial gradient and objective\n",
        "    grad, obj = get_grad(optimizer2, X_train[Sk], y_train[Sk], opfun)\n",
        "    \n",
        "    # two-loop recursion to compute search direction\n",
        "    p = optimizer2.two_loop_recursion(-grad)\n",
        "            \n",
        "    # define closure for line search\n",
        "    def closure():              \n",
        "        \n",
        "        optimizer2.zero_grad()\n",
        "        \n",
        "        if cuda:\n",
        "            loss_fn = torch.tensor(0, dtype=torch.float).cuda()\n",
        "        else:\n",
        "            loss_fn = torch.tensor(0, dtype=torch.float)\n",
        "        \n",
        "        for subsmpl in np.array_split(Sk, max(int(batch_size / ghost_batch), 1)):\n",
        "                        \n",
        "            ops = opfun(X_train[subsmpl])\n",
        "            \n",
        "            if cuda:\n",
        "                tgts = torch.from_numpy(y_train[subsmpl]).cuda().long().squeeze()\n",
        "            else:\n",
        "                tgts = torch.from_numpy(y_train[subsmpl]).long().squeeze()\n",
        "                \n",
        "            loss_fn += F.cross_entropy(ops, tgts) * (len(subsmpl) / batch_size)\n",
        "                        \n",
        "        return loss_fn\n",
        "    \n",
        "    # perform line search step\n",
        "    options = {'closure': closure, 'current_loss': obj}\n",
        "    obj, grad, lr, _, _, _, _, _ = optimizer2.step(p, grad, options=options)\n",
        "        \n",
        "    # curvature update\n",
        "    optimizer2.curvature_update(grad)\n",
        "    \n",
        "    # compute statistics\n",
        "    model.eval()\n",
        "    train_loss, test_loss, test_acc = compute_stats(X_train, y_train, X_test, y_test, opfun, accfun,\n",
        "                                                    ghost_batch=128)\n",
        "    \n",
        "    train_loss_list.append(float(train_loss))\n",
        "    test_loss_list.append(float(test_loss))\n",
        "    test_acc_list.append(float(test_acc))\n",
        "    x_list.append(n_iter+1)\n",
        "    # print data\n",
        "    print('Iter:', n_iter + 1, 'lr:', lr, 'Training Loss:', train_loss, 'Test Loss:', test_loss,\n",
        "          'Test Accuracy:', test_acc)\n",
        "print(\"运行时间是: {:9.9}s\".format(process_time()))\n",
        "a=np.load('train_loss_list.npy')\n",
        "train_loss_list_new=a.tolist()\n",
        "\n",
        "a=np.load('test_loss_list.npy')\n",
        "test_loss_list_new=a.tolist()\n",
        "\n",
        "a=np.load('test_acc_list.npy')\n",
        "test_acc_list_new=a.tolist()\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(test_acc_list_new, label='New test Accuracy')\n",
        "plt.plot(test_acc_list, label='Test Accuracy')\n",
        "plt.title('New and old test Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(test_loss_list_new, label='New test Loss')\n",
        "plt.plot(test_loss_list, label='Test Loss')\n",
        "plt.title('New and old test Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# plt.plot(x_list, train_loss_list, color='red', marker='o', linestyle='dashed', linewidth=2, markersize=1)\n",
        "# plt.plot(x_list, test_loss_list, color='green', marker='o', linestyle='dashed', linewidth=2, markersize=1)\n",
        "# plt.plot(x_list, test_acc_list, color='blue', marker='o', linestyle='dashed', linewidth=2, markersize=1)\n",
        "# plt.legend(labels=('Training Loss', 'Test Loss', 'Test Accuracy'))\n",
        "# plt.savefig('./a2.jpg')\n",
        "# plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hnoBt_rxe7t"
      },
      "outputs": [],
      "source": [
        "# !ps -aux|grep python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%reset"
      ],
      "metadata": {
        "id": "HNLDn1EAmTHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4Xtsz8qejE0"
      },
      "outputs": [],
      "source": [
        "from torch import matmul\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import reduce\n",
        "from copy import deepcopy\n",
        "from torch.optim import Optimizer\n",
        "\n",
        "\n",
        "def is_legal(v):\n",
        "    \"\"\"\n",
        "    Checks that tensor is not NaN or Inf.\n",
        "\n",
        "    Inputs:\n",
        "        v (tensor): tensor to be checked\n",
        "\n",
        "    \"\"\"\n",
        "    legal = not torch.isnan(v).any() and not torch.isinf(v)\n",
        "\n",
        "    return legal\n",
        "\n",
        "\n",
        "def polyinterp(points, x_min_bound=None, x_max_bound=None, plot=False):\n",
        "    \"\"\"\n",
        "    Gives the minimizer and minimum of the interpolating polynomial over given points\n",
        "    based on function and derivative information. Defaults to bisection if no critical\n",
        "    points are valid.\n",
        "\n",
        "    Based on polyinterp.m Matlab function in minFunc by Mark Schmidt with some slight\n",
        "    modifications.\n",
        "\n",
        "    Implemented by: Hao-Jun Michael Shi and Dheevatsa Mudigere\n",
        "    Last edited 12/6/18.\n",
        "\n",
        "    Inputs:\n",
        "        points (nparray): two-dimensional array with each point of form [x f g]\n",
        "        x_min_bound (float): minimum value that brackets minimum (default: minimum of points)\n",
        "        x_max_bound (float): maximum value that brackets minimum (default: maximum of points)\n",
        "        plot (bool): plot interpolating polynomial\n",
        "\n",
        "    Outputs:\n",
        "        x_sol (float): minimizer of interpolating polynomial\n",
        "        F_min (float): minimum of interpolating polynomial\n",
        "\n",
        "    Note:\n",
        "      . Set f or g to np.nan if they are unknown\n",
        "\n",
        "    \"\"\"\n",
        "    no_points = points.shape[0]\n",
        "    order = np.sum(1 - np.isnan(points[:, 1:3]).astype('int')) - 1\n",
        "\n",
        "    x_min = np.min(points[:, 0])\n",
        "    x_max = np.max(points[:, 0])\n",
        "\n",
        "    # compute bounds of interpolation area\n",
        "    if x_min_bound is None:\n",
        "        x_min_bound = x_min\n",
        "    if x_max_bound is None:\n",
        "        x_max_bound = x_max\n",
        "\n",
        "    # explicit formula for quadratic interpolation\n",
        "    if no_points == 2 and order == 2 and plot is False:\n",
        "        # Solution to quadratic interpolation is given by:\n",
        "        # a = -(f1 - f2 - g1(x1 - x2))/(x1 - x2)^2\n",
        "        # x_min = x1 - g1/(2a)\n",
        "        # if x1 = 0, then is given by:\n",
        "        # x_min = - (g1*x2^2)/(2(f2 - f1 - g1*x2))\n",
        "\n",
        "        if points[0, 0] == 0:\n",
        "            x_sol = -points[0, 2] * points[1, 0] ** 2 / (\n",
        "                        2 * (points[1, 1] - points[0, 1] - points[0, 2] * points[1, 0]))\n",
        "        else:\n",
        "            a = -(points[0, 1] - points[1, 1] - points[0, 2] * (points[0, 0] - points[1, 0])) / (\n",
        "                        points[0, 0] - points[1, 0]) ** 2\n",
        "            x_sol = points[0, 0] - points[0, 2] / (2 * a)\n",
        "\n",
        "        x_sol = np.minimum(np.maximum(x_min_bound, x_sol), x_max_bound)\n",
        "\n",
        "    # explicit formula for cubic interpolation\n",
        "    elif no_points == 2 and order == 3 and plot is False:\n",
        "        # Solution to cubic interpolation is given by:\n",
        "        # d1 = g1 + g2 - 3((f1 - f2)/(x1 - x2))\n",
        "        # d2 = sqrt(d1^2 - g1*g2)\n",
        "        # x_min = x2 - (x2 - x1)*((g2 + d2 - d1)/(g2 - g1 + 2*d2))\n",
        "        d1 = points[0, 2] + points[1, 2] - 3 * ((points[0, 1] - points[1, 1]) / (points[0, 0] - points[1, 0]))\n",
        "        d2 = np.sqrt(d1 ** 2 - points[0, 2] * points[1, 2])\n",
        "        if np.isreal(d2):\n",
        "            x_sol = points[1, 0] - (points[1, 0] - points[0, 0]) * (\n",
        "                        (points[1, 2] + d2 - d1) / (points[1, 2] - points[0, 2] + 2 * d2))\n",
        "            x_sol = np.minimum(np.maximum(x_min_bound, x_sol), x_max_bound)\n",
        "        else:\n",
        "            x_sol = (x_max_bound + x_min_bound) / 2\n",
        "\n",
        "    # solve linear system\n",
        "    else:\n",
        "        # define linear constraints\n",
        "        A = np.zeros((0, order + 1))\n",
        "        b = np.zeros((0, 1))\n",
        "\n",
        "        # add linear constraints on function values\n",
        "        for i in range(no_points):\n",
        "            if not np.isnan(points[i, 1]):\n",
        "                constraint = np.zeros((1, order + 1))\n",
        "                for j in range(order, -1, -1):\n",
        "                    constraint[0, order - j] = points[i, 0] ** j\n",
        "                A = np.append(A, constraint, 0)\n",
        "                b = np.append(b, points[i, 1])\n",
        "\n",
        "        # add linear constraints on gradient values\n",
        "        for i in range(no_points):\n",
        "            if not np.isnan(points[i, 2]):\n",
        "                constraint = np.zeros((1, order + 1))\n",
        "                for j in range(order):\n",
        "                    constraint[0, j] = (order - j) * points[i, 0] ** (order - j - 1)\n",
        "                A = np.append(A, constraint, 0)\n",
        "                b = np.append(b, points[i, 2])\n",
        "\n",
        "        # check if system is solvable\n",
        "        if A.shape[0] != A.shape[1] or np.linalg.matrix_rank(A) != A.shape[0]:\n",
        "            x_sol = (x_min_bound + x_max_bound) / 2\n",
        "            f_min = np.Inf\n",
        "        else:\n",
        "            # solve linear system for interpolating polynomial\n",
        "            coeff = np.linalg.solve(A, b)\n",
        "\n",
        "            # compute critical points\n",
        "            dcoeff = np.zeros(order)\n",
        "            for i in range(len(coeff) - 1):\n",
        "                dcoeff[i] = coeff[i] * (order - i)\n",
        "\n",
        "            crit_pts = np.array([x_min_bound, x_max_bound])\n",
        "            crit_pts = np.append(crit_pts, points[:, 0])\n",
        "\n",
        "            if not np.isinf(dcoeff).any():\n",
        "                roots = np.roots(dcoeff)\n",
        "                crit_pts = np.append(crit_pts, roots)\n",
        "\n",
        "            # test critical points\n",
        "            f_min = np.Inf\n",
        "            x_sol = (x_min_bound + x_max_bound) / 2  # defaults to bisection\n",
        "            for crit_pt in crit_pts:\n",
        "                if np.isreal(crit_pt) and crit_pt >= x_min_bound and crit_pt <= x_max_bound:\n",
        "                    F_cp = np.polyval(coeff, crit_pt)\n",
        "                    if np.isreal(F_cp) and F_cp < f_min:\n",
        "                        x_sol = np.real(crit_pt)\n",
        "                        f_min = np.real(F_cp)\n",
        "\n",
        "            if (plot):\n",
        "                plt.figure()\n",
        "                x = np.arange(x_min_bound, x_max_bound, (x_max_bound - x_min_bound) / 10000)\n",
        "                f = np.polyval(coeff, x)\n",
        "                plt.plot(x, f)\n",
        "                plt.plot(x_sol, f_min, 'x')\n",
        "\n",
        "    return x_sol\n",
        "\n",
        "\n",
        "class LBFGS1(Optimizer):\n",
        "    \"\"\"\n",
        "    Implements the L-BFGS algorithm. Compatible with multi-batch and full-overlap\n",
        "    L-BFGS implementations and (stochastic) Powell damping. Partly based on the\n",
        "    original L-BFGS implementation in PyTorch, Mark Schmidt's minFunc MATLAB code,\n",
        "    and Michael Overton's weak Wolfe line search MATLAB code.\n",
        "\n",
        "    Implemented by: Hao-Jun Michael Shi and Dheevatsa Mudigere\n",
        "    Last edited 10/20/20.\n",
        "\n",
        "    Warnings:\n",
        "      . Does not support per-parameter options and parameter groups.\n",
        "      . All parameters have to be on a single device.\n",
        "\n",
        "    Inputs:\n",
        "        lr (float): steplength or learning rate (default: 1)\n",
        "        history_size (int): update history size (default: 10)\n",
        "        line_search (str): designates line search to use (default: 'Wolfe')\n",
        "            Options:\n",
        "                'None': uses steplength designated in algorithm\n",
        "                'Armijo': uses Armijo backtracking line search\n",
        "                'Wolfe': uses Armijo-Wolfe bracketing line search\n",
        "        dtype: data type (default: torch.float)\n",
        "        debug (bool): debugging mode\n",
        "\n",
        "    References:\n",
        "    [1] Berahas, Albert S., Jorge Nocedal, and Martin Takác. \"A Multi-Batch L-BFGS\n",
        "        Method for Machine Learning.\" Advances in Neural Information Processing\n",
        "        Systems. 2016.\n",
        "    [2] Bollapragada, Raghu, et al. \"A Progressive Batching L-BFGS Method for Machine\n",
        "        Learning.\" International Conference on Machine Learning. 2018.\n",
        "    [3] Lewis, Adrian S., and Michael L. Overton. \"Nonsmooth Optimization via Quasi-Newton\n",
        "        Methods.\" Mathematical Programming 141.1-2 (2013): 135-163.\n",
        "    [4] Liu, Dong C., and Jorge Nocedal. \"On the Limited Memory BFGS Method for\n",
        "        Large Scale Optimization.\" Mathematical Programming 45.1-3 (1989): 503-528.\n",
        "    [5] Nocedal, Jorge. \"Updating Quasi-Newton Matrices With Limited Storage.\"\n",
        "        Mathematics of Computation 35.151 (1980): 773-782.\n",
        "    [6] Nocedal, Jorge, and Stephen J. Wright. \"Numerical Optimization.\" Springer New York,\n",
        "        2006.\n",
        "    [7] Schmidt, Mark. \"minFunc: Unconstrained Differentiable Multivariate Optimization\n",
        "        in Matlab.\" Software available at http://www.cs.ubc.ca/~schmidtm/Software/minFunc.html\n",
        "        (2005).\n",
        "    [8] Schraudolph, Nicol N., Jin Yu, and Simon Günter. \"A Stochastic Quasi-Newton\n",
        "        Method for Online Convex Optimization.\" Artificial Intelligence and Statistics.\n",
        "        2007.\n",
        "    [9] Wang, Xiao, et al. \"Stochastic Quasi-Newton Methods for Nonconvex Stochastic\n",
        "        Optimization.\" SIAM Journal on Optimization 27.2 (2017): 927-956.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=1., history_size=10, line_search='Wolfe',\n",
        "                 dtype=torch.float, debug=False):\n",
        "\n",
        "        # ensure inputs are valid\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0 <= history_size:\n",
        "            raise ValueError(\"Invalid history size: {}\".format(history_size))\n",
        "        if line_search not in ['Armijo', 'Wolfe', 'None']:\n",
        "            raise ValueError(\"Invalid line search: {}\".format(line_search))\n",
        "\n",
        "        defaults = dict(lr=lr, history_size=history_size, line_search=line_search, dtype=dtype, debug=debug)\n",
        "        super(LBFGS1, self).__init__(params, defaults)\n",
        "\n",
        "        if len(self.param_groups) != 1:\n",
        "            raise ValueError(\"L-BFGS doesn't support per-parameter options \"\n",
        "                             \"(parameter groups)\")\n",
        "\n",
        "        self._params = self.param_groups[0]['params']\n",
        "        self._numel_cache = None\n",
        "\n",
        "        state = self.state['global_state']\n",
        "        state.setdefault('n_iter', 0)\n",
        "        state.setdefault('curv_skips', 0)\n",
        "        state.setdefault('fail_skips', 0)\n",
        "        state.setdefault('H_diag', 1)\n",
        "        state.setdefault('fail', True)\n",
        "\n",
        "        state['old_dirs'] = []\n",
        "        state['old_stps'] = []\n",
        "\n",
        "    def _numel(self):\n",
        "        if self._numel_cache is None:\n",
        "            self._numel_cache = reduce(lambda total, p: total + p.numel(), self._params, 0)\n",
        "        return self._numel_cache\n",
        "\n",
        "    def _gather_flat_grad(self):\n",
        "        views = []\n",
        "        for p in self._params:\n",
        "            if p.grad is None:\n",
        "                view = p.data.new(p.data.numel()).zero_()\n",
        "            elif p.grad.data.is_sparse:\n",
        "                view = p.grad.data.to_dense().view(-1)\n",
        "            else:\n",
        "                view = p.grad.data.view(-1)\n",
        "            views.append(view)\n",
        "        return torch.cat(views, 0)\n",
        "\n",
        "    def _add_update(self, step_size, update):\n",
        "        offset = 0\n",
        "        for p in self._params:\n",
        "            numel = p.numel()\n",
        "            # view as to avoid deprecated pointwise semantics\n",
        "            p.data.add_(step_size, update[offset:offset + numel].view_as(p.data))\n",
        "            offset += numel\n",
        "        assert offset == self._numel()\n",
        "\n",
        "    def _copy_params(self):\n",
        "        current_params = []\n",
        "        for param in self._params:\n",
        "            current_params.append(deepcopy(param.data))\n",
        "        return current_params\n",
        "\n",
        "    def _load_params(self, current_params):\n",
        "        i = 0\n",
        "        for param in self._params:\n",
        "            param.data[:] = current_params[i]\n",
        "            i += 1\n",
        "\n",
        "    def line_search(self, line_search):\n",
        "        \"\"\"\n",
        "        Switches line search option.\n",
        "\n",
        "        Inputs:\n",
        "            line_search (str): designates line search to use\n",
        "                Options:\n",
        "                    'None': uses steplength designated in algorithm\n",
        "                    'Armijo': uses Armijo backtracking line search\n",
        "                    'Wolfe': uses Armijo-Wolfe bracketing line search\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        group = self.param_groups[0]\n",
        "        group['line_search'] = line_search\n",
        "\n",
        "        return\n",
        "\n",
        "    def two_loop_recursion(self, vec):\n",
        "        \"\"\"\n",
        "        Performs two-loop recursion on given vector to obtain Hv.\n",
        "\n",
        "        Inputs:\n",
        "            vec (tensor): 1-D tensor to apply two-loop recursion to\n",
        "\n",
        "        Output:\n",
        "            r (tensor): matrix-vector product Hv\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        group = self.param_groups[0]\n",
        "        history_size = group['history_size']\n",
        "\n",
        "        state = self.state['global_state']\n",
        "        old_dirs = state.get('old_dirs')  # change in gradients\n",
        "        old_stps = state.get('old_stps')  # change in iterates\n",
        "        H_diag = state.get('H_diag')\n",
        "\n",
        "        # compute the product of the inverse Hessian approximation and the gradient\n",
        "        num_old = len(old_dirs)\n",
        "\n",
        "        if 'rho' not in state:\n",
        "            state['rho'] = [None] * history_size\n",
        "            state['alpha'] = [None] * history_size\n",
        "        rho = state['rho']\n",
        "        alpha = state['alpha']\n",
        "\n",
        "        for i in range(num_old):\n",
        "            rho[i] = 1. / old_stps[i].dot(old_dirs[i])\n",
        "\n",
        "        q = vec\n",
        "        for i in range(num_old - 1, -1, -1):\n",
        "            alpha[i] = old_dirs[i].dot(q) * rho[i]\n",
        "            q.add_(-alpha[i], old_stps[i])\n",
        "\n",
        "        # multiply by initial Hessian\n",
        "        # r/d is the final direction\n",
        "        r = torch.mul(q, H_diag)\n",
        "        for i in range(num_old):\n",
        "            beta = old_stps[i].dot(r) * rho[i]\n",
        "            r.add_(alpha[i] - beta, old_dirs[i])\n",
        "\n",
        "        return r\n",
        "\n",
        "    def curvature_update(self, flat_grad, eps=1e-2, damping=False):\n",
        "        \"\"\"\n",
        "        Performs curvature update.\n",
        "\n",
        "        Inputs:\n",
        "            flat_grad (tensor): 1-D tensor of flattened gradient for computing\n",
        "                gradient difference with previously stored gradient\n",
        "            eps (float): constant for curvature pair rejection or damping (default: 1e-2)\n",
        "            damping (bool): flag for using Powell damping (default: False)\n",
        "        \"\"\"\n",
        "\n",
        "        assert len(self.param_groups) == 1\n",
        "\n",
        "        # load parameters\n",
        "        if (eps <= 0):\n",
        "            raise (ValueError('Invalid eps; must be positive.'))\n",
        "\n",
        "        group = self.param_groups[0]\n",
        "        history_size = group['history_size']\n",
        "        debug = group['debug']\n",
        "\n",
        "        # variables cached in state (for tracing)\n",
        "        state = self.state['global_state']\n",
        "        fail = state.get('fail')\n",
        "\n",
        "        # check if line search failed\n",
        "        if not fail:\n",
        "\n",
        "            d = state.get('d')\n",
        "            t = state.get('t')\n",
        "            old_dirs = state.get('old_dirs')\n",
        "            old_stps = state.get('old_stps')\n",
        "            H_diag = state.get('H_diag')\n",
        "            prev_flat_grad = state.get('prev_flat_grad')\n",
        "            Bs = state.get('Bs')\n",
        "\n",
        "            # compute y's\n",
        "            y = flat_grad.sub(prev_flat_grad)\n",
        "            s = d.mul(t)\n",
        "            sBs = s.dot(Bs)\n",
        "            ys = y.dot(s)  # y*s\n",
        "\n",
        "            # update L-BFGS matrix\n",
        "            if ys > eps * sBs or damping == True:\n",
        "\n",
        "                # perform Powell damping\n",
        "                if damping == True and ys < eps * sBs:\n",
        "                    if debug:\n",
        "                        print('Applying Powell damping...')\n",
        "                    theta = ((1 - eps) * sBs) / (sBs - ys)\n",
        "                    y = theta * y + (1 - theta) * Bs\n",
        "\n",
        "                # updating memory\n",
        "                if len(old_dirs) == history_size:\n",
        "                    # shift history by one (limited-memory)\n",
        "                    old_dirs.pop(0)\n",
        "                    old_stps.pop(0)\n",
        "\n",
        "                # store new direction/step\n",
        "                old_dirs.append(s)\n",
        "                old_stps.append(y)\n",
        "\n",
        "                # update scale of initial Hessian approximation\n",
        "                H_diag = ys / y.dot(y)  # (y*y)\n",
        "\n",
        "                state['old_dirs'] = old_dirs\n",
        "                state['old_stps'] = old_stps\n",
        "                state['H_diag'] = H_diag\n",
        "\n",
        "            else:\n",
        "                # save skip\n",
        "                state['curv_skips'] += 1\n",
        "                if debug:\n",
        "                    print('Curvature pair skipped due to failed criterion')\n",
        "\n",
        "        else:\n",
        "            # save skip\n",
        "            state['fail_skips'] += 1\n",
        "            if debug:\n",
        "                print('Line search failed; curvature pair update skipped')\n",
        "\n",
        "        return\n",
        "\n",
        "    def _step(self, p_k, g_Ok, g_Sk=None, options=None):\n",
        "\n",
        "        if options is None:\n",
        "            options = {}\n",
        "        assert len(self.param_groups) == 1\n",
        "\n",
        "        # load parameter options\n",
        "        group = self.param_groups[0]\n",
        "        lr = group['lr']\n",
        "        line_search = group['line_search']\n",
        "        dtype = group['dtype']\n",
        "        debug = group['debug']\n",
        "\n",
        "        # variables cached in state (for tracing)\n",
        "        state = self.state['global_state']\n",
        "        d = state.get('d')\n",
        "        t = state.get('t')\n",
        "        prev_flat_grad = state.get('prev_flat_grad')\n",
        "        Bs = state.get('Bs')\n",
        "\n",
        "        # keep track of nb of iterations\n",
        "        state['n_iter'] += 1\n",
        "\n",
        "        d = p_k\n",
        "        \n",
        "\n",
        "        # modify previous gradient\n",
        "        if prev_flat_grad is None:\n",
        "            prev_flat_grad = g_Ok.clone()\n",
        "        else:\n",
        "            prev_flat_grad.copy_(g_Ok)\n",
        "        # d =-prev_flat_grad\n",
        "        # if state['n_iter']==1:\n",
        "        #     d =-prev_flat_grad\n",
        "        # else:\n",
        "        #     # set search direction\n",
        "        #     d = p_k\n",
        "        # set initial step size\n",
        "        t = lr\n",
        "\n",
        "        # closure evaluation counter\n",
        "        closure_eval = 0\n",
        "\n",
        "        if g_Sk is None:\n",
        "            g_Sk = g_Ok.clone()\n",
        "\n",
        "        # perform Armijo backtracking line search\n",
        "        if line_search == 'Armijo':\n",
        "\n",
        "            # load options\n",
        "            if options:\n",
        "                if 'closure' not in options.keys():\n",
        "                    raise (ValueError('closure option not specified.'))\n",
        "                else:\n",
        "                    closure = options['closure']\n",
        "\n",
        "                if 'gtd' not in options.keys():\n",
        "                    # if closure_eval==0:\n",
        "                    #     d =-prev_flat_grad\n",
        "                    gtd = g_Sk.dot(d)\n",
        "                else:\n",
        "                    gtd = options['gtd']\n",
        "\n",
        "                if 'current_loss' not in options.keys():\n",
        "                    F_k = closure()\n",
        "                    closure_eval += 1\n",
        "                else:\n",
        "                    F_k = options['current_loss']\n",
        "\n",
        "                if 'eta' not in options.keys():\n",
        "                    eta = 2\n",
        "                elif options['eta'] <= 0:\n",
        "                    raise (ValueError('Invalid eta; must be positive.'))\n",
        "                else:\n",
        "                    eta = options['eta']\n",
        "\n",
        "                if 'c1' not in options.keys():\n",
        "                    c1 = 1e-4\n",
        "                elif options['c1'] >= 1 or options['c1'] <= 0:\n",
        "                    raise (ValueError('Invalid c1; must be strictly between 0 and 1.'))\n",
        "                else:\n",
        "                    c1 = options['c1']\n",
        "\n",
        "                if 'max_ls' not in options.keys():\n",
        "                    max_ls = 10\n",
        "                elif options['max_ls'] <= 0:\n",
        "                    raise (ValueError('Invalid max_ls; must be positive.'))\n",
        "                else:\n",
        "                    max_ls = options['max_ls']\n",
        "\n",
        "                if 'interpolate' not in options.keys():\n",
        "                    interpolate = True\n",
        "                else:\n",
        "                    interpolate = options['interpolate']\n",
        "\n",
        "                if 'inplace' not in options.keys():\n",
        "                    inplace = True\n",
        "                else:\n",
        "                    inplace = options['inplace']\n",
        "\n",
        "                if 'ls_debug' not in options.keys():\n",
        "                    ls_debug = False\n",
        "                else:\n",
        "                    ls_debug = options['ls_debug']\n",
        "\n",
        "            else:\n",
        "                raise (ValueError('Options are not specified; need closure evaluating function.'))\n",
        "\n",
        "            # initialize values\n",
        "            if interpolate:\n",
        "                if torch.cuda.is_available():\n",
        "                    F_prev = torch.tensor(np.nan, dtype=dtype).cuda()\n",
        "                else:\n",
        "                    F_prev = torch.tensor(np.nan, dtype=dtype)\n",
        "\n",
        "            ls_step = 0\n",
        "            t_prev = 0  # old steplength\n",
        "            fail = False  # failure flag\n",
        "\n",
        "            # begin print for debug mode\n",
        "            if ls_debug:\n",
        "                print(\n",
        "                    '==================================== Begin Armijo line search ===================================')\n",
        "                print('F(x): %.8e  g*d: %.8e' % (F_k, gtd))\n",
        "\n",
        "            # check if search direction is descent direction\n",
        "            if gtd >= 0:\n",
        "                desc_dir = False\n",
        "                if debug:\n",
        "                    print('Not a descent direction!')\n",
        "            else:\n",
        "                desc_dir = True\n",
        "\n",
        "            # store values if not in-place\n",
        "            if not inplace:\n",
        "                current_params = self._copy_params()\n",
        "            # if closure_eval == 0:\n",
        "            #     d = -prev_flat_grad\n",
        "            # update and evaluate at new point\n",
        "            self._add_update(t, d)\n",
        "            F_new = closure()\n",
        "            closure_eval += 1\n",
        "\n",
        "            # print info if debugging\n",
        "            if ls_debug:\n",
        "                print('LS Step: %d  t: %.8e  F(x+td): %.8e  F-c1*t*g*d: %.8e  F(x): %.8e'\n",
        "                      % (ls_step, t, F_new, F_k + c1 * t * gtd, F_k))\n",
        "\n",
        "            # check Armijo condition\n",
        "            while F_new > F_k + c1 * t * gtd or not is_legal(F_new):\n",
        "\n",
        "                # check if maximum number of iterations reached\n",
        "                if ls_step >= max_ls:\n",
        "                    if inplace:\n",
        "                        self._add_update(-t, d)\n",
        "                    else:\n",
        "                        self._load_params(current_params)\n",
        "\n",
        "                    t = 0\n",
        "                    F_new = closure()\n",
        "                    closure_eval += 1\n",
        "                    fail = True\n",
        "                    break\n",
        "\n",
        "                else:\n",
        "                    # store current steplength\n",
        "                    t_new = t\n",
        "\n",
        "                    # compute new steplength\n",
        "\n",
        "                    # if first step or not interpolating, then multiply by factor\n",
        "                    if ls_step == 0 or not interpolate or not is_legal(F_new):\n",
        "                        t = t / eta\n",
        "\n",
        "                    # if second step, use function value at new point along with\n",
        "                    # gradient and function at current iterate\n",
        "                    elif ls_step == 1 or not is_legal(F_prev):\n",
        "                        t = polyinterp(np.array([[0, F_k.item(), gtd.item()], [t_new, F_new.item(), np.nan]]))\n",
        "\n",
        "                    # otherwise, use function values at new point, previous point,\n",
        "                    # and gradient and function at current iterate\n",
        "                    else:\n",
        "                        t = polyinterp(np.array([[0, F_k.item(), gtd.item()], [t_new, F_new.item(), np.nan],\n",
        "                                                 [t_prev, F_prev.item(), np.nan]]))\n",
        "\n",
        "                    # if values are too extreme, adjust t\n",
        "                    if interpolate:\n",
        "                        if t < 1e-3 * t_new:\n",
        "                            t = 1e-3 * t_new\n",
        "                        elif t > 0.6 * t_new:\n",
        "                            t = 0.6 * t_new\n",
        "\n",
        "                        # store old point\n",
        "                        F_prev = F_new\n",
        "                        t_prev = t_new\n",
        "\n",
        "                    # update iterate and reevaluate\n",
        "\n",
        "                    if inplace:\n",
        "                        self._add_update(t - t_new, d)\n",
        "                    else:\n",
        "                        self._load_params(current_params)\n",
        "                        self._add_update(t, d)\n",
        "\n",
        "                    F_new = closure()\n",
        "                    closure_eval += 1\n",
        "                    ls_step += 1  # iterate\n",
        "\n",
        "                    # print info if debugging\n",
        "                    if ls_debug:\n",
        "                        print('LS Step: %d  t: %.8e  F(x+td):   %.8e  F-c1*t*g*d: %.8e  F(x): %.8e'\n",
        "                              % (ls_step, t, F_new, F_k + c1 * t * gtd, F_k))\n",
        "\n",
        "            # store Bs\n",
        "            if Bs is None:\n",
        "                Bs = (g_Sk.mul(-t)).clone()\n",
        "            else:\n",
        "                Bs.copy_(g_Sk.mul(-t))\n",
        "\n",
        "            # print final steplength\n",
        "            if ls_debug:\n",
        "                print('Final Steplength:', t)\n",
        "                print(\n",
        "                    '===================================== End Armijo line search ====================================')\n",
        "\n",
        "            state['d'] = d\n",
        "            state['prev_flat_grad'] = prev_flat_grad\n",
        "            state['t'] = t\n",
        "            state['Bs'] = Bs\n",
        "            state['fail'] = fail\n",
        "\n",
        "            return F_new, t, ls_step, closure_eval, desc_dir, fail\n",
        "\n",
        "        # perform weak Wolfe line search\n",
        "        elif line_search == 'Wolfe':\n",
        "\n",
        "            # load options\n",
        "            if options:\n",
        "                if 'closure' not in options.keys():\n",
        "                    raise (ValueError('closure option not specified.'))\n",
        "                else:\n",
        "                    closure = options['closure']\n",
        "\n",
        "                if 'current_loss' not in options.keys():\n",
        "                    F_k = closure()\n",
        "                    closure_eval += 1\n",
        "                else:\n",
        "                    F_k = options['current_loss']\n",
        "\n",
        "                if 'gtd' not in options.keys():\n",
        "                    gtd = g_Sk.dot(d)\n",
        "                else:\n",
        "                    gtd = options['gtd']\n",
        "\n",
        "                if 'eta' not in options.keys():\n",
        "                    eta = 2\n",
        "                elif options['eta'] <= 1:\n",
        "                    raise (ValueError('Invalid eta; must be greater than 1.'))\n",
        "                else:\n",
        "                    eta = options['eta']\n",
        "\n",
        "                if 'c1' not in options.keys():\n",
        "                    c1 = 1e-4\n",
        "                elif options['c1'] >= 1 or options['c1'] <= 0:\n",
        "                    raise (ValueError('Invalid c1; must be strictly between 0 and 1.'))\n",
        "                else:\n",
        "                    c1 = options['c1']\n",
        "\n",
        "                if 'c2' not in options.keys():\n",
        "                    c2 = 0.9\n",
        "                elif options['c2'] >= 1 or options['c2'] <= 0:\n",
        "                    raise (ValueError('Invalid c2; must be strictly between 0 and 1.'))\n",
        "                elif options['c2'] <= c1:\n",
        "                    raise (ValueError('Invalid c2; must be strictly larger than c1.'))\n",
        "                else:\n",
        "                    c2 = options['c2']\n",
        "\n",
        "                if 'max_ls' not in options.keys():\n",
        "                    max_ls = 10\n",
        "                elif options['max_ls'] <= 0:\n",
        "                    raise (ValueError('Invalid max_ls; must be positive.'))\n",
        "                else:\n",
        "                    max_ls = options['max_ls']\n",
        "\n",
        "                if 'interpolate' not in options.keys():\n",
        "                    interpolate = True\n",
        "                else:\n",
        "                    interpolate = options['interpolate']\n",
        "\n",
        "                if 'inplace' not in options.keys():\n",
        "                    inplace = True\n",
        "                else:\n",
        "                    inplace = options['inplace']\n",
        "\n",
        "                if 'ls_debug' not in options.keys():\n",
        "                    ls_debug = False\n",
        "                else:\n",
        "                    ls_debug = options['ls_debug']\n",
        "\n",
        "            else:\n",
        "                raise (ValueError('Options are not specified; need closure evaluating function.'))\n",
        "\n",
        "            # initialize counters\n",
        "            ls_step = 0\n",
        "            grad_eval = 0  # tracks gradient evaluations\n",
        "            t_prev = 0  # old steplength\n",
        "\n",
        "            # initialize bracketing variables and flag\n",
        "            alpha = 0\n",
        "            beta = float('Inf')\n",
        "            fail = False\n",
        "\n",
        "            # initialize values for line search\n",
        "            if (interpolate):\n",
        "                F_a = F_k\n",
        "                g_a = gtd\n",
        "\n",
        "                if (torch.cuda.is_available()):\n",
        "                    F_b = torch.tensor(np.nan, dtype=dtype).cuda()\n",
        "                    g_b = torch.tensor(np.nan, dtype=dtype).cuda()\n",
        "                else:\n",
        "                    F_b = torch.tensor(np.nan, dtype=dtype)\n",
        "                    g_b = torch.tensor(np.nan, dtype=dtype)\n",
        "\n",
        "            # begin print for debug mode\n",
        "            if ls_debug:\n",
        "                print(\n",
        "                    '==================================== Begin Wolfe line search ====================================')\n",
        "                print('F(x): %.8e  g*d: %.8e' % (F_k, gtd))\n",
        "\n",
        "            # check if search direction is descent direction\n",
        "            if gtd >= 0:\n",
        "                desc_dir = False\n",
        "                if debug:\n",
        "                    print('Not a descent direction!')\n",
        "            else:\n",
        "                desc_dir = True\n",
        "\n",
        "            # store values if not in-place\n",
        "            if not inplace:\n",
        "                current_params = self._copy_params()\n",
        "\n",
        "            # update and evaluate at new point\n",
        "            self._add_update(t, d)\n",
        "            F_new = closure()\n",
        "            closure_eval += 1\n",
        "\n",
        "            # main loop\n",
        "            while True:\n",
        "                \n",
        "                # check if maximum number of line search steps have been reached\n",
        "                if ls_step >= max_ls:\n",
        "                    if inplace:\n",
        "                        self._add_update(-t, d)\n",
        "                    else:\n",
        "                        self._load_params(current_params)\n",
        "\n",
        "                    t = 0\n",
        "                    F_new = closure()\n",
        "                    F_new.backward()\n",
        "                    g_new = self._gather_flat_grad()\n",
        "                    closure_eval += 1\n",
        "                    grad_eval += 1\n",
        "                    fail = True\n",
        "                    break\n",
        "\n",
        "                # print info if debugging\n",
        "                if ls_debug:\n",
        "                    print('LS Step: %d  t: %.8e  alpha: %.8e  beta: %.8e'\n",
        "                          % (ls_step, t, alpha, beta))\n",
        "                    print('Armijo:  F(x+td): %.8e  F-c1*t*g*d: %.8e  F(x): %.8e'\n",
        "                          % (F_new, F_k + c1 * t * gtd, F_k))\n",
        "                global g_baocun\n",
        "\n",
        "                # check Armijo condition\n",
        "                if F_new > F_k + c1 * t * gtd:\n",
        "\n",
        "                    # set upper bound\n",
        "                    beta = t\n",
        "                    t_prev = t\n",
        "                    # if ls_step>=1 and ls_step < 200:\n",
        "                    #     g_new = gtd/d\n",
        "                    #     old_d =d\n",
        "                    #     bj = (matmul(g_new.t(), (g_new - g_baocun))) / (\n",
        "                    #         matmul(g_baocun.t(), g_baocun))\n",
        "                    #     d = -g_new + (bj * d)\n",
        "\n",
        "                    #     print('1',old_d,d)\n",
        "                    #     g_baocun = g_new\n",
        "                    # else:\n",
        "                    \n",
        "                    g_baocun = gtd/d#\n",
        "                    # update interpolation quantities\n",
        "                    if interpolate:\n",
        "                        F_b = F_new\n",
        "                        if torch.cuda.is_available():\n",
        "                            g_b = torch.tensor(np.nan, dtype=dtype).cuda()\n",
        "                        else:\n",
        "                            g_b = torch.tensor(np.nan, dtype=dtype)\n",
        "\n",
        "                else:\n",
        "\n",
        "                    # compute gradient\n",
        "                    F_new.backward()\n",
        "                    g_new = self._gather_flat_grad()\n",
        "                    if ls_step>1 and ls_step < 200:\n",
        "                        bj = (matmul(g_new.t(), (g_new - g_baocun))) / (\n",
        "                            matmul(g_baocun.t(), g_baocun))\n",
        "                        d = -g_new + (bj * d)\n",
        "                        # print('2')\n",
        "                        g_baocun = g_new\n",
        "                    else:\n",
        "                        g_baocun = g_new\n",
        "\n",
        "                    grad_eval += 1\n",
        "                    gtd_new = g_new.dot(d)\n",
        "\n",
        "                    # print info if debugging\n",
        "                    if ls_debug:\n",
        "                        print('Wolfe: g(x+td)*d: %.8e  c2*g*d: %.8e  gtd: %.8e'\n",
        "                              % (gtd_new, c2 * gtd, gtd))\n",
        "\n",
        "                    # check curvature condition\n",
        "                    if gtd_new < c2 * gtd:\n",
        "\n",
        "                        # set lower bound\n",
        "                        alpha = t\n",
        "                        t_prev = t\n",
        "\n",
        "                        # update interpolation quantities\n",
        "                        if interpolate:\n",
        "                            F_a = F_new\n",
        "                            g_a = gtd_new\n",
        "\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "                # compute new steplength\n",
        "\n",
        "                # if first step or not interpolating, then bisect or multiply by factor\n",
        "                if not interpolate or not is_legal(F_b):\n",
        "                    if beta == float('Inf'):\n",
        "                        t = eta * t\n",
        "                    else:\n",
        "                        t = (alpha + beta) / 2.0\n",
        "\n",
        "                # otherwise interpolate between a and b\n",
        "                else:\n",
        "                    t = polyinterp(np.array([[alpha, F_a.item(), g_a.item()], [beta, F_b.item(), g_b.item()]]))\n",
        "\n",
        "                    # if values are too extreme, adjust t\n",
        "                    if beta == float('Inf'):\n",
        "                        if t > 2 * eta * t_prev:\n",
        "                            t = 2 * eta * t_prev\n",
        "                        elif t < eta * t_prev:\n",
        "                            t = eta * t_prev\n",
        "                    else:\n",
        "                        if t < alpha + 0.2 * (beta - alpha):\n",
        "                            t = alpha + 0.2 * (beta - alpha)\n",
        "                        elif t > (beta - alpha) / 2.0:\n",
        "                            t = (beta - alpha) / 2.0\n",
        "\n",
        "                    # if we obtain nonsensical value from interpolation\n",
        "                    if t <= 0:\n",
        "                        t = (beta - alpha) / 2.0\n",
        "\n",
        "                \n",
        "                    # gtd = g_Sk.dot(d)\n",
        "                # update parameters\n",
        "                if inplace:\n",
        "                    self._add_update(t - t_prev, d)\n",
        "                else:\n",
        "                    self._load_params(current_params)\n",
        "                    self._add_update(t, d)\n",
        "\n",
        "                # evaluate closure\n",
        "                F_new = closure()\n",
        "                closure_eval += 1\n",
        "                ls_step += 1\n",
        "\n",
        "            # store Bs\n",
        "            if Bs is None:\n",
        "                Bs = (g_Sk.mul(-t)).clone()\n",
        "            else:\n",
        "                Bs.copy_(g_Sk.mul(-t))\n",
        "\n",
        "            # print final steplength\n",
        "            if ls_debug:\n",
        "                print('Final Steplength:', t)\n",
        "                print(\n",
        "                    '===================================== End Wolfe line search =====================================')\n",
        "\n",
        "            state['d'] = d\n",
        "            state['prev_flat_grad'] = prev_flat_grad\n",
        "            state['t'] = t\n",
        "            state['Bs'] = Bs\n",
        "            state['fail'] = fail\n",
        "            return F_new, g_new, t, ls_step, closure_eval, grad_eval, desc_dir, fail\n",
        "\n",
        "        else:\n",
        "\n",
        "            # perform update\n",
        "            self._add_update(t, d)\n",
        "\n",
        "            # store Bs\n",
        "            if Bs is None:\n",
        "                Bs = (g_Sk.mul(-t)).clone()\n",
        "            else:\n",
        "                Bs.copy_(g_Sk.mul(-t))\n",
        "\n",
        "            state['d'] = d\n",
        "            state['prev_flat_grad'] = prev_flat_grad\n",
        "            state['t'] = t\n",
        "            state['Bs'] = Bs\n",
        "            state['fail'] = False\n",
        "\n",
        "            return t\n",
        "\n",
        "    def step(self, p_k, g_Ok, g_Sk=None, options={}):\n",
        "        return self._step(p_k, g_Ok, g_Sk, options)\n",
        "\n",
        "\n",
        "class FullBatchLBFGS(LBFGS1):\n",
        "\n",
        "    def __init__(self, params, lr=1, history_size=10, line_search='Wolfe',\n",
        "                 dtype=torch.float, debug=False):\n",
        "        super(FullBatchLBFGS, self).__init__(params, lr, history_size, line_search,\n",
        "                                             dtype, debug)\n",
        "\n",
        "    def step(self, options=None):\n",
        "\n",
        "        # load options for damping and eps\n",
        "        if 'damping' not in options.keys():\n",
        "            damping = False\n",
        "        else:\n",
        "            damping = options['damping']\n",
        "\n",
        "        if 'eps' not in options.keys():\n",
        "            eps = 1e-2\n",
        "        else:\n",
        "            eps = options['eps']\n",
        "\n",
        "        # gather gradient\n",
        "        grad = self._gather_flat_grad()\n",
        "\n",
        "        # update curvature if after 1st iteration\n",
        "        state = self.state['global_state']\n",
        "        if state['n_iter'] > 0:\n",
        "            self.curvature_update(grad, eps, damping)\n",
        "\n",
        "        # compute search direction\n",
        "        p = self.two_loop_recursion(-grad)\n",
        "\n",
        "        # take step\n",
        "        return self._step(p, grad, options=options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o48_O55KejM3"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Full-Overlap L-BFGS Implementation with Stochastic Wolfe Line Search\n",
        "\n",
        "Demonstrates how to implement full-overlap L-BFGS with stochastic weak Wolfe line\n",
        "search without Powell damping to train a simple convolutional neural network using the \n",
        "LBFGS optimizer. Full-overlap L-BFGS is a stochastic quasi-Newton method that uses \n",
        "the same sample as the one used in the stochastic gradient to perform quasi-Newton \n",
        "updating, then resamples an entirely independent new sample in the next iteration.\n",
        "\n",
        "This implementation is CUDA-compatible.\n",
        "\n",
        "Implemented by: Hao-Jun Michael Shi and Dheevatsa Mudigere\n",
        "Last edited 10/20/20.\n",
        "\n",
        "Requirements:\n",
        "    - Keras (for CIFAR-10 dataset)\n",
        "    - NumPy\n",
        "    - PyTorch\n",
        "\n",
        "Run Command:\n",
        "    python full_overlap_lbfgs_example.py\n",
        "\n",
        "Based on stable quasi-Newton updating introduced by Schraudolph, Yu, and Gunter in\n",
        "\"A Stochastic Quasi-Newton Method for Online Convex Optimization\" (2007)\n",
        "\n",
        "\"\"\"\n",
        "from time import process_time\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/lbfgs/PyTorch-LBFGS/functions')\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from keras.datasets import cifar10 # to load dataset\n",
        "\n",
        "from utils import compute_stats, get_grad\n",
        "# from LBFGS import LBFGS\n",
        "\n",
        "# Parameters for L-BFGS training\n",
        "max_iter = 100\n",
        "ghost_batch = 128\n",
        "batch_size = 8192\n",
        "\n",
        "# Load data\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "\n",
        "X_train = np.transpose(X_train, (0, 3, 1, 2))\n",
        "X_test = np.transpose(X_test, (0, 3, 1, 2))\n",
        "# class Alexnet(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(Alexnet, self).__init__()\n",
        "#         self.conv1 = nn.Conv2d(3,64,3,2,1)\n",
        "#         self.pool = nn.MaxPool2d(3, 2)\n",
        "#         self.conv2 = nn.Conv2d(64,192, 5, 1, 2)\n",
        "#         self.conv3 = nn.Conv2d(192, 384, 3, 1, 1)\n",
        "#         self.conv4 = nn.Conv2d(384,256, 3, 1, 1)\n",
        "#         self.conv5 = nn.Conv2d(256,256, 3, 1, 1)\n",
        "#         self.drop = nn.Dropout(0.5)\n",
        "#         self.fc1 = nn.Linear(256*6*6, 4096)\n",
        "#         self.fc2 = nn.Linear(4096, 4096)\n",
        "#         self.fc3 = nn.Linear(4096, 1000)\n",
        "#         self.fc4 = nn.Linear(1000, 10)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         print('1')\n",
        "#         x = self.pool(F.relu(self.conv1(x)))\n",
        "#         print('1')\n",
        "#         x = self.pool(F.relu(self.conv2(x)))\n",
        "#         print('1')\n",
        "#         x = F.relu(self.conv3(x))\n",
        "#         x = F.relu(self.conv4(x))\n",
        "#         x = self.pool(F.relu(self.conv5(x)))\n",
        "#         x = x.view(-1, self.num_flat_features(x))\n",
        "#         x = F.relu(self.fc1(x))\n",
        "#         x = self.drop(F.relu(self.fc1(x)))\n",
        "#         x = self.drop(F.relu(self.fc2(x)))\n",
        "#         x = self.fc3(x)\n",
        "#         x = self.fc4(x)\n",
        "#         return x\n",
        "# Define network\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 1000)\n",
        "        self.fc2 = nn.Linear(1000, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Check cuda availability\n",
        "cuda = torch.cuda.is_available()\n",
        "    \n",
        "# Create neural network model\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(2018)\n",
        "    model = ConvNet().cuda() \n",
        "else:\n",
        "    torch.manual_seed(2018)\n",
        "    model = ConvNet()\n",
        "# Define helper functions\n",
        "\n",
        "# Forward pass\n",
        "if cuda:\n",
        "    opfun = lambda X: model.forward(torch.from_numpy(X).cuda())\n",
        "else:\n",
        "    opfun = lambda X: model.forward(torch.from_numpy(X))\n",
        "\n",
        "# Forward pass through the network given the input\n",
        "if cuda:\n",
        "    predsfun = lambda op: np.argmax(op.cpu().data.numpy(), 1)\n",
        "else:\n",
        "    predsfun = lambda op: np.argmax(op.data.numpy(), 1)\n",
        "\n",
        "# Do the forward pass, then compute the accuracy\n",
        "accfun = lambda op, y: np.mean(np.equal(predsfun(op), y.squeeze())) * 100\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = LBFGS1(model.parameters(), lr=1., history_size=10, line_search='Wolfe', debug=True)\n",
        "# optimizer2 = LBFGS2(model.parameters(), lr=1., history_size=10, line_search='Wolfe', debug=True)\n",
        "train_loss_list ,test_loss_list, test_acc_list,x_list = [],[],[],[]\n",
        "# Main training loop\n",
        "process_time()\n",
        "for n_iter in range(max_iter):\n",
        "    \n",
        "    # training mode\n",
        "    model.train()\n",
        "    \n",
        "    # sample batch\n",
        "    random_index = np.random.permutation(range(X_train.shape[0]))\n",
        "    Sk = random_index[0:batch_size]\n",
        "    \n",
        "    # compute initial gradient and objective\n",
        "    grad, obj = get_grad(optimizer, X_train[Sk], y_train[Sk], opfun)\n",
        "    \n",
        "    # two-loop recursion to compute search direction\n",
        "    p = optimizer.two_loop_recursion(-grad)\n",
        "            \n",
        "    # define closure for line search\n",
        "    def closure():              \n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        if cuda:\n",
        "            loss_fn = torch.tensor(0, dtype=torch.float).cuda()\n",
        "        else:\n",
        "            loss_fn = torch.tensor(0, dtype=torch.float)\n",
        "        \n",
        "        for subsmpl in np.array_split(Sk, max(int(batch_size / ghost_batch), 1)):\n",
        "                        \n",
        "            ops = opfun(X_train[subsmpl])\n",
        "            \n",
        "            if cuda:\n",
        "                tgts = torch.from_numpy(y_train[subsmpl]).cuda().long().squeeze()\n",
        "            else:\n",
        "                tgts = torch.from_numpy(y_train[subsmpl]).long().squeeze()\n",
        "                \n",
        "            loss_fn += F.cross_entropy(ops, tgts) * (len(subsmpl) / batch_size)\n",
        "                        \n",
        "        return loss_fn\n",
        "    \n",
        "    # perform line search step\n",
        "    options = {'closure': closure, 'current_loss': obj}\n",
        "    obj, grad, lr, _, _, _, _, _ = optimizer.step(p, grad, options=options)\n",
        "        \n",
        "    # curvature update\n",
        "    optimizer.curvature_update(grad)\n",
        "    \n",
        "    # compute statistics\n",
        "    model.eval()\n",
        "    train_loss, test_loss, test_acc = compute_stats(X_train, y_train, X_test, y_test, opfun, accfun,\n",
        "                                                    ghost_batch=128)\n",
        "    train_loss_list.append(float(train_loss))\n",
        "    test_loss_list.append(float(test_loss))\n",
        "    test_acc_list.append(float(test_acc))\n",
        "    x_list.append(n_iter+1)\n",
        "    # print data\n",
        "    print('Iter:', n_iter + 1, 'lr:', lr, 'Training Loss:', train_loss, 'Test Loss:', test_loss,\n",
        "          'Test Accuracy:', test_acc)\n",
        "print(\"运行时间是: {:9.9}s\".format(process_time()))\n",
        "plt.plot(x_list, train_loss_list, color='red', marker='o', linestyle='dashed', linewidth=2, markersize=1)\n",
        "plt.plot(x_list, test_loss_list, color='green', marker='o', linestyle='dashed', linewidth=2, markersize=1)\n",
        "plt.plot(x_list, test_acc_list, color='blue', marker='o', linestyle='dashed', linewidth=2, markersize=1)\n",
        "plt.legend(labels=('Training Loss', 'Test Loss', 'Test Accuracy'))\n",
        "plt.savefig('./a1.jpg')\n",
        "plt.show()\n",
        "import numpy as np\n",
        "train_loss_list=np.array(train_loss_list)\n",
        "np.save('train_loss_list.npy',train_loss_list)\n",
        "test_loss_list=np.array(test_loss_list)\n",
        "np.save('test_loss_list.npy',test_loss_list)\n",
        "test_acc_list=np.array(test_acc_list)\n",
        "np.save('test_acc_list.npy',test_acc_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAPURtYaHFJ3"
      },
      "outputs": [],
      "source": [
        "test_loss_list_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTGARqld1P1q"
      },
      "outputs": [],
      "source": [
        "sdsd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4y65t5b1P4Q"
      },
      "outputs": [],
      "source": [
        "!pip install backpack-for-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hU_PDcB91NcT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3Q0mmDP1P95"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/youli-jlu/PyTorch_Adam_vs_LBFGS.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sfaek8IGQej1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJSEreU0Qemm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHR4JLGuITmk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from functools import reduce\n",
        "from torch.optim.optimizer import Optimizer\n",
        "from torch.nn.functional import normalize\n",
        "from torch import  matmul\n",
        "import math\n",
        "be_verbose = False\n",
        "\n",
        "\n",
        "class LBFGSNew(Optimizer):\n",
        "    \"\"\"Implements L-BFGS algorithm.\n",
        "\n",
        "    .. warning::\n",
        "        This optimizer doesn't support per-parameter options and parameter\n",
        "        groups (there can be only one).\n",
        "\n",
        "    .. warning::\n",
        "        Right now all parameters have to be on a single device. This will be\n",
        "        improved in the future.\n",
        "\n",
        "    .. note::\n",
        "        This is a very memory intensive optimizer (it requires additional\n",
        "        ``param_bytes * (history_size + 1)`` bytes). If it doesn't fit in memory\n",
        "        try reducing the history size, or use a different algorithm.\n",
        "\n",
        "    Arguments:\n",
        "        lr (float): learning rate (fallback value when line search fails. not really needed) (default: 1)\n",
        "        max_iter (int): maximal number of iterations per optimization step\n",
        "            (default: 10)\n",
        "        max_eval (int): maximal number of function evaluations per optimization\n",
        "            step (default: max_iter * 1.25).\n",
        "        tolerance_grad (float): termination tolerance on first order optimality\n",
        "            (default: 1e-5).\n",
        "        tolerance_change (float): termination tolerance on function\n",
        "            value/parameter changes (default: 1e-9).\n",
        "        history_size (int): update history size (default: 7).\n",
        "        line_search_fn: if True, use cubic interpolation to findstep size, if False: fixed step size\n",
        "        batch_mode: True for stochastic version (default False)\n",
        "\n",
        "        Example usage for full batch mode:\n",
        "\n",
        "          optimizer = LBFGSNew(model.parameters(), history_size=7, max_iter=100, line_search_fn=True, batch_mode=False)\n",
        "\n",
        "        Example usage for batch mode (stochastic):\n",
        "\n",
        "          optimizer = LBFGSNew(net.parameters(), history_size=7, max_iter=4, line_search_fn=True,batch_mode=True)\n",
        "          Note: when using a closure(), only do backward() after checking the gradient is available,\n",
        "          Eg:\n",
        "            def closure():\n",
        "             optimizer.zero_grad()\n",
        "             outputs=net(inputs)\n",
        "             loss=criterion(outputs,labels)\n",
        "             if loss.requires_grad:\n",
        "               loss.backward()\n",
        "             return loss\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=1, max_iter=10, max_eval=None,\n",
        "                 tolerance_grad=1e-5, tolerance_change=1e-9, history_size=7,\n",
        "                 line_search_fn=False, batch_mode=False):\n",
        "        if max_eval is None:\n",
        "            max_eval = max_iter * 5 // 4\n",
        "        defaults = dict(lr=lr, max_iter=max_iter, max_eval=max_eval,\n",
        "                        tolerance_grad=tolerance_grad, tolerance_change=tolerance_change,\n",
        "                        history_size=history_size, line_search_fn=line_search_fn,\n",
        "                        batch_mode=batch_mode)\n",
        "        super(LBFGSNew, self).__init__(params, defaults)\n",
        "\n",
        "        if len(self.param_groups) != 1:\n",
        "            raise ValueError(\"LBFGS doesn't support per-parameter options \"\n",
        "                             \"(parameter groups)\")\n",
        "\n",
        "        self._params = self.param_groups[0]['params']\n",
        "        self._numel_cache = None\n",
        "\n",
        "    def _numel(self):\n",
        "        if self._numel_cache is None:\n",
        "            self._numel_cache = reduce(lambda total, p: total + p.numel(), self._params, 0)\n",
        "        return self._numel_cache\n",
        "\n",
        "    def _gather_flat_grad(self):\n",
        "        views = []\n",
        "        for p in self._params:\n",
        "            if p.grad is None:\n",
        "                view = p.data.new(p.data.numel()).zero_()\n",
        "            elif p.grad.data.is_sparse:\n",
        "                view = p.grad.data.to_dense().contiguous().view(-1)\n",
        "            else:\n",
        "                view = p.grad.data.contiguous().view(-1)\n",
        "            views.append(view)\n",
        "        return torch.cat(views, 0)\n",
        "\n",
        "    def _add_grad(self, step_size, update):\n",
        "        offset = 0\n",
        "        for p in self._params:\n",
        "            numel = p.numel()\n",
        "            # view as to avoid deprecated pointwise semantics\n",
        "            p.data.add_(update[offset:offset + numel].view_as(p.data), alpha=step_size)\n",
        "            offset += numel\n",
        "        assert offset == self._numel()\n",
        "\n",
        "    # FF copy the parameter values out, create a single vector\n",
        "    def _copy_params_out(self):\n",
        "        offset = 0\n",
        "        new_params = []\n",
        "        for p in self._params:\n",
        "            numel = p.numel()\n",
        "            new_param1 = p.data.clone().contiguous().view(-1)\n",
        "            offset += numel\n",
        "            new_params.append(new_param1)\n",
        "        assert offset == self._numel()\n",
        "        return torch.cat(new_params, 0)\n",
        "\n",
        "    # FF copy the parameter values back, dividing the vector into a list\n",
        "    def _copy_params_in(self, new_params):\n",
        "        offset = 0\n",
        "        for p in self._params:\n",
        "            numel = p.numel()\n",
        "            p.data.copy_(new_params[offset:offset + numel].view_as(p.data))\n",
        "            offset += numel\n",
        "        assert offset == self._numel()\n",
        "\n",
        "    # FF line search xk=self._params, pk=step direction, gk=gradient, alphabar=max. step size\n",
        "    def _linesearch_backtrack(self, closure, pk, gk, alphabar):\n",
        "        \"\"\"Line search (backtracking)\n",
        "\n",
        "        Arguments:\n",
        "            closure (callable): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "            pk: step direction vector\n",
        "            gk: gradient vector\n",
        "            alphabar: max step size\n",
        "        \"\"\"\n",
        "\n",
        "        # constants (FIXME) find proper values\n",
        "        # c1: large values better for small batch sizes\n",
        "        c1 = 1e-4\n",
        "        citer = 35\n",
        "        alphak = alphabar  # default return step\n",
        "\n",
        "        # state parameter\n",
        "        state = self.state[self._params[0]]\n",
        "\n",
        "        # make a copy of original params\n",
        "        xk = self._copy_params_out()\n",
        "\n",
        "        f_old = float(closure())\n",
        "        # param = param + alphak * pk\n",
        "        self._add_grad(alphak, pk)\n",
        "        f_new = float(closure())\n",
        "\n",
        "        # prod = c1 * ( alphak ) * gk^T pk = alphak * prodterm\n",
        "        s = gk\n",
        "        prodterm = c1 * (s.dot(pk))\n",
        "\n",
        "        ci = 0\n",
        "        if be_verbose:\n",
        "            print('LN %d alpha=%f fnew=%f fold=%f prod=%f' % (ci, alphak, f_new, f_old, prodterm))\n",
        "        # catch cases where f_new is NaN\n",
        "        while (ci < citer and (math.isnan(f_new) or f_new > f_old + alphak * prodterm)):\n",
        "            alphak = 0.5 * alphak\n",
        "            self._copy_params_in(xk)\n",
        "            self._add_grad(alphak, pk)\n",
        "            f_new = float(closure())\n",
        "            if be_verbose:\n",
        "                print('LN %d alpha=%f fnew=%f fold=%f' % (ci, alphak, f_new, f_old))\n",
        "            ci = ci + 1\n",
        "\n",
        "        # if the cost is not sufficiently decreased, also try -ve steps\n",
        "        if (f_old - f_new < torch.abs(prodterm)):\n",
        "            alphak1 = -alphabar\n",
        "            self._copy_params_in(xk)\n",
        "            self._add_grad(alphak1, pk)\n",
        "            f_new1 = float(closure())\n",
        "            if be_verbose:\n",
        "                print('NLN fnew=%f' % f_new1)\n",
        "            while (ci < citer and (math.isnan(f_new1) or f_new1 > f_old + alphak1 * prodterm)):\n",
        "                alphak1 = 0.5 * alphak1\n",
        "                self._copy_params_in(xk)\n",
        "                self._add_grad(alphak1, pk)\n",
        "                f_new1 = float(closure())\n",
        "                if be_verbose:\n",
        "                    print('NLN %d alpha=%f fnew=%f fold=%f' % (ci, alphak1, f_new1, f_old))\n",
        "                ci = ci + 1\n",
        "\n",
        "            if f_new1 < f_new:\n",
        "                # select -ve step\n",
        "                alphak = alphak1\n",
        "\n",
        "        # recover original params\n",
        "        self._copy_params_in(xk)\n",
        "        # update state\n",
        "        state['func_evals'] += ci\n",
        "        return alphak\n",
        "\n",
        "    # FF line search xk=self._params, pk=gradient\n",
        "    def _linesearch_cubic(self, closure, pk, step):\n",
        "        \"\"\"Line search (strong-Wolfe)\n",
        "\n",
        "        Arguments:\n",
        "            closure (callable): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "            pk: gradient vector\n",
        "            step: step size for differencing\n",
        "        \"\"\"\n",
        "\n",
        "        # constants\n",
        "        alpha1 = 10 * self.param_groups[0]['lr']  # 10.0\n",
        "        sigma = 0.1\n",
        "        rho = 0.01\n",
        "        t1 = 9\n",
        "        t2 = 0.1\n",
        "        t3 = 0.5\n",
        "        alphak = self.param_groups[0]['lr']  # default return step\n",
        "\n",
        "        # state parameter\n",
        "        state = self.state[self._params[0]]\n",
        "\n",
        "        # make a copy of original params\n",
        "        xk = self._copy_params_out()\n",
        "\n",
        "        phi_0 = float(closure())\n",
        "        tol = min(phi_0 * 0.01, 1e-6)\n",
        "\n",
        "        # xp <- xk+step. pk\n",
        "        self._add_grad(step, pk)  # FF param = param + t * grad\n",
        "        p01 = float(closure())\n",
        "        # xp <- xk-step. pk\n",
        "        self._add_grad(-2.0 * step, pk)  # FF param = param - t * grad\n",
        "        p02 = float(closure())\n",
        "\n",
        "        ##print(\"p01=\"+str(p01)+\" p02=\"+str(p02))\n",
        "        gphi_0 = (p01 - p02) / (2.0 * step)\n",
        "        ##print(\"tol=\"+str(tol)+\" phi_0=\"+str(phi_0)+\" gphi_0=\"+str(gphi_0))\n",
        "        # catch instances when step size is too small\n",
        "        if abs(gphi_0) < 1e-12:\n",
        "            return 1.0\n",
        "\n",
        "        mu = (tol - phi_0) / (rho * gphi_0)\n",
        "        # catch if mu is not finite\n",
        "        if math.isnan(mu):\n",
        "            return 1.0\n",
        "\n",
        "        ##print(\"mu=\"+str(mu))\n",
        "\n",
        "        # counting function evals\n",
        "        closure_evals = 3\n",
        "\n",
        "        ci = 1\n",
        "        alphai = alpha1  # initial value for alpha(i) : check if 0<alphai<=mu\n",
        "        alphai1 = 0.0\n",
        "        phi_alphai1 = phi_0\n",
        "        while (ci < 4):  # FIXME\n",
        "            # evalualte phi(alpha(i))=f(xk+alphai pk)\n",
        "            self._copy_params_in(xk)  # original\n",
        "            # xp <- xk+alphai. pk\n",
        "            self._add_grad(alphai, pk)  #\n",
        "            phi_alphai = float(closure())\n",
        "            if phi_alphai < tol:\n",
        "                alphak = alphai\n",
        "                if be_verbose:\n",
        "                    print(\"Linesearch: condition 0 met\")\n",
        "                break\n",
        "            if (phi_alphai > phi_0 + alphai * gphi_0) or (ci > 1 and phi_alphai >= phi_alphai1):\n",
        "                # ai=alphai1, bi=alphai bracket\n",
        "                if be_verbose:\n",
        "                    print(\"bracket \" + str(alphai1) + \",\" + str(alphai))\n",
        "                alphak = self._linesearch_zoom(closure, xk, pk, alphai1, alphai, phi_0, gphi_0, sigma, rho, t1, t2, t3,\n",
        "                                               step)\n",
        "                if be_verbose:\n",
        "                    print(\"Linesearch: condition 1 met\")\n",
        "                break\n",
        "\n",
        "            # evaluate grad(phi(alpha(i))) */\n",
        "            # note that self._params already is xk+alphai. pk, so only add the missing term\n",
        "            # xp <- xk+(alphai+step). pk\n",
        "            self._add_grad(step, pk)  # FF param = param - t * grad\n",
        "            p01 = float(closure())\n",
        "            # xp <- xk+(alphai-step). pk\n",
        "            self._add_grad(-2.0 * step, pk)  # FF param = param - t * grad\n",
        "            p02 = float(closure())\n",
        "            gphi_i = (p01 - p02) / (2.0 * step);\n",
        "\n",
        "            if (abs(gphi_i) <= -sigma * gphi_0):\n",
        "                alphak = alphai\n",
        "                if be_verbose:\n",
        "                    print(\"Linesearch: condition 2 met\")\n",
        "                break\n",
        "\n",
        "            if gphi_i >= 0.0:\n",
        "                # ai=alphai, bi=alphai1 bracket\n",
        "                if be_verbose:\n",
        "                    print(\"bracket \" + str(alphai) + \",\" + str(alphai1))\n",
        "                alphak = self._linesearch_zoom(closure, xk, pk, alphai, alphai1, phi_0, gphi_0, sigma, rho, t1, t2, t3,\n",
        "                                               step)\n",
        "                if be_verbose:\n",
        "                    print(\"Linesearch: condition 3 met\")\n",
        "                break\n",
        "            # else preserve old values\n",
        "            if (mu <= 2.0 * alphai - alphai1):\n",
        "                alphai1 = alphai\n",
        "                alphai = mu\n",
        "            else:\n",
        "                # choose by interpolation in [2*alphai-alphai1,min(mu,alphai+t1*(alphai-alphai1)]\n",
        "                p01 = 2.0 * alphai - alphai1;\n",
        "                p02 = min(mu, alphai + t1 * (alphai - alphai1))\n",
        "                alphai = self._cubic_interpolate(closure, xk, pk, p01, p02, step)\n",
        "\n",
        "            phi_alphai1 = phi_alphai;\n",
        "            # update function evals\n",
        "            closure_evals += 3\n",
        "            ci = ci + 1\n",
        "\n",
        "        # recover original params\n",
        "        self._copy_params_in(xk)\n",
        "        # update state\n",
        "        state['func_evals'] += closure_evals\n",
        "        return alphak\n",
        "\n",
        "    def _cubic_interpolate(self, closure, xk, pk, a, b, step):\n",
        "        \"\"\" Cubic interpolation within interval [a,b] or [b,a] (a>b is possible)\n",
        "\n",
        "           Arguments:\n",
        "            closure (callable): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "            xk: copy of parameter values\n",
        "            pk: gradient vector\n",
        "            a/b:  interval for interpolation\n",
        "            step: step size for differencing\n",
        "        \"\"\"\n",
        "\n",
        "        self._copy_params_in(xk)\n",
        "\n",
        "        # state parameter\n",
        "        state = self.state[self._params[0]]\n",
        "        # count function evals\n",
        "        closure_evals = 0\n",
        "\n",
        "        # xp <- xk+a. pk\n",
        "        self._add_grad(a, pk)  # FF param = param + t * grad\n",
        "        f0 = float(closure())\n",
        "        # xp <- xk+(a+step). pk\n",
        "        self._add_grad(step, pk)  # FF param = param + t * grad\n",
        "        p01 = float(closure())\n",
        "        # xp <- xk+(a-step). pk\n",
        "        self._add_grad(-2.0 * step, pk)  # FF param = param - t * grad\n",
        "        p02 = float(closure())\n",
        "        f0d = (p01 - p02) / (2.0 * step)\n",
        "\n",
        "        # xp <- xk+b. pk\n",
        "        self._add_grad(-a + step + b, pk)  # FF param = param + t * grad\n",
        "        f1 = float(closure())\n",
        "        # xp <- xk+(b+step). pk\n",
        "        self._add_grad(step, pk)  # FF param = param + t * grad\n",
        "        p01 = float(closure())\n",
        "        # xp <- xk+(b-step). pk\n",
        "        self._add_grad(-2.0 * step, pk)  # FF param = param - t * grad\n",
        "        p02 = float(closure())\n",
        "        f1d = (p01 - p02) / (2.0 * step)\n",
        "\n",
        "        closure_evals = 6\n",
        "\n",
        "        aa = 3.0 * (f0 - f1) / (b - a) + f1d - f0d\n",
        "        p01 = aa * aa - f0d * f1d\n",
        "        if (p01 > 0.0):\n",
        "            cc = math.sqrt(p01)\n",
        "            # print('f0='+str(f0d)+' f1='+str(f1d)+' cc='+str(cc))\n",
        "            if (f1d - f0d + 2.0 * cc) == 0.0:\n",
        "                return (a + b) * 0.5\n",
        "            z0 = b - (f1d + cc - aa) * (b - a) / (f1d - f0d + 2.0 * cc)\n",
        "            aa = max(a, b)\n",
        "            cc = min(a, b)\n",
        "            if z0 > aa or z0 < cc:\n",
        "                fz0 = f0 + f1\n",
        "            else:\n",
        "                # xp <- xk+(a+z0*(b-a))*pk\n",
        "                self._add_grad(-b + step + a + z0 * (b - a), pk)  # FF param = param + t * grad\n",
        "                fz0 = float(closure())\n",
        "                closure_evals += 1\n",
        "\n",
        "            # update state\n",
        "            state['func_evals'] += closure_evals\n",
        "\n",
        "            if f0 < f1 and f0 < fz0:\n",
        "                return a\n",
        "\n",
        "            if f1 < fz0:\n",
        "                return b\n",
        "            # else\n",
        "            return z0\n",
        "        else:\n",
        "\n",
        "            # update state\n",
        "            state['func_evals'] += closure_evals\n",
        "\n",
        "            if f0 < f1:\n",
        "                return a\n",
        "            else:\n",
        "                return b\n",
        "\n",
        "        # update state\n",
        "        state['func_evals'] += closure_evals\n",
        "\n",
        "        # fallback value\n",
        "        return (a + b) * 0.5\n",
        "\n",
        "    # FF bracket [a,b]\n",
        "    # xk: copy of parameters, use it to refresh self._param\n",
        "    def _linesearch_zoom(self, closure, xk, pk, a, b, phi_0, gphi_0, sigma, rho, t1, t2, t3, step):\n",
        "        \"\"\"Zoom step in line search\n",
        "\n",
        "        Arguments:\n",
        "            closure (callable): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "            xk: copy of parameter values\n",
        "            pk: gradient vector\n",
        "            a/b:  bracket interval for line search,\n",
        "            phi_0: phi(0)\n",
        "            gphi_0: grad(phi(0))\n",
        "            sigma,rho,t1,t2,t3: line search parameters (from Fletcher)\n",
        "            step: step size for differencing\n",
        "        \"\"\"\n",
        "\n",
        "        # state parameter\n",
        "        state = self.state[self._params[0]]\n",
        "        # count function evals\n",
        "        closure_evals = 0\n",
        "\n",
        "        aj = a\n",
        "        bj = b\n",
        "        ci = 0\n",
        "        found_step = False\n",
        "        while ci < 4:  # FIXME original 10\n",
        "            # choose alphaj from [a+t2(b-a),b-t3(b-a)]\n",
        "            p01 = aj + t2 * (bj - aj)\n",
        "            p02 = bj - t3 * (bj - aj)\n",
        "            alphaj = self._cubic_interpolate(closure, xk, pk, p01, p02, step)\n",
        "\n",
        "            # evaluate phi(alphaj)\n",
        "            self._copy_params_in(xk)\n",
        "            # xp <- xk+alphaj. pk\n",
        "            self._add_grad(alphaj, pk)  # FF param = param + t * grad\n",
        "            phi_j = float(closure())\n",
        "\n",
        "            # evaluate phi(aj)\n",
        "            # xp <- xk+aj. pk\n",
        "            self._add_grad(-alphaj + aj, pk)  # FF param = param + t * grad\n",
        "            phi_aj = float(closure())\n",
        "\n",
        "            closure_evals += 2\n",
        "\n",
        "            if (phi_j > phi_0 + rho * alphaj * gphi_0) or phi_j >= phi_aj:\n",
        "                bj = alphaj  # aj is unchanged\n",
        "            else:\n",
        "                # evaluate grad(alphaj)\n",
        "                # xp <- xk+(alphaj+step). pk\n",
        "                self._add_grad(-aj + alphaj + step, pk)  # FF param = param + t * grad\n",
        "                p01 = float(closure())\n",
        "                # xp <- xk+(alphaj-step). pk\n",
        "                self._add_grad(-2.0 * step, pk)  # FF param = param + t * grad\n",
        "                p02 = float(closure())\n",
        "                gphi_j = (p01 - p02) / (2.0 * step)\n",
        "\n",
        "                closure_evals += 2\n",
        "\n",
        "                # termination due to roundoff/other errors pp. 38, Fletcher\n",
        "                if (aj - alphaj) * gphi_j <= step:\n",
        "                    alphak = alphaj\n",
        "                    found_step = True\n",
        "                    break\n",
        "\n",
        "                if abs(gphi_j) <= -sigma * gphi_0:\n",
        "                    alphak = alphaj\n",
        "                    found_step = True\n",
        "                    break\n",
        "\n",
        "                if gphi_j * (bj - aj) >= 0.0:\n",
        "                    bj = aj\n",
        "                # else bj is unchanged\n",
        "                aj = alphaj\n",
        "\n",
        "            ci = ci + 1\n",
        "\n",
        "        if not found_step:\n",
        "            alphak = alphaj\n",
        "\n",
        "        # update state\n",
        "        state['func_evals'] += closure_evals\n",
        "\n",
        "        return alphak\n",
        "\n",
        "    def step(self, closure):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "\n",
        "        Arguments:\n",
        "            closure (callable): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        assert len(self.param_groups) == 1\n",
        "\n",
        "        group = self.param_groups[0]\n",
        "        lr = group['lr']\n",
        "        max_iter = group['max_iter']\n",
        "        max_eval = group['max_eval']\n",
        "        tolerance_grad = group['tolerance_grad']\n",
        "        tolerance_change = group['tolerance_change']\n",
        "        line_search_fn = group['line_search_fn']\n",
        "        history_size = group['history_size']\n",
        "\n",
        "        batch_mode = group['batch_mode']\n",
        "\n",
        "        # NOTE: LBFGS has only global state, but we register it as state for\n",
        "        # the first param, because this helps with casting in load_state_dict\n",
        "        state = self.state[self._params[0]]\n",
        "        state.setdefault('func_evals', 0)\n",
        "        state.setdefault('n_iter', 0)\n",
        "        # ---------------------------------------------------\n",
        "\n",
        "        first_total = 0.001\n",
        "        orig_loss = closure()\n",
        "        flat_grad_first = self._gather_flat_grad()\n",
        "        abs_grad_sum = flat_grad_first.abs().sum()\n",
        "        if abs_grad_sum <= first_total:\n",
        "            return orig_loss\n",
        "        d = -flat_grad_first\n",
        "        torch.set_grad_enabled(False)\n",
        "        # if not batch_mode:\n",
        "        t = self._linesearch_cubic(closure, d, 1e-6)\n",
        "        torch.set_grad_enabled(True)\n",
        "\n",
        "        if math.isnan(t):\n",
        "            print('Warning: stepsize nan')\n",
        "            t = lr\n",
        "        self._add_grad(t, d)  # FF param = param + t * d\n",
        "        maxk = 200\n",
        "        k = 0\n",
        "        grad_list = []\n",
        "        num_list = []\n",
        "        flat_grad_first = normalize(flat_grad_first, p=2.0, dim=0)\n",
        "        second_out = 1000\n",
        "        while k < maxk:\n",
        "            loss = float(closure())\n",
        "            flat_grad = self._gather_flat_grad()\n",
        "            abs_grad_sum = flat_grad.abs().sum()\n",
        "            if math.isnan(abs_grad_sum):\n",
        "                print('Warning: gradient nan')\n",
        "                break\n",
        "            if abs_grad_sum <= second_out:\n",
        "                break\n",
        "            flat_grad = normalize(flat_grad, p=2.0, dim=0)\n",
        "            bj = (matmul(flat_grad.t(), (flat_grad - flat_grad_first))) / (matmul(flat_grad_first.t(), flat_grad_first))\n",
        "            if len(grad_list) < 3:\n",
        "                grad_list.append(flat_grad - flat_grad_first)\n",
        "            else:\n",
        "                grad_list.append(flat_grad - flat_grad_first)\n",
        "                grad_list.pop(0)\n",
        "            flat_grad_first = flat_grad\n",
        "            d = -flat_grad + (bj * d)\n",
        "            d = normalize(d, p=2.0, dim=0)\n",
        "            torch.set_grad_enabled(False)\n",
        "            t = self._linesearch_cubic(closure, d, 1e-6)\n",
        "            torch.set_grad_enabled(True)\n",
        "            if len(num_list) < 3:\n",
        "                num_list.append(-t * d)\n",
        "            else:\n",
        "                num_list.append(-t * d)\n",
        "                num_list.pop(0)\n",
        "            self._add_grad(t, d)  # FF param = param + t * d\n",
        "            k += 1\n",
        "        if len(grad_list)>=1:\n",
        "            yk_1 = grad_list[0]\n",
        "            rk = (matmul(num_list[0].t(), yk_1)) / (matmul(yk_1.t(), yk_1))\n",
        "            # del yk_1\n",
        "        else:\n",
        "            rk =1\n",
        "        # del num_list,grad_list,flat_grad_first,t,d,abs_grad_sum,flat_grad,bj,orig_loss\n",
        "        # ----------------------------------------------------\n",
        "        # evaluate initial f(x) and df/dx\n",
        "        orig_loss = closure()\n",
        "        loss = float(orig_loss)\n",
        "        current_evals = 1\n",
        "        state['func_evals'] += 1\n",
        "\n",
        "        flat_grad = self._gather_flat_grad()\n",
        "        abs_grad_sum = flat_grad.abs().sum()\n",
        "\n",
        "        if abs_grad_sum <= tolerance_grad:\n",
        "            return orig_loss\n",
        "\n",
        "        # tensors cached in state (for tracing)\n",
        "        d = state.get('d')\n",
        "        t = state.get('t')\n",
        "        old_dirs = state.get('old_dirs')\n",
        "        old_stps = state.get('old_stps')\n",
        "        H_diag = state.get('H_diag')\n",
        "        prev_flat_grad = state.get('prev_flat_grad')\n",
        "        prev_loss = state.get('prev_loss')\n",
        "\n",
        "        n_iter = 0\n",
        "\n",
        "        if batch_mode:\n",
        "            alphabar = lr\n",
        "            lm0 = 1e-6\n",
        "\n",
        "        # optimize for a max of max_iter iterations\n",
        "        grad_nrm = flat_grad.norm().item()\n",
        "        while n_iter < max_iter and not math.isnan(grad_nrm):\n",
        "            # keep track of nb of iterations\n",
        "            n_iter += 1\n",
        "            state['n_iter'] += 1\n",
        "\n",
        "            ############################################################\n",
        "            # compute gradient descent direction\n",
        "            ############################################################\n",
        "            if state['n_iter'] == 1:\n",
        "                d = flat_grad.neg()\n",
        "                old_dirs = []\n",
        "                old_stps = []\n",
        "                H_diag = rk\n",
        "                if batch_mode:\n",
        "                    running_avg = torch.zeros_like(flat_grad.data)\n",
        "                    running_avg_sq = torch.zeros_like(flat_grad.data)\n",
        "            else:\n",
        "                if batch_mode:\n",
        "                    running_avg = state.get('running_avg')\n",
        "                    running_avg_sq = state.get('running_avg_sq')\n",
        "                    if running_avg is None:\n",
        "                        running_avg = torch.zeros_like(flat_grad.data)\n",
        "                        running_avg_sq = torch.zeros_like(flat_grad.data)\n",
        "\n",
        "                # do lbfgs update (update memory)\n",
        "                # what happens if current and prev grad are equal, ||y||->0 ??\n",
        "                y = flat_grad.sub(prev_flat_grad)\n",
        "\n",
        "                s = d.mul(t)\n",
        "\n",
        "                if batch_mode:  # y = y+ lm0 * s, to have a trust region\n",
        "                    y.add_(s, alpha=lm0)\n",
        "\n",
        "                ys = y.dot(s)  # y^T*s\n",
        "                sn = s.norm().item()  # ||s||\n",
        "                # FIXME batch_changed does not work for full batch mode (data might be the same)\n",
        "                batch_changed = batch_mode and (n_iter == 1 and state['n_iter'] > 1)\n",
        "                if batch_changed:  # batch has changed\n",
        "                    # online estimate of mean,variance of gradient (inter-batch, not intra-batch)\n",
        "                    # newmean <- oldmean + (grad - oldmean)/niter\n",
        "                    # moment <- oldmoment + (grad-oldmean)(grad-newmean)\n",
        "                    # variance = moment/(niter-1)\n",
        "\n",
        "                    g_old = flat_grad.clone()\n",
        "                    g_old.add_(running_avg, alpha=-1.0)  # grad-oldmean\n",
        "                    running_avg.add_(g_old, alpha=1.0 / state['n_iter'])  # newmean\n",
        "                    g_new = flat_grad.clone()\n",
        "                    g_new.add_(running_avg, alpha=-1.0)  # grad-newmean\n",
        "                    running_avg_sq.addcmul_(g_new, g_old, value=1)  # +(grad-newmean)(grad-oldmean)\n",
        "                    alphabar = 1 / (1 + running_avg_sq.sum() / ((state['n_iter'] - 1) * (grad_nrm)))\n",
        "                    if be_verbose:\n",
        "                        print('iter %d |mean| %f |var| %f ||grad|| %f step %f y^Ts %f alphabar=%f' % (\n",
        "                            state['n_iter'], running_avg.sum(), running_avg_sq.sum() / (state['n_iter'] - 1), grad_nrm,\n",
        "                            t,\n",
        "                            ys, alphabar))\n",
        "\n",
        "                if ys > 1e-10 * sn * sn and not batch_changed:\n",
        "                    # updating memory (only when we have y within a single batch)\n",
        "                    if len(old_dirs) == history_size:\n",
        "                        # shift history by one (limited-memory)\n",
        "                        old_dirs.pop(0)\n",
        "                        old_stps.pop(0)\n",
        "\n",
        "                    # store new direction/step\n",
        "                    old_dirs.append(y)\n",
        "                    old_stps.append(s)\n",
        "\n",
        "                    # update scale of initial Hessian approximation\n",
        "                    H_diag = ys / y.dot(y)  # (y*y)\n",
        "\n",
        "                if math.isnan(H_diag):\n",
        "                    print('Warning H_diag nan')\n",
        "\n",
        "                # compute the approximate (L-BFGS) inverse Hessian\n",
        "                # multiplied by the gradient\n",
        "                num_old = len(old_dirs)\n",
        "\n",
        "                if 'ro' not in state:\n",
        "                    state['ro'] = [None] * history_size\n",
        "                    state['al'] = [None] * history_size\n",
        "                ro = state['ro']\n",
        "                al = state['al']\n",
        "\n",
        "                for i in range(num_old):\n",
        "                    ro[i] = 1. / old_dirs[i].dot(old_stps[i])\n",
        "\n",
        "                # iteration in L-BFGS loop collapsed to use just one buffer\n",
        "                q = flat_grad.neg()\n",
        "                for i in range(num_old - 1, -1, -1):\n",
        "                    al[i] = old_stps[i].dot(q) * ro[i]\n",
        "                    q.add_(old_dirs[i], alpha=-al[i])\n",
        "\n",
        "                # multiply by initial Hessian\n",
        "                # r/d is the final direction\n",
        "                d = r = torch.mul(q, H_diag)\n",
        "                for i in range(num_old):\n",
        "                    be_i = old_dirs[i].dot(r) * ro[i]\n",
        "                    r.add_(old_stps[i], alpha=al[i] - be_i)\n",
        "\n",
        "            if prev_flat_grad is None:\n",
        "                prev_flat_grad = flat_grad.clone()\n",
        "\n",
        "            else:\n",
        "                prev_flat_grad.copy_(flat_grad)\n",
        "\n",
        "            prev_loss = loss\n",
        "\n",
        "            ############################################################\n",
        "            # compute step length\n",
        "            ############################################################\n",
        "            # reset initial guess for step size\n",
        "            if state['n_iter'] == 1:\n",
        "                t = min(1., 1. / abs_grad_sum) * lr\n",
        "            else:\n",
        "                t = lr\n",
        "\n",
        "            # directional derivative\n",
        "            gtd = flat_grad.dot(d)  # g * d\n",
        "\n",
        "            if math.isnan(gtd.item()):\n",
        "                print('Warning grad norm infinite')\n",
        "                print('iter %d' % state['n_iter'])\n",
        "                print('||grad||=%f' % grad_nrm)\n",
        "                print('||d||=%f' % d.norm().item())\n",
        "            # optional line search: user function\n",
        "            ls_func_evals = 0\n",
        "            if line_search_fn:\n",
        "                # perform line search, using user function\n",
        "                ##raise RuntimeError(\"line search function is not supported yet\")\n",
        "                # FF#################################\n",
        "                # Note: we disable gradient calculation during line search\n",
        "                # because it is not needed\n",
        "                torch.set_grad_enabled(False)\n",
        "                if not batch_mode:\n",
        "                    t = self._linesearch_cubic(closure, d, 1e-6)\n",
        "                else:\n",
        "                    t = self._linesearch_backtrack(closure, d, flat_grad, alphabar)\n",
        "                torch.set_grad_enabled(True)\n",
        "\n",
        "                if math.isnan(t):\n",
        "                    print('Warning: stepsize nan')\n",
        "                    t = lr\n",
        "                self._add_grad(t, d)  # FF param = param + t * d\n",
        "                if be_verbose:\n",
        "                    print('step size=%f' % (t))\n",
        "                # FF#################################\n",
        "            else:\n",
        "                # FF Here, t = stepsize,  d = -grad, in cache\n",
        "                # no line search, simply move with fixed-step\n",
        "                self._add_grad(t, d)  # FF param = param + t * d\n",
        "            if n_iter != max_iter:\n",
        "                # re-evaluate function only if not in last iteration\n",
        "                # the reason we do this: in a stochastic setting,\n",
        "                # no use to re-evaluate that function here\n",
        "                loss = float(closure())\n",
        "                flat_grad = self._gather_flat_grad()\n",
        "                abs_grad_sum = flat_grad.abs().sum()\n",
        "                if math.isnan(abs_grad_sum):\n",
        "                    print('Warning: gradient nan')\n",
        "                    break\n",
        "                ls_func_evals = 1\n",
        "\n",
        "            # update func eval\n",
        "            current_evals += ls_func_evals\n",
        "            state['func_evals'] += ls_func_evals\n",
        "\n",
        "            ############################################################\n",
        "            # check conditions\n",
        "            ############################################################\n",
        "            if n_iter == max_iter:\n",
        "                break\n",
        "\n",
        "            if current_evals >= max_eval:\n",
        "                break\n",
        "\n",
        "            if abs_grad_sum <= tolerance_grad:\n",
        "                break\n",
        "\n",
        "            if gtd > -tolerance_change:\n",
        "                break\n",
        "\n",
        "            if d.mul(t).abs_().sum() <= tolerance_change:\n",
        "                break\n",
        "\n",
        "            if abs(loss - prev_loss) < tolerance_change:\n",
        "                break\n",
        "\n",
        "        state['d'] = d\n",
        "        state['t'] = t\n",
        "        state['old_dirs'] = old_dirs\n",
        "        state['old_stps'] = old_stps\n",
        "        state['H_diag'] = H_diag\n",
        "        state['prev_flat_grad'] = prev_flat_grad\n",
        "        state['prev_loss'] = prev_loss\n",
        "\n",
        "        if batch_mode:\n",
        "            if 'running_avg' not in locals() or running_avg is None:\n",
        "                running_avg = torch.zeros_like(flat_grad.data)\n",
        "                running_avg_sq = torch.zeros_like(flat_grad.data)\n",
        "            state['running_avg'] = running_avg\n",
        "            state['running_avg_sq'] = running_avg_sq\n",
        "\n",
        "        return orig_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9XGRpkBsJ0W"
      },
      "outputs": [],
      "source": [
        "#!/home/youli/miniconda3/bin/python3\n",
        "# coding=utf8\n",
        "\"\"\"\n",
        "# Author: youli\n",
        "# Created Time : 2021-12-27 15:38:05\n",
        "\n",
        "# File Name: model_construct.py\n",
        "# Description:\n",
        "    test for Pytorch model\n",
        "\n",
        "\"\"\"\n",
        "print(f\"pytorch test\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader,TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "\n",
        "input_size = 20000\n",
        "train_size = int(input_size*0.9)\n",
        "test_size  = input_size-train_size\n",
        "batch_size = 64\n",
        "\n",
        "\n",
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# device = \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "\n",
        " \n",
        "# 搭建基于SENet的Conv Block和Identity Block的网络结构\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, in_channels, filters, stride = 1, is_1x1conv = False):\n",
        "        super(Block, self).__init__()\n",
        " \n",
        "        # 各个Stage中的每一大块中每一小块的输出维度，即channel（filter1 = filter2 = filter3 / 4）\n",
        "        filter1, filter2, filter3 = filters\n",
        " \n",
        "        self.is_1x1conv = is_1x1conv # 判断是否是Conv Block\n",
        "        self.relu = nn.ReLU(inplace = True) # RELU操作\n",
        " \n",
        "        # 第一小块， stride = 1(stage = 1) or stride = 2(stage = 2, 3, 4)\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, filter1, kernel_size = 1, stride = stride, bias = False),\n",
        "            nn.BatchNorm2d(filter1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        " \n",
        "        # 中间小块\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(filter1, filter2, kernel_size=3, stride = 1, padding = 1, bias=False),\n",
        "            nn.BatchNorm2d(filter2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        " \n",
        "        # 最后小块，不需要进行ReLu操作\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(filter2, filter3, kernel_size = 1, stride = 1, bias=False),\n",
        "            nn.BatchNorm2d(filter3),\n",
        "        )\n",
        " \n",
        "        # Conv Block的输入需要额外进行卷积和归一化操作（结合Conv Block网络图理解）\n",
        "        if is_1x1conv:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, filter3, kernel_size = 1, stride = stride, bias = False),\n",
        "                nn.BatchNorm2d(filter3)\n",
        "            )\n",
        " \n",
        "        # SENet(结合SENet的网络图理解)\n",
        "        self.se = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)), # 全局平均池化\n",
        "            nn.Conv2d(filter3, filter3 // 16, kernel_size=1), # 16表示r，filter3//16表示C/r，这里用卷积层代替全连接层\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(filter3 // 16, filter3, kernel_size=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        " \n",
        "    def forward(self, x):\n",
        "        x_shortcut = x\n",
        "        x1 = self.conv1(x) # 执行第一Block操作\n",
        "        x1 = self.conv2(x1) # 执行中间Block操作\n",
        "        x1 = self.conv3(x1) # 执行最后Block操作\n",
        " \n",
        "        x2 = self.se(x1)  # 利用SENet计算出每个通道的权重大小\n",
        "        x1 = x1 * x2  # 对原通道进行加权操作\n",
        " \n",
        "        if self.is_1x1conv:  # Conv Block进行额外的卷积归一化操作\n",
        "            x_shortcut = self.shortcut(x_shortcut)\n",
        " \n",
        "        x1 = x1 + x_shortcut  # Add操作\n",
        "        x1 = self.relu(x1)  # ReLU操作\n",
        " \n",
        "        return x1\n",
        "\n",
        "# 搭建SEResNet50\n",
        "class SEResnet(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super(SEResnet, self).__init__()\n",
        "        classes = cfg['classes']  # 分类的类别\n",
        "        num = cfg['num']  # ResNet50[3, 4, 6, 3]；Conv Block和 Identity Block的个数\n",
        " \n",
        "        # Stem Block\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size = 7, stride = 2, padding = 3, bias = False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
        "        )\n",
        " \n",
        "        # Stage1\n",
        "        filters = (64, 64, 256)  # channel\n",
        "        self.Stage1 = self._make_layer(in_channels = 64, filters = filters, num = num[0], stride = 1)\n",
        " \n",
        "        # Stage2\n",
        "        filters = (128, 128, 512) # channel\n",
        "        self.Stage2 = self._make_layer(in_channels = 256, filters = filters, num = num[1], stride = 2)\n",
        " \n",
        "        # Stage3\n",
        "        filters = (256, 256, 1024) # channel\n",
        "        self.Stage3 = self._make_layer(in_channels = 512, filters = filters, num = num[2], stride = 2)\n",
        " \n",
        "        # Stage4\n",
        "        filters = (512, 512, 2048) # channel\n",
        "        self.Stage4 = self._make_layer(in_channels = 1024, filters = filters, num = num[3], stride = 2)\n",
        " \n",
        "        # 自适应平均池化，(1, 1)表示输出的大小(H x W)\n",
        "        self.global_average_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        " \n",
        "        # 全连接层 这里可理解为网络中四个Stage后的Subsequent Processing 环节\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(2048, classes)\n",
        "        )\n",
        " \n",
        " \n",
        "    # 形成单个Stage的网络结构\n",
        "    def _make_layer(self, in_channels, filters, num, stride = 1):\n",
        "        layers = []\n",
        " \n",
        "        # Conv Block\n",
        "        block_1 = Block(in_channels, filters, stride = stride, is_1x1conv = True)\n",
        "        layers.append(block_1)\n",
        " \n",
        "        # Identity Block结构叠加; 基于[3, 4, 6, 3]\n",
        "        for i in range(1, num):\n",
        "            layers.append(Block(filters[2], filters, stride = 1, is_1x1conv = False))\n",
        " \n",
        "        # 返回Conv Block和Identity Block的集合，形成一个Stage的网络结构\n",
        "        return nn.Sequential(*layers)\n",
        " \n",
        " \n",
        "    def forward(self, x):\n",
        " \n",
        "        # Stem Block环节\n",
        "        x = self.conv1(x)\n",
        " \n",
        "        # 执行四个Stage环节\n",
        "        x = self.Stage1(x)\n",
        "        x = self.Stage2(x)\n",
        "        x = self.Stage3(x)\n",
        "        x = self.Stage4(x)\n",
        " \n",
        "        # 执行Subsequent Processing环节\n",
        "        x = self.global_average_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        " \n",
        "        return x\n",
        " \n",
        "    \n",
        "# SeResNet50的参数  （注意调用这个函数将间接调用SEResnet，这里单独编写一个函数是为了方便修改成其它ResNet网络的结构）\n",
        "def SeResNet50():\n",
        "    cfg = {\n",
        "        'num':(3, 4, 6, 3), # ResNet50，四个Stage中Block的个数（其中Conv Block为1个，剩下均为增加Identity Block）\n",
        "        'classes': (10)  # 数据集分类的个数\n",
        "    }\n",
        " \n",
        "    return SEResnet(cfg) # 调用SEResnet网络\n",
        "\n",
        "\n",
        "model = SeResNet50().to(device)\n",
        "print(model)\n",
        "\n",
        "# loss_fn = nn.MSELoss()\n",
        "loss_fn = nn.CrossEntropyLoss() \n",
        "# optimizer_lbfgs= torch.optim.LBFGS(model.parameters(), lr=1, \n",
        "#         history_size=100, max_iter=20,\n",
        "#         line_search_fn=\"strong_wolfe\"\n",
        "#         )\n",
        "optimizer_lbfgs= LBFGSNew(model.parameters(),  \n",
        "        history_size=100, max_iter=20,\n",
        "        line_search_fn=True,batch_mode=True\n",
        "        )\n",
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    lm_lbfgs=model.to(device)\n",
        "    #spacial function for LBFGS\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        x_ = Variable(X, requires_grad=True).to(device)\n",
        "        y_ = Variable(y).to(device)\n",
        "        def closure():\n",
        "            # Zero gradients\n",
        "            # optimizer.zero_grad()\n",
        "            # # Forward pass\n",
        "            # y_pred = lm_lbfgs(x_)\n",
        "            # # Compute loss\n",
        "            # loss = loss_fn(y_pred, y_)\n",
        "            # # [x.grad.data for x in model.parameters()]\n",
        "            # # Backward pass\n",
        "            # # loss.requires_grad = True\n",
        "            # loss.backward()\n",
        "            if torch.is_grad_enabled():\n",
        "                optimizer.zero_grad()\n",
        "            y_pred = lm_lbfgs(x_)\n",
        "            # print(y_pred.shape,y_.shape)\n",
        "            loss = loss_fn(y_pred, y_)\n",
        "            if loss.requires_grad:\n",
        "                loss.backward()\n",
        "            # if torch.is_grad_enabled():\n",
        "            #     optimizer.zero_grad()\n",
        "            # y_pred = lm_lbfgs(x_)\n",
        "            # loss = loss_fn(y_pred, y_)\n",
        "            # if loss.requires_grad:\n",
        "            #     loss.backward()\n",
        "            return loss\n",
        "\n",
        "        optimizer.step(closure)\n",
        "        loss=closure()\n",
        "\n",
        "        if batch % train_size == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_train\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    print(f\"Test Error: \\n  Avg loss: {test_loss:>8f} \\n\")\n",
        "    return test_loss\n",
        "\n",
        "\n",
        "# 下载训练集\n",
        "train_set = torchvision.datasets.CIFAR10(\n",
        "    root = \"data/cifar-10\", train = True,\n",
        "    download = True, transform = transforms.ToTensor()\n",
        ")\n",
        "\n",
        "# 下载测试集\n",
        "test_set = torchvision.datasets.CIFAR10(\n",
        "    root = \"data/cifar-10\", train = False,\n",
        "    download = True, transform = transforms.ToTensor()\n",
        ")\n",
        "\n",
        "train_iter = torch.utils.data.DataLoader(\n",
        "    train_set, batch_size = batch_size, shuffle = True, num_workers = 2\n",
        ")\n",
        "\n",
        "test_iter = torch.utils.data.DataLoader(\n",
        "    test_set, batch_size = batch_size, shuffle = True, num_workers = 2\n",
        ")\n",
        "\n",
        "\n",
        "# training\n",
        "\n",
        "opt_label='lbfgs_original-t20-t20'\n",
        "epochs = 50\n",
        "print(f\"test for {opt_label}\")\n",
        "optimizer=optimizer_lbfgs\n",
        "loss_train=[]\n",
        "loss_test=[]\n",
        "\n",
        "t1= time.perf_counter()\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_iter, model, loss_fn, optimizer)\n",
        "    loss_train+=[\n",
        "            test(train_iter, model, loss_fn)\n",
        "            ]\n",
        "    loss_test+=[\n",
        "            test(test_iter, model, loss_fn)\n",
        "            ]\n",
        "    print(\"Done!\")\n",
        "\n",
        "t2= time.perf_counter()\n",
        "print(\"Elapsed time: \", t2- t1)\n",
        "record=pd.DataFrame({\n",
        "    'epochs':np.arange(epochs)\n",
        "    ,'loss_train':np.array(loss_train)\n",
        "    ,'loss_test':np.array(loss_test)\n",
        "    })\n",
        "record.to_csv(f\"{opt_label}\",sep=' ')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), f\"model{opt_label}.pth\")\n",
        "print(f\"Saved PyTorch Model State to model{opt_label}.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1FSeVrKFbiU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from functools import reduce\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "import math\n",
        "\n",
        "be_verbose=False\n",
        "\n",
        "class LBFGSNew(Optimizer):\n",
        "    \"\"\"Implements L-BFGS algorithm.\n",
        "\n",
        "    .. warning::\n",
        "        This optimizer doesn't support per-parameter options and parameter\n",
        "        groups (there can be only one).\n",
        "\n",
        "    .. warning::\n",
        "        Right now all parameters have to be on a single device. This will be\n",
        "        improved in the future.\n",
        "\n",
        "    .. note::\n",
        "        This is a very memory intensive optimizer (it requires additional\n",
        "        ``param_bytes * (history_size + 1)`` bytes). If it doesn't fit in memory\n",
        "        try reducing the history size, or use a different algorithm.\n",
        "\n",
        "    Arguments:\n",
        "        lr (float): learning rate (fallback value when line search fails. not really needed) (default: 1)\n",
        "        max_iter (int): maximal number of iterations per optimization step\n",
        "            (default: 10)\n",
        "        max_eval (int): maximal number of function evaluations per optimization\n",
        "            step (default: max_iter * 1.25).\n",
        "        tolerance_grad (float): termination tolerance on first order optimality\n",
        "            (default: 1e-5).\n",
        "        tolerance_change (float): termination tolerance on function\n",
        "            value/parameter changes (default: 1e-9).\n",
        "        history_size (int): update history size (default: 7).\n",
        "        line_search_fn: if True, use cubic interpolation to findstep size, if False: fixed step size\n",
        "        batch_mode: True for stochastic version (default False)\n",
        "\n",
        "        Example usage for full batch mode:\n",
        "\n",
        "          optimizer = LBFGSNew(model.parameters(), history_size=7, max_iter=100, line_search_fn=True, batch_mode=False)\n",
        "\n",
        "        Example usage for batch mode (stochastic):\n",
        "\n",
        "          optimizer = LBFGSNew(net.parameters(), history_size=7, max_iter=4, line_search_fn=True,batch_mode=True)\n",
        "          Note: when using a closure(), only do backward() after checking the gradient is available,\n",
        "          Eg: \n",
        "            def closure():\n",
        "             optimizer.zero_grad()\n",
        "             outputs=net(inputs)\n",
        "             loss=criterion(outputs,labels)\n",
        "             if loss.requires_grad:\n",
        "               loss.backward()\n",
        "             return loss\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=1, max_iter=10, max_eval=None,\n",
        "                 tolerance_grad=1e-5, tolerance_change=1e-9, history_size=7,\n",
        "                 line_search_fn=False, batch_mode=False):\n",
        "        if max_eval is None:\n",
        "            max_eval = max_iter * 5 // 4\n",
        "        defaults = dict(lr=lr, max_iter=max_iter, max_eval=max_eval,\n",
        "                        tolerance_grad=tolerance_grad, tolerance_change=tolerance_change,\n",
        "                        history_size=history_size, line_search_fn=line_search_fn,\n",
        "                        batch_mode=batch_mode)\n",
        "        super(LBFGSNew, self).__init__(params, defaults)\n",
        "\n",
        "        if len(self.param_groups) != 1:\n",
        "            raise ValueError(\"LBFGS doesn't support per-parameter options \"\n",
        "                             \"(parameter groups)\")\n",
        "\n",
        "        self._params = self.param_groups[0]['params']\n",
        "        self._numel_cache = None\n",
        "        torch.set_grad_enabled(True)\n",
        "    def _numel(self):\n",
        "        if self._numel_cache is None:\n",
        "            self._numel_cache = reduce(lambda total, p: total + p.numel(), self._params, 0)\n",
        "        return self._numel_cache\n",
        "\n",
        "    def _gather_flat_grad(self):\n",
        "        views = []\n",
        "        for p in self._params:\n",
        "            if p.grad is None:\n",
        "                view = p.data.new(p.data.numel()).zero_()\n",
        "            elif p.grad.data.is_sparse:\n",
        "                view = p.grad.data.to_dense().contiguous().view(-1)\n",
        "            else:\n",
        "                view = p.grad.data.contiguous().view(-1)\n",
        "            views.append(view)\n",
        "        return torch.cat(views, 0)\n",
        "\n",
        "    def _add_grad(self, step_size, update):\n",
        "        offset = 0\n",
        "        for p in self._params:\n",
        "            numel = p.numel()\n",
        "            # view as to avoid deprecated pointwise semantics\n",
        "            p.data.add_(update[offset:offset + numel].view_as(p.data), alpha=step_size)\n",
        "            offset += numel\n",
        "        assert offset == self._numel()\n",
        "\n",
        "    #FF copy the parameter values out, create a single vector\n",
        "    def _copy_params_out(self):\n",
        "        offset = 0\n",
        "        new_params = []\n",
        "        for p in self._params:\n",
        "            numel = p.numel()\n",
        "            new_param1=p.data.clone().contiguous().view(-1)\n",
        "            offset += numel\n",
        "            new_params.append(new_param1)\n",
        "        assert offset == self._numel()\n",
        "        return torch.cat(new_params,0)\n",
        "\n",
        "    #FF copy the parameter values back, dividing the vector into a list\n",
        "    def _copy_params_in(self,new_params):\n",
        "        offset = 0\n",
        "        for p in self._params:\n",
        "            numel = p.numel()\n",
        "            p.data.copy_(new_params[offset:offset+numel].view_as(p.data))\n",
        "            offset += numel\n",
        "        assert offset == self._numel()\n",
        "\n",
        "    #FF line search xk=self._params, pk=step direction, gk=gradient, alphabar=max. step size\n",
        "    def _linesearch_backtrack(self,closure,pk,gk,alphabar):\n",
        "        \"\"\"Line search (backtracking)\n",
        "\n",
        "        Arguments:\n",
        "            closure (callable): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "            pk: step direction vector\n",
        "            gk: gradient vector \n",
        "            alphabar: max step size\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        # constants (FIXME) find proper values\n",
        "        # c1: large values better for small batch sizes\n",
        "        c1=1e-4\n",
        "        citer=35\n",
        "        alphak=alphabar# default return step\n",
        " \n",
        "        # state parameter \n",
        "        state = self.state[self._params[0]]\n",
        "\n",
        "        # make a copy of original params\n",
        "        xk=self._copy_params_out()\n",
        "\n",
        "   \n",
        "        f_old=float(closure())\n",
        "        # param = param + alphak * pk\n",
        "        self._add_grad(alphak, pk)\n",
        "        f_new=float(closure())\n",
        "\n",
        "        # prod = c1 * ( alphak ) * gk^T pk = alphak * prodterm\n",
        "        s=gk\n",
        "        prodterm=c1*(s.dot(pk))\n",
        "\n",
        "        ci=0\n",
        "        if be_verbose:\n",
        "         print('LN %d alpha=%f fnew=%f fold=%f prod=%f'%(ci,alphak,f_new,f_old,prodterm))\n",
        "        # catch cases where f_new is NaN\n",
        "        while (ci<citer and (math.isnan(f_new) or  f_new > f_old + alphak*prodterm)):\n",
        "           alphak=0.5*alphak\n",
        "           self._copy_params_in(xk)\n",
        "           self._add_grad(alphak, pk)\n",
        "           f_new=float(closure())\n",
        "           if be_verbose:\n",
        "             print('LN %d alpha=%f fnew=%f fold=%f'%(ci,alphak,f_new,f_old))\n",
        "           ci=ci+1\n",
        "\n",
        "        # if the cost is not sufficiently decreased, also try -ve steps\n",
        "        if (f_old-f_new < torch.abs(prodterm)):\n",
        "          alphak1=-alphabar\n",
        "          self._copy_params_in(xk)\n",
        "          self._add_grad(alphak1, pk)\n",
        "          f_new1=float(closure())\n",
        "          if be_verbose:\n",
        "            print('NLN fnew=%f'%f_new1)\n",
        "          while (ci<citer and (math.isnan(f_new1) or  f_new1 > f_old + alphak1*prodterm)):\n",
        "             alphak1=0.5*alphak1\n",
        "             self._copy_params_in(xk)\n",
        "             self._add_grad(alphak1, pk)\n",
        "             f_new1=float(closure())\n",
        "             if be_verbose:\n",
        "               print('NLN %d alpha=%f fnew=%f fold=%f'%(ci,alphak1,f_new1,f_old))\n",
        "             ci=ci+1\n",
        "\n",
        "          if f_new1<f_new:\n",
        "            # select -ve step\n",
        "            alphak=alphak1\n",
        "\n",
        "        # recover original params\n",
        "        self._copy_params_in(xk)\n",
        "        # update state\n",
        "        state['func_evals'] += ci\n",
        "        return alphak\n",
        "\n",
        "\n",
        "\n",
        "    #FF line search xk=self._params, pk=gradient\n",
        "    def _linesearch_cubic(self,closure,pk,step):\n",
        "        \"\"\"Line search (strong-Wolfe)\n",
        "\n",
        "        Arguments:\n",
        "            closure (callable): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "            pk: gradient vector \n",
        "            step: step size for differencing \n",
        "        \"\"\"\n",
        "\n",
        "        # constants\n",
        "        alpha1=10*self.param_groups[0]['lr']#10.0\n",
        "        sigma=0.1\n",
        "        rho=0.01\n",
        "        t1=9 \n",
        "        t2=0.1\n",
        "        t3=0.5\n",
        "        alphak=self.param_groups[0]['lr']# default return step\n",
        " \n",
        "        # state parameter \n",
        "        state = self.state[self._params[0]]\n",
        "\n",
        "        # make a copy of original params\n",
        "        xk=self._copy_params_out()\n",
        "\n",
        "   \n",
        "        phi_0=float(closure())\n",
        "        tol=min(phi_0*0.01,1e-6)\n",
        "\n",
        "        # xp <- xk+step. pk\n",
        "        self._add_grad(step, pk) #FF param = param + t * grad \n",
        "        p01=float(closure())\n",
        "        # xp <- xk-step. pk\n",
        "        self._add_grad(-2.0*step, pk) #FF param = param - t * grad \n",
        "        p02=float(closure())\n",
        "\n",
        "        ##print(\"p01=\"+str(p01)+\" p02=\"+str(p02))\n",
        "        gphi_0=(p01-p02)/(2.0*step)\n",
        "        ##print(\"tol=\"+str(tol)+\" phi_0=\"+str(phi_0)+\" gphi_0=\"+str(gphi_0))\n",
        "        # catch instances when step size is too small \n",
        "        if abs(gphi_0)<1e-12:\n",
        "          return 1.0\n",
        "\n",
        "        mu=(tol-phi_0)/(rho*gphi_0)\n",
        "        # catch if mu is not finite\n",
        "        if math.isnan(mu):\n",
        "           return 1.0\n",
        "\n",
        "        ##print(\"mu=\"+str(mu))\n",
        "        \n",
        "        # counting function evals\n",
        "        closure_evals=3\n",
        "\n",
        "        ci=1\n",
        "        alphai=alpha1 # initial value for alpha(i) : check if 0<alphai<=mu \n",
        "        alphai1=0.0\n",
        "        phi_alphai1=phi_0\n",
        "        while (ci<4) : # FIXME\n",
        "          # evalualte phi(alpha(i))=f(xk+alphai pk)\n",
        "          self._copy_params_in(xk) # original\n",
        "          # xp <- xk+alphai. pk\n",
        "          self._add_grad(alphai, pk) #\n",
        "          phi_alphai=float(closure())\n",
        "          if phi_alphai<tol:\n",
        "             alphak=alphai \n",
        "             if be_verbose:\n",
        "              print(\"Linesearch: condition 0 met\")\n",
        "             break\n",
        "          if (phi_alphai>phi_0+alphai*gphi_0) or (ci>1 and phi_alphai>=phi_alphai1) :\n",
        "             # ai=alphai1, bi=alphai bracket\n",
        "             if be_verbose:\n",
        "              print(\"bracket \"+str(alphai1)+\",\"+str(alphai))\n",
        "             alphak=self._linesearch_zoom(closure,xk,pk,alphai1,alphai,phi_0,gphi_0,sigma,rho,t1,t2,t3,step)\n",
        "             if be_verbose:\n",
        "              print(\"Linesearch: condition 1 met\") \n",
        "             break\n",
        "\n",
        "          # evaluate grad(phi(alpha(i))) */\n",
        "          # note that self._params already is xk+alphai. pk, so only add the missing term\n",
        "          # xp <- xk+(alphai+step). pk\n",
        "          self._add_grad(step, pk) #FF param = param - t * grad \n",
        "          p01=float(closure())\n",
        "          # xp <- xk+(alphai-step). pk\n",
        "          self._add_grad(-2.0*step, pk) #FF param = param - t * grad \n",
        "          p02=float(closure())\n",
        "          gphi_i=(p01-p02)/(2.0*step);\n",
        "        \n",
        "          if (abs(gphi_i)<=-sigma*gphi_0):\n",
        "             alphak=alphai\n",
        "             if be_verbose:\n",
        "              print(\"Linesearch: condition 2 met\") \n",
        "             break\n",
        "\n",
        "          if gphi_i>=0.0 :\n",
        "             # ai=alphai, bi=alphai1 bracket\n",
        "             if be_verbose:\n",
        "              print(\"bracket \"+str(alphai)+\",\"+str(alphai1))\n",
        "             alphak=self._linesearch_zoom(closure,xk,pk,alphai,alphai1,phi_0,gphi_0,sigma,rho,t1,t2,t3,step)\n",
        "             if be_verbose:\n",
        "              print(\"Linesearch: condition 3 met\") \n",
        "             break\n",
        "          # else preserve old values\n",
        "          if (mu<=2.0*alphai-alphai1):\n",
        "             alphai1=alphai\n",
        "             alphai=mu\n",
        "          else:\n",
        "             # choose by interpolation in [2*alphai-alphai1,min(mu,alphai+t1*(alphai-alphai1)] \n",
        "            p01=2.0*alphai-alphai1;\n",
        "            p02=min(mu,alphai+t1*(alphai-alphai1))\n",
        "            alphai=self._cubic_interpolate(closure,xk,pk,p01,p02,step)\n",
        "\n",
        "\n",
        "          phi_alphai1=phi_alphai;\n",
        "          # update function evals\n",
        "          closure_evals +=3\n",
        "          ci=ci+1\n",
        "\n",
        "          \n",
        "\n",
        "\n",
        "        # recover original params\n",
        "        self._copy_params_in(xk)\n",
        "        # update state\n",
        "        state['func_evals'] += closure_evals\n",
        "        return alphak\n",
        "\n",
        "\n",
        "    def _cubic_interpolate(self,closure,xk,pk,a,b,step):\n",
        "        \"\"\" Cubic interpolation within interval [a,b] or [b,a] (a>b is possible)\n",
        "          \n",
        "           Arguments:\n",
        "            closure (callable): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "            xk: copy of parameter values \n",
        "            pk: gradient vector \n",
        "            a/b:  interval for interpolation\n",
        "            step: step size for differencing \n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        self._copy_params_in(xk)\n",
        "\n",
        "        # state parameter \n",
        "        state = self.state[self._params[0]]\n",
        "        # count function evals\n",
        "        closure_evals=0\n",
        "\n",
        "        # xp <- xk+a. pk\n",
        "        self._add_grad(a, pk) #FF param = param + t * grad \n",
        "        f0=float(closure())\n",
        "        # xp <- xk+(a+step). pk\n",
        "        self._add_grad(step, pk) #FF param = param + t * grad \n",
        "        p01=float(closure())\n",
        "        # xp <- xk+(a-step). pk\n",
        "        self._add_grad(-2.0*step, pk) #FF param = param - t * grad \n",
        "        p02=float(closure())\n",
        "        f0d=(p01-p02)/(2.0*step)\n",
        "\n",
        "        # xp <- xk+b. pk\n",
        "        self._add_grad(-a+step+b, pk) #FF param = param + t * grad \n",
        "        f1=float(closure())\n",
        "        # xp <- xk+(b+step). pk\n",
        "        self._add_grad(step, pk) #FF param = param + t * grad \n",
        "        p01=float(closure())\n",
        "        # xp <- xk+(b-step). pk\n",
        "        self._add_grad(-2.0*step, pk) #FF param = param - t * grad \n",
        "        p02=float(closure())\n",
        "        f1d=(p01-p02)/(2.0*step)\n",
        "\n",
        "        closure_evals=6\n",
        "\n",
        "        aa=3.0*(f0-f1)/(b-a)+f1d-f0d\n",
        "        p01=aa*aa-f0d*f1d\n",
        "        if (p01>0.0):\n",
        "           cc=math.sqrt(p01)\n",
        "           #print('f0='+str(f0d)+' f1='+str(f1d)+' cc='+str(cc))\n",
        "           if (f1d-f0d+2.0*cc)==0.0:\n",
        "             return (a+b)*0.5\n",
        "           z0=b-(f1d+cc-aa)*(b-a)/(f1d-f0d+2.0*cc)\n",
        "           aa=max(a,b)\n",
        "           cc=min(a,b)\n",
        "           if z0>aa or z0<cc:\n",
        "             fz0=f0+f1\n",
        "           else:\n",
        "             # xp <- xk+(a+z0*(b-a))*pk\n",
        "             self._add_grad(-b+step+a+z0*(b-a), pk) #FF param = param + t * grad \n",
        "             fz0=float(closure())\n",
        "             closure_evals +=1\n",
        "\n",
        "           # update state\n",
        "           state['func_evals'] += closure_evals\n",
        "\n",
        "           if f0<f1 and f0<fz0:\n",
        "             return a\n",
        "\n",
        "           if f1<fz0:\n",
        "             return b\n",
        "           # else\n",
        "           return z0\n",
        "        else:\n",
        "\n",
        "           # update state\n",
        "           state['func_evals'] += closure_evals\n",
        "\n",
        "           if f0<f1:\n",
        "             return a\n",
        "           else:\n",
        "             return b\n",
        "\n",
        "        # update state\n",
        "        state['func_evals'] += closure_evals\n",
        "\n",
        "        # fallback value\n",
        "        return (a+b)*0.5\n",
        "     \n",
        "\n",
        "\n",
        "\n",
        "    #FF bracket [a,b]\n",
        "    # xk: copy of parameters, use it to refresh self._param \n",
        "    def _linesearch_zoom(self,closure,xk,pk,a,b,phi_0,gphi_0,sigma,rho,t1,t2,t3,step):\n",
        "        \"\"\"Zoom step in line search\n",
        "\n",
        "        Arguments:\n",
        "            closure (callable): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "            xk: copy of parameter values \n",
        "            pk: gradient vector \n",
        "            a/b:  bracket interval for line search, \n",
        "            phi_0: phi(0)\n",
        "            gphi_0: grad(phi(0))\n",
        "            sigma,rho,t1,t2,t3: line search parameters (from Fletcher) \n",
        "            step: step size for differencing \n",
        "        \"\"\"\n",
        "\n",
        "        # state parameter \n",
        "        state = self.state[self._params[0]]\n",
        "        # count function evals\n",
        "        closure_evals=0\n",
        "\n",
        "        aj=a\n",
        "        bj=b\n",
        "        ci=0\n",
        "        found_step=False\n",
        "        while ci<4: # FIXME original 10\n",
        "           # choose alphaj from [a+t2(b-a),b-t3(b-a)]\n",
        "           p01=aj+t2*(bj-aj)\n",
        "           p02=bj-t3*(bj-aj)\n",
        "           alphaj=self._cubic_interpolate(closure,xk,pk,p01,p02,step)\n",
        "\n",
        "           # evaluate phi(alphaj)\n",
        "           self._copy_params_in(xk)\n",
        "           # xp <- xk+alphaj. pk\n",
        "           self._add_grad(alphaj, pk) #FF param = param + t * grad \n",
        "           phi_j=float(closure())\n",
        "          \n",
        "           # evaluate phi(aj)\n",
        "           # xp <- xk+aj. pk\n",
        "           self._add_grad(-alphaj+aj, pk) #FF param = param + t * grad \n",
        "           phi_aj=float(closure())\n",
        "\n",
        "           closure_evals +=2\n",
        "\n",
        "           if (phi_j>phi_0+rho*alphaj*gphi_0) or phi_j>=phi_aj :\n",
        "              bj=alphaj # aj is unchanged\n",
        "           else:\n",
        "              # evaluate grad(alphaj)\n",
        "              # xp <- xk+(alphaj+step). pk\n",
        "              self._add_grad(-aj+alphaj+step, pk) #FF param = param + t * grad \n",
        "              p01=float(closure())\n",
        "              # xp <- xk+(alphaj-step). pk\n",
        "              self._add_grad(-2.0*step, pk) #FF param = param + t * grad \n",
        "              p02=float(closure())\n",
        "              gphi_j=(p01-p02)/(2.0*step)\n",
        "        \n",
        "\n",
        "              closure_evals +=2\n",
        "\n",
        "              # termination due to roundoff/other errors pp. 38, Fletcher\n",
        "              if (aj-alphaj)*gphi_j <= step:\n",
        "                 alphak=alphaj\n",
        "                 found_step=True\n",
        "                 break\n",
        "             \n",
        "              if abs(gphi_j)<=-sigma*gphi_0 :\n",
        "                 alphak=alphaj\n",
        "                 found_step=True\n",
        "                 break\n",
        "\n",
        "              if gphi_j*(bj-aj)>=0.0:\n",
        "                 bj=aj\n",
        "              # else bj is unchanged\n",
        "              aj=alphaj\n",
        "\n",
        "\n",
        "           ci=ci+1\n",
        "        \n",
        "        if not found_step:\n",
        "          alphak=alphaj\n",
        "\n",
        "        # update state\n",
        "        state['func_evals'] += closure_evals\n",
        "\n",
        "        return alphak\n",
        "\n",
        "\n",
        "    def step(self, closure):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "\n",
        "        Arguments:\n",
        "            closure (callable): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        assert len(self.param_groups) == 1\n",
        "\n",
        "        group = self.param_groups[0]\n",
        "        lr = group['lr']\n",
        "        max_iter = group['max_iter']\n",
        "        max_eval = group['max_eval']\n",
        "        tolerance_grad = group['tolerance_grad']\n",
        "        tolerance_change = group['tolerance_change']\n",
        "        line_search_fn = group['line_search_fn']\n",
        "        history_size = group['history_size']\n",
        "\n",
        "        batch_mode = group['batch_mode']\n",
        "\n",
        "\n",
        "        # NOTE: LBFGS has only global state, but we register it as state for\n",
        "        # the first param, because this helps with casting in load_state_dict\n",
        "        state = self.state[self._params[0]]\n",
        "        state.setdefault('func_evals', 0)\n",
        "        state.setdefault('n_iter', 0)\n",
        "\n",
        "\n",
        "        # evaluate initial f(x) and df/dx\n",
        "        orig_loss = closure()\n",
        "        loss = float(orig_loss)\n",
        "        current_evals = 1\n",
        "        state['func_evals'] += 1\n",
        "\n",
        "        flat_grad = self._gather_flat_grad()\n",
        "        abs_grad_sum = flat_grad.abs().sum()\n",
        "\n",
        "        if abs_grad_sum <= tolerance_grad:\n",
        "            return orig_loss\n",
        "\n",
        "        # tensors cached in state (for tracing)\n",
        "        d = state.get('d')\n",
        "        t = state.get('t')\n",
        "        old_dirs = state.get('old_dirs')\n",
        "        old_stps = state.get('old_stps')\n",
        "        H_diag = state.get('H_diag')\n",
        "        prev_flat_grad = state.get('prev_flat_grad')\n",
        "        prev_loss = state.get('prev_loss')\n",
        "\n",
        "        n_iter = 0\n",
        "\n",
        "        if batch_mode:\n",
        "          alphabar=lr\n",
        "          lm0=1e-6\n",
        "\n",
        "        # optimize for a max of max_iter iterations\n",
        "        grad_nrm=flat_grad.norm().item()\n",
        "        while n_iter < max_iter and not math.isnan(grad_nrm):\n",
        "            # keep track of nb of iterations\n",
        "            n_iter += 1\n",
        "            state['n_iter'] += 1\n",
        "\n",
        "            ############################################################\n",
        "            # compute gradient descent direction\n",
        "            ############################################################\n",
        "            if state['n_iter'] == 1:\n",
        "                d = flat_grad.neg()\n",
        "                old_dirs = []\n",
        "                old_stps = []\n",
        "                H_diag = 1\n",
        "                if batch_mode:\n",
        "                 running_avg=torch.zeros_like(flat_grad.data)\n",
        "                 running_avg_sq=torch.zeros_like(flat_grad.data)\n",
        "            else:\n",
        "                if batch_mode:\n",
        "                 running_avg=state.get('running_avg')\n",
        "                 running_avg_sq=state.get('running_avg_sq')\n",
        "                 if running_avg is None:\n",
        "                  running_avg=torch.zeros_like(flat_grad.data)\n",
        "                  running_avg_sq=torch.zeros_like(flat_grad.data)\n",
        "\n",
        "                # do lbfgs update (update memory) \n",
        "                # what happens if current and prev grad are equal, ||y||->0 ??\n",
        "                y = flat_grad.sub(prev_flat_grad)\n",
        "\n",
        "                s = d.mul(t)\n",
        "\n",
        "                if batch_mode: # y = y+ lm0 * s, to have a trust region\n",
        "                  y.add_(s,alpha=lm0)\n",
        "\n",
        "                ys = y.dot(s)  # y^T*s\n",
        "                sn = s.norm().item()  # ||s||\n",
        "                # FIXME batch_changed does not work for full batch mode (data might be the same)\n",
        "                batch_changed= batch_mode and (n_iter==1 and state['n_iter']>1)\n",
        "                if batch_changed: # batch has changed\n",
        "                   # online estimate of mean,variance of gradient (inter-batch, not intra-batch)\n",
        "                   # newmean <- oldmean + (grad - oldmean)/niter\n",
        "                   # moment <- oldmoment + (grad-oldmean)(grad-newmean)\n",
        "                   # variance = moment/(niter-1)\n",
        "\n",
        "                   g_old=flat_grad.clone()\n",
        "                   g_old.add_(running_avg,alpha=-1.0) # grad-oldmean\n",
        "                   running_avg.add_(g_old,alpha=1.0/state['n_iter']) # newmean\n",
        "                   g_new=flat_grad.clone()\n",
        "                   g_new.add_(running_avg,alpha=-1.0) # grad-newmean\n",
        "                   running_avg_sq.addcmul_(g_new,g_old,value=1) # +(grad-newmean)(grad-oldmean)\n",
        "                   alphabar=1/(1+running_avg_sq.sum()/((state['n_iter']-1)*(grad_nrm)))\n",
        "                   if be_verbose:\n",
        "                     print('iter %d |mean| %f |var| %f ||grad|| %f step %f y^Ts %f alphabar=%f'%(state['n_iter'],running_avg.sum(),running_avg_sq.sum()/(state['n_iter']-1),grad_nrm,t,ys,alphabar))\n",
        "\n",
        "\n",
        "                if ys > 1e-10*sn*sn and not batch_changed :\n",
        "                    # updating memory (only when we have y within a single batch)\n",
        "                    if len(old_dirs) == history_size:\n",
        "                        # shift history by one (limited-memory)\n",
        "                        old_dirs.pop(0)\n",
        "                        old_stps.pop(0)\n",
        "\n",
        "                    # store new direction/step\n",
        "                    old_dirs.append(y)\n",
        "                    old_stps.append(s)\n",
        "\n",
        "                    # update scale of initial Hessian approximation\n",
        "                    H_diag = ys / y.dot(y)  # (y*y)\n",
        "\n",
        "                if math.isnan(H_diag):\n",
        "                  print('Warning H_diag nan')\n",
        "\n",
        "                # compute the approximate (L-BFGS) inverse Hessian\n",
        "                # multiplied by the gradient\n",
        "                num_old = len(old_dirs)\n",
        "\n",
        "                if 'ro' not in state:\n",
        "                    state['ro'] = [None] * history_size\n",
        "                    state['al'] = [None] * history_size\n",
        "                ro = state['ro']\n",
        "                al = state['al']\n",
        "\n",
        "                for i in range(num_old):\n",
        "                    ro[i] = 1. / old_dirs[i].dot(old_stps[i])\n",
        "\n",
        "                # iteration in L-BFGS loop collapsed to use just one buffer\n",
        "                q = flat_grad.neg()\n",
        "                for i in range(num_old - 1, -1, -1):\n",
        "                    al[i] = old_stps[i].dot(q) * ro[i]\n",
        "                    q.add_(old_dirs[i],alpha=-al[i])\n",
        "\n",
        "                # multiply by initial Hessian\n",
        "                # r/d is the final direction\n",
        "                d = r = torch.mul(q, H_diag)\n",
        "                for i in range(num_old):\n",
        "                    be_i = old_dirs[i].dot(r) * ro[i]\n",
        "                    r.add_(old_stps[i],alpha=al[i] - be_i)\n",
        "\n",
        "            if prev_flat_grad is None:\n",
        "                prev_flat_grad = flat_grad.clone()\n",
        "\n",
        "            else:\n",
        "                prev_flat_grad.copy_(flat_grad)\n",
        "\n",
        "            prev_loss = loss\n",
        "\n",
        "            ############################################################\n",
        "            # compute step length\n",
        "            ############################################################\n",
        "            # reset initial guess for step size\n",
        "            if state['n_iter'] == 1:\n",
        "                t = min(1., 1. / abs_grad_sum) * lr\n",
        "            else:\n",
        "                t = lr\n",
        "\n",
        "            # directional derivative\n",
        "            gtd = flat_grad.dot(d)  # g * d\n",
        "\n",
        "            if math.isnan(gtd.item()):\n",
        "              print('Warning grad norm infinite')\n",
        "              print('iter %d'%state['n_iter'])\n",
        "              print('||grad||=%f'%grad_nrm)\n",
        "              print('||d||=%f'%d.norm().item())\n",
        "            # optional line search: user function\n",
        "            ls_func_evals = 0\n",
        "            if line_search_fn:\n",
        "                # perform line search, using user function\n",
        "                ##raise RuntimeError(\"line search function is not supported yet\")\n",
        "                #FF#################################\n",
        "                # Note: we disable gradient calculation during line search\n",
        "                # because it is not needed\n",
        "                torch.set_grad_enabled(False)\n",
        "                if not batch_mode:\n",
        "                 t=self._linesearch_cubic(closure,d,1e-6) \n",
        "                else:\n",
        "                 t=self._linesearch_backtrack(closure,d,flat_grad,alphabar)\n",
        "                torch.set_grad_enabled(True)\n",
        "\n",
        "                if math.isnan(t):\n",
        "                  print('Warning: stepsize nan')\n",
        "                  t=lr\n",
        "                self._add_grad(t, d) #FF param = param + t * d \n",
        "                if be_verbose:\n",
        "                 print('step size=%f'%(t))\n",
        "                #FF#################################\n",
        "            else:\n",
        "                #FF Here, t = stepsize,  d = -grad, in cache\n",
        "                # no line search, simply move with fixed-step\n",
        "                self._add_grad(t, d) #FF param = param + t * d \n",
        "            if n_iter != max_iter:\n",
        "                    # re-evaluate function only if not in last iteration\n",
        "                    # the reason we do this: in a stochastic setting,\n",
        "                    # no use to re-evaluate that function here\n",
        "                    loss = float(closure())\n",
        "                    flat_grad = self._gather_flat_grad()\n",
        "                    abs_grad_sum = flat_grad.abs().sum()\n",
        "                    if math.isnan(abs_grad_sum):\n",
        "                       print('Warning: gradient nan')\n",
        "                       break\n",
        "                    ls_func_evals = 1\n",
        "\n",
        "            # update func eval\n",
        "            current_evals += ls_func_evals\n",
        "            state['func_evals'] += ls_func_evals\n",
        "\n",
        "            ############################################################\n",
        "            # check conditions\n",
        "            ############################################################\n",
        "            if n_iter == max_iter:\n",
        "                break\n",
        "\n",
        "            if current_evals >= max_eval:\n",
        "                break\n",
        "\n",
        "            if abs_grad_sum <= tolerance_grad:\n",
        "                break\n",
        "\n",
        "            if gtd > -tolerance_change:\n",
        "                break\n",
        "\n",
        "            if d.mul(t).abs_().sum() <= tolerance_change:\n",
        "                break\n",
        "\n",
        "            if abs(loss - prev_loss) < tolerance_change:\n",
        "                break\n",
        "\n",
        "        state['d'] = d\n",
        "        state['t'] = t\n",
        "        state['old_dirs'] = old_dirs\n",
        "        state['old_stps'] = old_stps\n",
        "        state['H_diag'] = H_diag\n",
        "        state['prev_flat_grad'] = prev_flat_grad\n",
        "        state['prev_loss'] = prev_loss\n",
        "\n",
        "        if batch_mode:\n",
        "         if 'running_avg' not in locals() or running_avg is None:\n",
        "           running_avg=torch.zeros_like(flat_grad.data)\n",
        "           running_avg_sq=torch.zeros_like(flat_grad.data)\n",
        "         state['running_avg']=running_avg\n",
        "         state['running_avg_sq']=running_avg_sq\n",
        "   \n",
        "\n",
        "        return orig_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VshZ24RnHxOA"
      },
      "outputs": [],
      "source": [
        "sdsds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvK6aVO7oCUm"
      },
      "outputs": [],
      "source": [
        "#!/home/youli/miniconda3/bin/python3\n",
        "# coding=utf8\n",
        "\"\"\"\n",
        "# Author: youli\n",
        "# Created Time : 2021-12-27 15:38:05\n",
        "\n",
        "# File Name: model_construct.py\n",
        "# Description:\n",
        "    test for Pytorch model\n",
        "\n",
        "\"\"\"\n",
        "print(f\"pytorch test\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader,TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "input_size = 20000\n",
        "train_size = int(input_size*0.9)\n",
        "test_size  = input_size-train_size\n",
        "batch_size = 1000\n",
        "\n",
        "x_total = np.linspace(-1.0, 1.0, input_size, dtype=np.float32)\n",
        "x_total = np.random.choice(x_total,size=input_size,replace=False) #random sampling\n",
        "x_train = x_total[0:train_size]\n",
        "x_train= x_train.reshape((train_size,1))\n",
        "x_test  = x_total[train_size:input_size]\n",
        "x_test= x_test.reshape((test_size,1))\n",
        "\n",
        "x_train=torch.from_numpy(x_train)\n",
        "x_test=torch.from_numpy(x_test)\n",
        "\n",
        "y_train = torch.from_numpy(np.sinc(10.0 * x_train))\n",
        "y_test  = torch.from_numpy(np.sinc(10.0 * x_test))\n",
        "\n",
        "training_data = TensorDataset(x_train,y_train)\n",
        "test_data = TensorDataset(x_test,y_test)\n",
        "\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(\"Shape of X: \", X.shape)\n",
        "    print(\"Shape of y: \", y.shape, y.dtype)\n",
        "    break\n",
        "for X, y in train_dataloader:\n",
        "    print(\"Shape of X: \", X.shape)\n",
        "    print(\"Shape of y: \", y.shape, y.dtype)\n",
        "    break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# device = \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.tanh_linear= nn.Sequential(\n",
        "                nn.Linear(1,20),\n",
        "                nn.Tanh(),\n",
        "               # nn.Linear(20,20),\n",
        "               # nn.Tanh(),\n",
        "                nn.Linear(20,1),\n",
        "                )\n",
        "        return\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.tanh_linear(x)\n",
        "        return out\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer_adam = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % train_size == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_train\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    print(f\"Test Error: \\n  Avg loss: {test_loss:>8f} \\n\")\n",
        "    return test_loss\n",
        "\n",
        "# training\n",
        "\n",
        "opt_label=f'adam_t20'\n",
        "epochs = 1000\n",
        "print(f\"test for {opt_label}\")\n",
        "optimizer=optimizer_adam\n",
        "loss_train=[]\n",
        "loss_test=[]\n",
        "\n",
        "t1= time.perf_counter()\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    loss_train+=[\n",
        "            test(train_dataloader, model, loss_fn)\n",
        "            ]\n",
        "    loss_test+=[\n",
        "            test(test_dataloader, model, loss_fn)\n",
        "            ]\n",
        "    print(\"Done!\")\n",
        "\n",
        "t2= time.perf_counter()\n",
        "print(\"Elapsed time: \", t2- t1)\n",
        "record=pd.DataFrame({\n",
        "    'epochs':np.arange(epochs)\n",
        "    ,'loss_train':np.array(loss_train)\n",
        "    ,'loss_test':np.array(loss_test)\n",
        "    })\n",
        "record.to_csv(f\"{opt_label}\",sep=' ')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), f\"model{opt_label}.pth\")\n",
        "print(f\"Saved PyTorch Model State to model{opt_label}.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMCvI74H6oWB"
      },
      "outputs": [],
      "source": [
        "sdfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXsSv7LO6nZT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import math\n",
        "import time\n",
        "\n",
        "# How many models (==slaves)\n",
        "K=10\n",
        "# train K models by Federated learning\n",
        "# each iteration over a subset of parameters: 1) average 2) pass back average to slaves 3) SGD step\n",
        "# initialize with pre-trained models (better to use common initialization)\n",
        "# loop order: loop 0: parameters/layers   {\n",
        "#               loop 1 : {  averaging (part of the model)\n",
        "#                loop 2: { epochs/databatches  { train; } } } }\n",
        "# repeat this Nloop times\n",
        "\n",
        "\n",
        "torch.manual_seed(69)\n",
        "# minibatch size\n",
        "default_batch=128 # no. of batches per model is (50000/K)/default_batch\n",
        "Nloop=12 # how many loops over the whole network\n",
        "Nepoch=1 # how many epochs?\n",
        "Nadmm=5 # how many ADMM iterations\n",
        "\n",
        "# regularization\n",
        "lambda1=0.0001 # L1 sweet spot 0.00031\n",
        "lambda2=0.0001 # L2 sweet spot ?\n",
        "admm_rho0=0.1 # ADMM penalty, default value\n",
        "# note that per each slave, and per each layer, there will be a unique rho value\n",
        "\n",
        "load_model=False\n",
        "init_model=True\n",
        "save_model=True\n",
        "check_results=True\n",
        "# if input is biased, each 1/K training data will have\n",
        "# (slightly) different normalization. Otherwise, same normalization\n",
        "biased_input=True\n",
        "be_verbose=False\n",
        "\n",
        "bb_update=False # if true, use adaptive ADMM (Barzilai-Borwein) update\n",
        "if bb_update:\n",
        " #periodicity for the rho update, normally > 1\n",
        " bb_period_T=2\n",
        " bb_alphacorrmin=0.2 # minimum correlation required before an update is done\n",
        " bb_epsilon=1e-3 # threshold to stop updating\n",
        " bb_rhomax=0.1 # keep regularization below a safe upper limit\n",
        "\n",
        "\n",
        "# Set this to true for using ResNet instead of simpler models\n",
        "# In that case, instead of one layer, one block will be trained\n",
        "use_resnet=False\n",
        "\n",
        "# (try to) use a GPU for computation?\n",
        "use_cuda=True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  mydevice=torch.device('cuda')\n",
        "else:\n",
        "  mydevice=torch.device('cpu')\n",
        "\n",
        "\n",
        "# split 50000 training data into K subsets (last one will be smaller if K is not a divisor)\n",
        "K_perslave=math.floor((50000+K-1)/K)\n",
        "subsets_dict={}\n",
        "for ck in range(K):\n",
        " if K_perslave*(ck+1)-1 <= 50000:\n",
        "  subsets_dict[ck]=range(K_perslave*ck,K_perslave*(ck+1)-1)\n",
        " else:\n",
        "  subsets_dict[ck]=range(K_perslave*ck,50000)\n",
        "\n",
        "transforms_dict={}\n",
        "for ck in range(K):\n",
        " if biased_input:\n",
        "  # slightly different normalization for each subset\n",
        "  transforms_dict[ck]=transforms.Compose(\n",
        "   [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5+ck/100,0.5-ck/100,0.5),(0.5+ck/100,0.5-ck/100,0.5))])\n",
        " else:\n",
        "  # same normalization for all training data\n",
        "  transforms_dict[ck]=transforms.Compose(\n",
        "   [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
        "\n",
        "\n",
        "trainset_dict={}\n",
        "testset_dict={}\n",
        "trainloader_dict={}\n",
        "testloader_dict={}\n",
        "for ck in range(K):\n",
        " trainset_dict[ck]=torchvision.datasets.CIFAR10(root='./torchdata', train=True,\n",
        "    download=True, transform=transforms_dict[ck])\n",
        " testset_dict[ck]=torchvision.datasets.CIFAR10(root='./torchdata', train=False,\n",
        "    download=True, transform=transforms_dict[ck])\n",
        " trainloader_dict[ck] = torch.utils.data.DataLoader(trainset_dict[ck], batch_size=default_batch, shuffle=False, sampler=torch.utils.data.SubsetRandomSampler(subsets_dict[ck]),num_workers=1)\n",
        " testloader_dict[ck]=torch.utils.data.DataLoader(testset_dict[ck], batch_size=default_batch,\n",
        "    shuffle=False, num_workers=0)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# define a cnn\n",
        "from simple_models import *\n",
        "\n",
        "net_dict={}\n",
        "\n",
        "for ck in range(K):\n",
        " if not use_resnet:\n",
        "  net_dict[ck]=Net().to(mydevice)\n",
        " else:\n",
        "  net_dict[ck]=ResNet18().to(mydevice)\n",
        " # update from saved models\n",
        " if load_model:\n",
        "   checkpoint=torch.load('./s'+str(ck)+'.model',map_location=mydevice)\n",
        "   net_dict[ck].load_state_dict(checkpoint['model_state_dict'])\n",
        "   net_dict[ck].train()\n",
        "\n",
        "########################################################################### helper functions\n",
        "from simple_utils import *\n",
        "\n",
        "def verification_error_check(net_dict):\n",
        "  for ck in range(K):\n",
        "   correct=0\n",
        "   total=0\n",
        "   net=net_dict[ck]\n",
        "   for data in testloader_dict[ck]:\n",
        "     images,labels=data\n",
        "     outputs=net(Variable(images).to(mydevice))\n",
        "     _,predicted=torch.max(outputs.data,1)\n",
        "     correct += (predicted==labels.to(mydevice)).sum()\n",
        "     total += labels.size(0)\n",
        "\n",
        "   print('Accuracy of the network %d on the %d test images:%%%f'%\n",
        "     (ck,total,100*correct//total))\n",
        "##############################################################################################\n",
        "\n",
        "if init_model:\n",
        "  for ck in range(K):\n",
        "   # note: use same seed for random number generation\n",
        "   torch.manual_seed(0)\n",
        "   net_dict[ck].apply(init_weights)\n",
        "\n",
        "criteria_dict={}\n",
        "for ck in range(K):\n",
        " criteria_dict[ck]=nn.CrossEntropyLoss()\n",
        "\n",
        "# get layer ids in given order 0..L-1 for selective training\n",
        "np.random.seed(0)# get same list\n",
        "Li=net_dict[0].train_order_block_ids()\n",
        "L=len(Li)\n",
        "\n",
        "# regularization (per layer, per slave)\n",
        "# Note: need to scale rho down when starting from scratch  \n",
        "rho=torch.ones(L,3).to(mydevice)*admm_rho0\n",
        "# this will be updated when using adaptive ADMM (bb_update=True)\n",
        "\n",
        "\n",
        "# from lbfgsnew import LBFGSNew # custom optimizer\n",
        "import torch.optim as optim\n",
        "############### loop 00 (over the full net)\n",
        "for nloop in range(Nloop):\n",
        "  ############ loop 0 (over layers of the network)\n",
        "  for ci in range(L):\n",
        "   for ck in range(K):\n",
        "      unfreeze_one_block(net_dict[ck],ci)\n",
        "   trainable=filter(lambda p: p.requires_grad, net_dict[0].parameters())\n",
        "   params_vec1=torch.cat([x.view(-1) for x in list(trainable)])\n",
        "  \n",
        "   # number of parameters trained\n",
        "   N=params_vec1.numel()\n",
        "   z=torch.empty(N,dtype=torch.float,requires_grad=False).to(mydevice)\n",
        "   z.fill_(0.0)\n",
        "   y_dict={}\n",
        "   for ck in range(K):\n",
        "      y_dict[ck]=torch.empty(N,dtype=torch.float,requires_grad=False).to(mydevice)\n",
        "      y_dict[ck].fill_(0.0)\n",
        "\n",
        "   if bb_update: # extra storage for adaptive ADMM\n",
        "      yhat_dict={}\n",
        "      yhat0_dict={}\n",
        "      x0_dict={}\n",
        "      for ck in range(K):\n",
        "         yhat_dict[ck]=torch.empty(N,dtype=torch.float,requires_grad=False).to(mydevice)\n",
        "         yhat_dict[ck].fill_(0.0)\n",
        "         x0_dict[ck]=torch.empty(N,dtype=torch.float,requires_grad=False).to(mydevice)\n",
        "         yhat0_dict[ck]=get_trainable_values(net_dict[ck],mydevice)\n",
        "      \n",
        "  \n",
        "   opt_dict={}\n",
        "   for ck in range(K):\n",
        "    opt_dict[ck]=LBFGSNew(filter(lambda p: p.requires_grad, net_dict[ck].parameters()), history_size=10, max_iter=4, line_search_fn=True,batch_mode=True)\n",
        "    # opt_dict[ck]=optim.Adam(filter(lambda p: p.requires_grad, net_dict[ck].parameters()),lr=0.001)\n",
        "  \n",
        "   ############# loop 1 (ADMM for subset of model)\n",
        "   for nadmm in range(Nadmm):\n",
        "     ##### loop 2 (data) (all network updates are done per epoch, because K is large\n",
        "     ##### and data per host is assumed to be small)\n",
        "     for epoch in range(Nepoch):\n",
        "\n",
        "        #### loop 3 (models)\n",
        "        for ck in range(K):\n",
        "          running_loss=0.0\n",
        "  \n",
        "          for i,data1 in enumerate(trainloader_dict[ck],0):\n",
        "            # get the inputs\n",
        "            inputs1,labels1=data1\n",
        "            # wrap them in variable\n",
        "            inputs1,labels1=Variable(inputs1).to(mydevice),Variable(labels1).to(mydevice)\n",
        "    \n",
        " \n",
        "            def closure1():\n",
        "                 if torch.is_grad_enabled():\n",
        "                    opt_dict[ck].zero_grad()\n",
        "                 outputs=net_dict[ck](inputs1)\n",
        "                 # augmented lagrangian terms y^T (x-z) + rho/2 ||x-z||^2\n",
        "                 trainable=filter(lambda p: p.requires_grad, net_dict[ck].parameters())\n",
        "                 params_vec1=torch.cat([x.view(-1) for x in list(trainable)])\n",
        "                 xdelta=params_vec1-z\n",
        "                 augmented_terms=(torch.dot(y_dict[ck],xdelta))+0.5*rho[ci,0]*(torch.norm(xdelta,2)**2)\n",
        "                 loss=criteria_dict[ck](outputs,labels1)+augmented_terms\n",
        "                 if ci in net_dict[ck].linear_layer_ids():\n",
        "                    loss+=lambda1*torch.norm(params_vec1,1)+lambda2*(torch.norm(params_vec1,2)**2)\n",
        "                 if loss.requires_grad:\n",
        "                    loss.backward()\n",
        "                 return loss\n",
        "  \n",
        "            # ADMM step 1\n",
        "            opt_dict[ck].step(closure1)\n",
        "  \n",
        "            # only for diagnostics\n",
        "            outputs1=net_dict[ck](inputs1)\n",
        "            loss1=criteria_dict[ck](outputs1,labels1).data.item()\n",
        "            running_loss +=loss1\n",
        "           \n",
        "            if be_verbose:\n",
        "              print('model=%d block=[%d,%d] %d(%d) minibatch=%d epoch=%d loss %e'%(ck,Li[ci][0],Li[ci][1],nloop,N,i,epoch,loss1))\n",
        "         \n",
        "        # ADMM step 2 update global z\n",
        "        x_dict={}\n",
        "        for ck in range(K):\n",
        "          x_dict[ck]=get_trainable_values(net_dict[ck],mydevice)\n",
        "\n",
        "        # decide and update rho for this ADMM iteration (not the first iteration)\n",
        "        if bb_update:\n",
        "          if nadmm==0:\n",
        "            # store for next use\n",
        "            for ck in range(K):\n",
        "              x0_dict[ck]=x_dict[ck]\n",
        "          elif (nadmm%bb_period_T)==0:\n",
        "            for ck in range(K):\n",
        "              yhat_1=y_dict[ck]+rho[ci,0]*(x_dict[ck]-z)\n",
        "              deltay1=yhat_1-yhat0_dict[ck]\n",
        "              deltax1=x_dict[ck]-x0_dict[ck]\n",
        "              # inner products\n",
        "              d11=torch.dot(deltay1,deltay1)\n",
        "              d12=torch.dot(deltay1,deltax1) # note: can be negative\n",
        "              d22=torch.dot(deltax1,deltax1)\n",
        "\n",
        "              print('admm %d deltas=(%e,%e,%e)'%(nadmm,d11,d12,d22))\n",
        "              rhonew=rho[ci,0]\n",
        "              # catch situation where denominator is very small\n",
        "              if torch.abs(d12).item()>bb_epsilon and d11.item()>bb_epsilon and d22.item()>bb_epsilon:\n",
        "                 alpha=d12/torch.sqrt(d11*d22)\n",
        "                 alphaSD=d11/d22\n",
        "                 alphaMG=d12/d22\n",
        "\n",
        "                 if 2.0*alphaMG>alphaSD:\n",
        "                   alphahat=alphaMG\n",
        "                 else:\n",
        "                   alphahat=alphaSD-0.5*alphaMG\n",
        "                 if alpha>=bb_alphacorrmin and alphahat<bb_rhomax: # catches d12 being negative\n",
        "                   rhonew=alphahat\n",
        "                 print('admm %d alphas=(%e,%e,%e)'%(nadmm,alpha,alphaSD,alphaMG))\n",
        "\n",
        "              rho[ci,0]=rhonew\n",
        "              ###############\n",
        "\n",
        "              # carry forward current values for the next update\n",
        "              yhat0_dict[ck]=yhat_1\n",
        "              x0_dict[ck]=x_dict[ck]\n",
        "\n",
        "\n",
        "        znew=torch.zeros(x_dict[0].shape).to(mydevice)\n",
        "        for ck in range(K):\n",
        "         # sum (y+rho x)\n",
        "         znew=znew+y_dict[ck]+rho[ci,0]*x_dict[ck]\n",
        "        znew=znew/(K*rho[ci,0])\n",
        "\n",
        "        dual_residual=torch.norm(z-znew).item()/N # per parameter\n",
        "        z=znew\n",
        "\n",
        "        # -> master will send z to all slaves\n",
        "        # ADMM step 3 update Lagrange multiplier \n",
        "        primal_residual=0.0\n",
        "        for ck in range(K):\n",
        "          ydelta=rho[ci,0]*(x_dict[ck]-z)\n",
        "          primal_residual=primal_residual+torch.norm(ydelta)\n",
        "          y_dict[ck].add_(ydelta)\n",
        "        primal_residual=primal_residual/N # per parameter\n",
        "\n",
        "        print('block=[%d,%d](%d,%f) ADMM=%d/%d primal=%e dual=%e'%(Li[ci][0],Li[ci][1],N,torch.mean(rho).item(),nadmm,nloop,primal_residual,dual_residual))\n",
        "\n",
        "        if check_results:\n",
        "          verification_error_check(net_dict)\n",
        "  \n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "\n",
        "if save_model:\n",
        " for ck in range(K):\n",
        "   torch.save({\n",
        "     'model_state_dict':net_dict[ck].state_dict(),\n",
        "     'epoch':epoch,\n",
        "     'optimizer_state_dict':opt_dict[ck].state_dict(),\n",
        "     'running_loss':running_loss,\n",
        "     },'./s'+str(ck)+'.model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmZU8fDd6oZK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4blviM-YoCXf"
      },
      "outputs": [],
      "source": [
        "#!/home/youli/miniconda3/bin/python3\n",
        "# coding=utf8\n",
        "\"\"\"\n",
        "# Author: youli\n",
        "# Created Time : 2021-12-27 15:38:05\n",
        "\n",
        "# File Name: model_construct.py\n",
        "# Description:\n",
        "    test for Pytorch model\n",
        "\n",
        "\"\"\"\n",
        "print(f\"pytorch test\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader,TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "input_size = 20000\n",
        "train_size = int(input_size*0.9)\n",
        "test_size  = input_size-train_size\n",
        "batch_size = 1000\n",
        "\n",
        "x_total = np.linspace(-1.0, 1.0, input_size, dtype=np.float32)\n",
        "x_total = np.random.choice(x_total,size=input_size,replace=False) #random sampling\n",
        "x_train = x_total[0:train_size]\n",
        "x_train= x_train.reshape((train_size,1))\n",
        "x_test  = x_total[train_size:input_size]\n",
        "x_test= x_test.reshape((test_size,1))\n",
        "\n",
        "x_train=torch.from_numpy(x_train)\n",
        "x_test=torch.from_numpy(x_test)\n",
        "\n",
        "y_train = torch.from_numpy(np.sinc(10.0 * x_train))\n",
        "y_test  = torch.from_numpy(np.sinc(10.0 * x_test))\n",
        "\n",
        "training_data = TensorDataset(x_train,y_train)\n",
        "test_data = TensorDataset(x_test,y_test)\n",
        "\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(\"Shape of X: \", X.shape)\n",
        "    print(\"Shape of y: \", y.shape, y.dtype)\n",
        "    break\n",
        "for X, y in train_dataloader:\n",
        "    print(\"Shape of X: \", X.shape)\n",
        "    print(\"Shape of y: \", y.shape, y.dtype)\n",
        "    break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Get cpu or gpu device for training.\n",
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.tanh_linear= nn.Sequential(\n",
        "                nn.Linear(1,20),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(20,20),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(20,1),\n",
        "                )\n",
        "        return\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.tanh_linear(x)\n",
        "        return out\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "# optimizer_lbfgs= torch.optim.LBFGS(model.parameters(), lr=1, \n",
        "#         history_size=100, max_iter=20,\n",
        "#         line_search_fn=\"strong_wolfe\"\n",
        "#         )\n",
        "optimizer_lbfgs= LBFGSNew(model.parameters(),  \n",
        "        history_size=100, max_iter=20,\n",
        "        line_search_fn=True,batch_mode=True\n",
        "        )\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    lm_lbfgs=model.to(device)\n",
        "    #spacial function for LBFGS\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        x_ = Variable(X, requires_grad=True)\n",
        "        y_ = Variable(y)\n",
        "        def closure():\n",
        "            # # Zero gradients\n",
        "            # optimizer.zero_grad()\n",
        "            # # Forward pass\n",
        "            # y_pred = lm_lbfgs(x_)\n",
        "            # # Compute loss\n",
        "            # loss = loss_fn(y_pred, y_)\n",
        "            # # Backward pass\n",
        "            # loss.backward()\n",
        "            if torch.is_grad_enabled():\n",
        "                optimizer.zero_grad()\n",
        "            y_pred = lm_lbfgs(x_)\n",
        "            loss = loss_fn(y_pred, y_)\n",
        "            if loss.requires_grad:\n",
        "                loss.backward()\n",
        "            return loss\n",
        "\n",
        "        optimizer.step(closure)\n",
        "        loss=closure()\n",
        "\n",
        "        if batch % train_size == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_train\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    print(f\"Test Error: \\n  Avg loss: {test_loss:>8f} \\n\")\n",
        "    return test_loss\n",
        "\n",
        "# training\n",
        "\n",
        "opt_label='lbfgsnew-t20-t20'\n",
        "epochs = 50\n",
        "print(f\"test for {opt_label}\")\n",
        "optimizer=optimizer_lbfgs\n",
        "loss_train=[]\n",
        "loss_test=[]\n",
        "\n",
        "t1= time.perf_counter()\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    loss_train+=[\n",
        "            test(train_dataloader, model, loss_fn)\n",
        "            ]\n",
        "    loss_test+=[\n",
        "            test(test_dataloader, model, loss_fn)\n",
        "            ]\n",
        "    print(\"Done!\")\n",
        "\n",
        "t2= time.perf_counter()\n",
        "print(\"Elapsed time: \", t2- t1)\n",
        "record=pd.DataFrame({\n",
        "    'epochs':np.arange(epochs)\n",
        "    ,'loss_train':np.array(loss_train)\n",
        "    ,'loss_test':np.array(loss_test)\n",
        "    })\n",
        "record.to_csv(f\"{opt_label}\",sep=' ')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), f\"model{opt_label}.pth\")\n",
        "print(f\"Saved PyTorch Model State to model{opt_label}.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8QMjooorXkY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1uHdrDUsJ3b"
      },
      "outputs": [],
      "source": [
        "#!/home/youli/miniconda3/bin/python\n",
        "# coding=utf8\n",
        "\"\"\"\n",
        "# Author: youli\n",
        "# Created Time : 2021-12-28 20:58:28\n",
        "\n",
        "# File Name: summary.py\n",
        "# Description:\n",
        "\n",
        "\"\"\"\n",
        "print(f\"plot test\")\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "adam1=pd.read_csv(\"lbfgs_original-t20\",sep=' ')\n",
        "lbfgs1=pd.read_csv(\"lbfgsnew-t20\",sep=' ')\n",
        "adam2=pd.read_csv(\"lbfgs_original-t20-t20\",sep=' ')\n",
        "lbfgs2=pd.read_csv(\"lbfgsnew-t20-t20\",sep=' ')\n",
        "\n",
        "adam1.loss_train=np.log(adam1.loss_train)\n",
        "adam2.loss_train=np.log(adam2.loss_train)\n",
        "lbfgs1.loss_train=np.log(lbfgs1.loss_train)\n",
        "lbfgs2.loss_train=np.log(lbfgs2.loss_train)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(6,4),dpi=200)\n",
        "\n",
        "plt.title(\" trainning error in log() \")\n",
        "\n",
        "lwidth=3.0\n",
        "\n",
        "plt.plot(adam1.epochs, adam1.loss_train, 'g--', label=\"lbfgs_original 1-20-1\"       ,linewidth=lwidth)\n",
        "plt.plot(adam2.epochs, adam2.loss_train, 'g', label=\"lbfgs_original 1-20-20-1\"      ,linewidth=lwidth)\n",
        "plt.plot(lbfgs1.epochs*10, lbfgs1.loss_train, 'r--', label=\"lbfgsnew 1-20-1\" ,linewidth=lwidth)\n",
        "plt.plot(lbfgs2.epochs*10, lbfgs2.loss_train, 'r', label=\"lbfgsnew 1-20-20-1\",linewidth=lwidth )\n",
        "#plt.plot(lbfgs1.epochs, lbfgs1.loss_train, 'r--', label=\"lbfgs 1-20-1\" ,linewidth=lwidth)\n",
        "#plt.plot(lbfgs2.epochs, lbfgs2.loss_train, 'r', label=\"lbfgs 1-20-20-1\",linewidth=lwidth )\n",
        "\n",
        "plt.text(0.0,-12,\"L-BFGSnew epochs*10\",color='r',size='large')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"log(MAE)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcxblV7asZn6"
      },
      "outputs": [],
      "source": [
        "saasdas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpvOst4joCaU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95EHB6TAoCdS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpEnGVIRoCgH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98rhu-kM1QAt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import math\n",
        "import time\n",
        "\n",
        "# (try to) use a GPU for computation?\n",
        "use_cuda=True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  mydevice=torch.device('cuda')\n",
        "else:\n",
        "  mydevice=torch.device('cpu')\n",
        "\n",
        "\n",
        "# try replacing relu with elu\n",
        "torch.manual_seed(69)\n",
        "default_batch=128 # no. of batches per epoch 50000/default_batch\n",
        "batches_for_report=10#\n",
        "\n",
        "transform=transforms.Compose(\n",
        "   [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
        "\n",
        "\n",
        "trainset=torchvision.datasets.CIFAR10(root='./torchdata', train=True,\n",
        "    download=True, transform=transform)\n",
        "\n",
        "trainloader=torch.utils.data.DataLoader(trainset, batch_size=default_batch,\n",
        "    shuffle=True, num_workers=2)\n",
        "\n",
        "testset=torchvision.datasets.CIFAR10(root='./torchdata', train=False,\n",
        "    download=True, transform=transform)\n",
        "\n",
        "testloader=torch.utils.data.DataLoader(testset, batch_size=default_batch,\n",
        "    shuffle=False, num_workers=0)\n",
        "\n",
        "\n",
        "classes=('plane', 'car', 'bird', 'cat', \n",
        "  'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "'''ResNet in PyTorch.\n",
        "Reference:\n",
        "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
        "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
        " \n",
        "From: https://github.com/kuangliu/pytorch-cifar\n",
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.elu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.elu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.elu(self.bn1(self.conv1(x)))\n",
        "        out = F.elu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.elu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.elu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def ResNet9():\n",
        "    return ResNet(BasicBlock, [1,1,1,1])\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2,2,2,2])\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3,4,6,3])\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3,4,6,3])\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3,4,23,3])\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3,8,36,3])\n",
        "\n",
        "\n",
        "# enable this to use wide ResNet\n",
        "wide_resnet=False\n",
        "if not wide_resnet:\n",
        "  net=ResNet18().to(mydevice)\n",
        "else:\n",
        "  # use wide residual net https://arxiv.org/abs/1605.07146\n",
        "  net=torchvision.models.resnet.wide_resnet50_2().to(mydevice)\n",
        "\n",
        "\n",
        "#####################################################\n",
        "def verification_error_check(net):\n",
        "   correct=0\n",
        "   total=0\n",
        "   for data in testloader:\n",
        "     images,labels=data\n",
        "     outputs=net(Variable(images).to(mydevice))\n",
        "     _,predicted=torch.max(outputs.data,1)\n",
        "     correct += (predicted==labels.to(mydevice)).sum()\n",
        "     total += labels.size(0)\n",
        "\n",
        "   return 100*correct//total\n",
        "#####################################################\n",
        "\n",
        "lambda1=0.000001\n",
        "lambda2=0.001\n",
        "\n",
        "# loss function and optimizer\n",
        "import torch.optim as optim\n",
        "from lbfgsnew import LBFGSNew # custom optimizer\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "#optimizer=optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "#optimizer=optim.Adam(net.parameters(), lr=0.001)\n",
        "optimizer = LBFGSNew(net.parameters(), history_size=7, max_iter=2, line_search_fn=True,batch_mode=True)\n",
        "\n",
        "\n",
        "load_model=False\n",
        "# update from a saved model \n",
        "if load_model:\n",
        "  checkpoint=torch.load('./res18.model',map_location=mydevice)\n",
        "  net.load_state_dict(checkpoint['model_state_dict'])\n",
        "  net.train() # initialize for training (BN,dropout)\n",
        "\n",
        "start_time=time.time()\n",
        "use_lbfgs=True\n",
        "# train network\n",
        "for epoch in range(20):\n",
        "  running_loss=0.0\n",
        "  for i,data in enumerate(trainloader,0):\n",
        "    # get the inputs\n",
        "    inputs,labels=data\n",
        "    # wrap them in variable\n",
        "    inputs,labels=Variable(inputs).to(mydevice),Variable(labels).to(mydevice)\n",
        "\n",
        "    if not use_lbfgs:\n",
        "     # zero gradients\n",
        "     optimizer.zero_grad()\n",
        "     # forward+backward optimize\n",
        "     outputs=net(inputs)\n",
        "     loss=criterion(outputs,labels)\n",
        "     loss.backward()\n",
        "     optimizer.step()\n",
        "    else:\n",
        "      if not wide_resnet:\n",
        "        layer1=torch.cat([x.view(-1) for x in net.layer1.parameters()])\n",
        "        layer2=torch.cat([x.view(-1) for x in net.layer2.parameters()])\n",
        "        layer3=torch.cat([x.view(-1) for x in net.layer3.parameters()])\n",
        "        layer4=torch.cat([x.view(-1) for x in net.layer4.parameters()])\n",
        "\n",
        "      def closure():\n",
        "        if torch.is_grad_enabled():\n",
        "         optimizer.zero_grad()\n",
        "        outputs=net(inputs)\n",
        "        if not wide_resnet:\n",
        "          l1_penalty=lambda1*(torch.norm(layer1,1)+torch.norm(layer2,1)+torch.norm(layer3,1)+torch.norm(layer4,1))\n",
        "          l2_penalty=lambda2*(torch.norm(layer1,2)+torch.norm(layer2,2)+torch.norm(layer3,2)+torch.norm(layer4,2))\n",
        "          loss=criterion(outputs,labels)+l1_penalty+l2_penalty\n",
        "        else:\n",
        "          l1_penalty=0\n",
        "          l2_penalty=0\n",
        "          loss=criterion(outputs,labels)\n",
        "        if loss.requires_grad:\n",
        "          loss.backward()\n",
        "          #print('loss %f l1 %f l2 %f'%(loss,l1_penalty,l2_penalty))\n",
        "        return loss\n",
        "      optimizer.step(closure)\n",
        "    # only for diagnostics\n",
        "    outputs=net(inputs)\n",
        "    loss=criterion(outputs,labels)\n",
        "    running_loss +=loss.data.item()\n",
        "\n",
        "    if math.isnan(loss.data.item()):\n",
        "       print('loss became nan at %d'%i)\n",
        "       break\n",
        "\n",
        "    # print statistics\n",
        "    if i%(batches_for_report) == (batches_for_report-1): # after every 'batches_for_report'\n",
        "      print('%f: [%d, %5d] loss: %.5f accuracy: %.3f'%\n",
        "         (time.time()-start_time,epoch+1,i+1,running_loss/batches_for_report,\n",
        "         verification_error_check(net)))\n",
        "      running_loss=0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "\n",
        "# save model (and other extra items)\n",
        "torch.save({\n",
        "            'model_state_dict':net.state_dict(),\n",
        "            'epoch':epoch,\n",
        "            'optimizer_state_dict':optimizer.state_dict(),\n",
        "            'running_loss':running_loss,\n",
        "           },'./res.model')\n",
        "\n",
        "\n",
        "# whole dataset\n",
        "correct=0\n",
        "total=0\n",
        "for data in trainloader:\n",
        "   images,labels=data\n",
        "   outputs=net(Variable(images).to(mydevice)).cpu()\n",
        "   _,predicted=torch.max(outputs.data,1)\n",
        "   total += labels.size(0)\n",
        "   correct += (predicted==labels).sum()\n",
        "   \n",
        "print('Accuracy of the network on the %d train images: %d %%'%\n",
        "    (total,100*correct//total))\n",
        "\n",
        "correct=0\n",
        "total=0\n",
        "for data in testloader:\n",
        "   images,labels=data\n",
        "   outputs=net(Variable(images).to(mydevice)).cpu()\n",
        "   _,predicted=torch.max(outputs.data,1)\n",
        "   total += labels.size(0)\n",
        "   correct += (predicted==labels).sum()\n",
        "   \n",
        "print('Accuracy of the network on the %d test images: %d %%'%\n",
        "    (total,100*correct//total))\n",
        "\n",
        "\n",
        "class_correct=list(0. for i in range(10))\n",
        "class_total=list(0. for i in range(10))\n",
        "for data in testloader:\n",
        "  images,labels=data\n",
        "  outputs=net(Variable(images).to(mydevice)).cpu()\n",
        "  _,predicted=torch.max(outputs.data,1)\n",
        "  c=(predicted==labels).squeeze()\n",
        "  for i in range(4):\n",
        "    label=labels[i]\n",
        "    class_correct[label] += c[i]\n",
        "    class_total[label] += 1\n",
        "\n",
        "for i in range(10):\n",
        "  print('Accuracy of %5s : %2d %%' %\n",
        "    (classes[i],100*float(class_correct[i])/float(class_total[i])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vx7BQp8DlRy1"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4MLRMCp1QDK"
      },
      "outputs": [],
      "source": [
        "a =[1,2,3,4,5,6,7,7,7,7,7,7,7]\n",
        "a = torch.from_numpy(np.array(a))\n",
        "b = a.t()\n",
        "new = torch.matmul(a, b)\n",
        "print(new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOwUmpykpWJK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_YtXtkv1QFo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebFJNSXW1QIn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiW5vN5r1QLE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SKszFbb1QN5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WTMPg5C1QQh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25WIPdH61QTK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "df_Qix7J1QWK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "1BEbBOw_AjcYZx4VLjbPZJUGpmvPAbZLi",
      "authorship_tag": "ABX9TyMtaLmwngaIG3reuedwQRVa",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}